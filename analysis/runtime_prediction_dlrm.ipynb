{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML\n",
    "from scipy.stats.mstats import gmean \n",
    "import argparse, logging, tempfile, json, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.rcParams['figure.max_open_warning'] = 100\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from trace_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "trace_file = \"./libgpumon_activities_425511.json\"\n",
    "trim_trace(trace_file, 0.90, 1.0)\n",
    "base_trace_dir = run_ATC()\n",
    "print(base_trace_dir)\n",
    "with open(base_trace_dir + \"/two_iteration_trace.json\") as two:\n",
    "    two_iteration_stats = json.load(two)\n",
    "\n",
    "ops = []\n",
    "roots, cc, corrected_start_time, corrected_end_time = process_event_hierarchy(two_iteration_stats, skip_module=False)\n",
    "get_operators(roots, ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizes of C*B and C'*A\n",
    "def get_addmm_backward_size(op):\n",
    "    sizes = []\n",
    "    for x in op.children:\n",
    "        if x.name() == \"mm\":\n",
    "            size = x.input_shape()\n",
    "            sizes.append((size[0][0], size[0][1], size[1][1]))\n",
    "    return sizes\n",
    "\n",
    "# Never seen BmmBackward0 having only one bmm. Possible though.\n",
    "def get_bmm_backward_size(op):\n",
    "    sizes = []\n",
    "    for x in op.children:\n",
    "        if x.name() == \"bmm\":\n",
    "            size = x.input_shape()\n",
    "            sizes.append((size[0][0], size[0][1], size[0][2], size[1][2]))\n",
    "    return sizes\n",
    "\n",
    "# Not working in new traces as the output_nr op calls are removed\n",
    "def get_embedding_lookup_forward_size(op):\n",
    "    sizes = []\n",
    "    rows_per_block = None\n",
    "    for x in op.children:\n",
    "        if x.name() == \"output_nr\":\n",
    "            sizes.append(x.input_shape())\n",
    "        if x.name() == \"cudaLaunchKernel\":\n",
    "            ex = cc[op.external_id()][\"callees\"][x.correlation_id()][\"executor\"]\n",
    "            rows_per_block = ex.event[\"args\"][\"block\"][1]\n",
    "    D = int(sizes[0][0][1])\n",
    "    T = int(sizes[1][0][0])\n",
    "    E = int(sizes[0][0][0] / T)\n",
    "    B = int((sizes[3][0][0] - 1) / T)\n",
    "    L = int(sizes[2][0][0] / B / T)\n",
    "    return B, E, T, L, D, rows_per_block\n",
    "\n",
    "# Not working in new traces as the size op calls are removed\n",
    "def get_embedding_lookup_backward_size(op):\n",
    "    sizes = []\n",
    "    rows_per_block = -1\n",
    "    for x in op.children:\n",
    "        if x.name() == \"size\":\n",
    "            sizes.append(x.input_shape())\n",
    "        if x.name() == \"cudaLaunchKernel\":\n",
    "            ex = cc[op.external_id()][\"callees\"][x.correlation_id()][\"executor\"]\n",
    "            rows_per_block = ex.event[\"args\"][\"block\"][1]\n",
    "    T = int(sizes[0][0][0])\n",
    "    E = int(sizes[1][0][0] / T)\n",
    "    D = int(sizes[1][0][1])\n",
    "    B = int((sizes[2][0][0] - 1) / T)\n",
    "    return B, E, T, _, D, rows_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_stats = pd.read_csv(\"./fully_connected_forward.csv\", delimiter=',')\n",
    "fc_stats = preprocessing(fc_stats)\n",
    "fc_stats = fc_stats[fc_stats[\"kernel_name\"].str.startswith(\"volta\")].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_size = 6 * 1024 * 1024 * 4\n",
    "num_SM = 80\n",
    "peak_dram_bw = 809 # GB/s\n",
    "peak_l2_bw = 2888 # GB/s\n",
    "peak_throughput = 12200 # GFLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4340.7676440049445"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "bento_obj_id": "139888146244208"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_forward_predictor(peak_dram_bw, peak_l2_bw, L2_size, num_SM, **kwargs):\n",
    "    # hit_rate = C(X, L) / C(E, L), X = avg_num_rows_per_table\n",
    "    def hit_rate(X, E, L):\n",
    "        ret = 1.0\n",
    "        e = E\n",
    "        x = X\n",
    "        for idx in range(L):\n",
    "            ret *= x / e\n",
    "            x -= 1\n",
    "            e -= 1\n",
    "        return ret\n",
    "\n",
    "    # Average number of rows per table in L2\n",
    "    y = kwargs\n",
    "    num_total_warps = y[\"batch_size\"] * y[\"num_tables\"] # Total warp number of the kernel\n",
    "    num_warps_per_sm = y[\"rows_per_block\"] # Number of warps per sm\n",
    "    num_warps_simul = num_SM * num_warps_per_sm # Total number of warps simultaneously running on the device\n",
    "    num_tables_simul = (num_warps_simul + y[\"batch_size\"] - 1) // y[\"batch_size\"] # Number of tables simultaneously being accessed on the device\n",
    "    avg_table_size = min(L2_size // num_tables_simul, y[\"num_embeddings\"] * y[\"embedding_dim\"] * 4) # Average table size that reside on the device\n",
    "    indices_size = 0 if y[\"shmem\"] else div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "    avg_num_rows_per_table = (avg_table_size - indices_size) // 4 // y[\"embedding_dim\"]\n",
    "\n",
    "    # Hit rate\n",
    "    hr = hit_rate(avg_num_rows_per_table, y[\"num_embeddings\"], y[\"batch_size\"])\n",
    "    \n",
    "    # num_thread_x\n",
    "    num_thread_x = max(y[\"embedding_dim\"] / 4, 1024 / y[\"rows_per_block\"])\n",
    "\n",
    "    # Traffics\n",
    "    table_offsets_traffic = 32\n",
    "    offsets_traffic = 32\n",
    "    if y[\"shmem\"]:\n",
    "        indices_dram_traffic = div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "        indices_l2_traffic = 0\n",
    "    else: # no_shmem\n",
    "        indices_dram_traffic = div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "        indices_l2_traffic = y[\"embedding_dim\"] // (4 * num_thread_x) * div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "    table_traffic = y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32)\n",
    "    output_traffic = (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32)\n",
    "\n",
    "    # avg_table_size all as dram traffic\n",
    "    # 21, 26, 13, 7, 4, 4, 24, (0.2 Â± 0.21)\n",
    "    total_l2_traffic = ((table_offsets_traffic + offsets_traffic + indices_l2_traffic) * y[\"batch_size\"] + \\\n",
    "                        hr * (table_traffic * y[\"batch_size\"] - avg_table_size)) * y[\"num_tables\"]\n",
    "    total_dram_traffic = ((indices_dram_traffic + output_traffic) * y[\"batch_size\"] + \\\n",
    "                          (1 - hr) * (table_traffic * y[\"batch_size\"] - avg_table_size) + avg_table_size) * y[\"num_tables\"]\n",
    "\n",
    "    return max(total_dram_traffic / peak_dram_bw / 1000.0, total_l2_traffic / peak_l2_bw / 1000.0)\n",
    "        \n",
    "# e.g. 4340 vs 4789\n",
    "embedding_forward_predictor(peak_dram_bw, peak_l2_bw, L2_size, num_SM, batch_size=4096, num_embeddings=500000, num_tables=197, bag_size=32, embedding_dim=32, rows_per_block=128, shmem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86291.71294932015"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "bento_obj_id": "139888145467952"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_backward_sgd_predictor(peak_dram_bw, **kwargs):\n",
    "    y = kwargs\n",
    "    if y[\"shmem\"]: # 40% GMAE...\n",
    "        indices_traffic = div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "        grad_output_traffic = div_round_up(y[\"embedding_dim\"] * 4, 32) * 32\n",
    "    else: # backward_sgd_no_shmem\n",
    "        indices_traffic = y[\"bag_size\"] * 32\n",
    "        grad_output_traffic = (y[\"bag_size\"] * div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) * 2\n",
    "\n",
    "    # Traffic per warp = t_offsets + t_table_offsets + t_indices + t_weights + t_grad_outputs\n",
    "    total_traffic_per_warp = 32 + \\\n",
    "                            32 + \\\n",
    "                            indices_traffic + \\\n",
    "                            2 * y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) + \\\n",
    "                            grad_output_traffic\n",
    "\n",
    "    # Traffic = warp * traffic per warp\n",
    "    total_traffic = y[\"batch_size\"] * y[\"num_tables\"] * total_traffic_per_warp\n",
    "\n",
    "    # Total compute throughput\n",
    "    mac_per_warp = y[\"bag_size\"] * 4 * (y[\"embedding_dim\"] // 4)\n",
    "    total_mac = y[\"batch_size\"] * y[\"num_tables\"] * mac_per_warp\n",
    "\n",
    "    return max(total_traffic / peak_dram_bw / 1000, total_mac / peak_throughput / 1000)\n",
    "\n",
    "# e.g 86291 vs 77508\n",
    "embedding_backward_sgd_predictor(peak_dram_bw, batch_size=2048, num_embeddings=200000, num_tables=128, bag_size=128, embedding_dim=128, rows_per_block=32, shmem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64226.252103831896"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "bento_obj_id": "139888145468304"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_backward_rowwise_adagrad_approx_predictor(peak_dram_bw, **kwargs):\n",
    "    y = kwargs\n",
    "    \n",
    "    # Traffic = warp * traffic per warp\n",
    "    total_traffic_per_warp = 32 + \\\n",
    "                            32 + \\\n",
    "                            2 * (div_round_up(y[\"bag_size\"] * 4, 32) * 32) + \\\n",
    "                            y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) + \\\n",
    "                            (y[\"bag_size\"] + 1) * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) + \\\n",
    "                            y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32)\n",
    "    total_traffic = y[\"batch_size\"] * y[\"num_tables\"] * total_traffic_per_warp\n",
    "\n",
    "    return total_traffic / peak_dram_bw / 1000.0\n",
    "\n",
    "# e.g 64226 vs 63304\n",
    "embedding_backward_rowwise_adagrad_approx_predictor(peak_dram_bw, batch_size=2048, num_embeddings=200000, num_tables=128, bag_size=128, embedding_dim=128, rows_per_block=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_time: 12117.71546046202\n"
     ]
    }
   ],
   "source": [
    "total_time = 0.0\n",
    "for op in ops:\n",
    "    t = 0.0\n",
    "    if op.name() == \"addmm\":\n",
    "        size = op.input_shape()\n",
    "        M, K, N = size[1][0], size[1][1], size[2][1]\n",
    "        t = fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=1, M=M, N=N, K=K)\n",
    "#         print(\"addmm\", M, N, K, t)\n",
    "    if op.name() == \"bmm\":\n",
    "        size = op.input_shape()\n",
    "        batch_size, M, K, N = size[0][0], size[0][1], size[0][2], size[1][2]\n",
    "        t = fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=batch_size, M=M, N=N, K=K)\n",
    "#         print(\"bmm\", batch_size, M, N, K, t)\n",
    "    if op.name() == \"LookupFunction\":\n",
    "        B, E, T, L, D, rows_per_block = get_embedding_lookup_forward_size(op)\n",
    "        lks = []\n",
    "        for c in op.children:\n",
    "            if c.name() == \"cudaLaunchKernel\":\n",
    "                lks.append(c)\n",
    "        callees = cc[lks[0].external_id()][\"callees\"]\n",
    "        shmem = list(callees.values())[0][\"executor\"].name().split(',')[1].strip()\n",
    "        t = embedding_forward_predictor(peak_dram_bw, peak_l2_bw, L2_size, num_SM, batch_size=B, num_embeddings=E, num_tables=T, bag_size=L, embedding_dim=D, rows_per_block=rows_per_block, shmem=shmem)\n",
    "#         print(\"Embedding forward\", t)\n",
    "    if op.name() == \"LookupFunctionBackward\":\n",
    "        B, E, T, _, D, rows_per_block = get_embedding_lookup_backward_size(op)\n",
    "        L = 38 # TODO: Cannot get it from trace. Hard code it.\n",
    "        sgd = False\n",
    "        lks = []\n",
    "        for c in op.children:\n",
    "            if c.name() == \"cudaLaunchKernel\":\n",
    "                lks.append(c)\n",
    "        callees = cc[lks[0].external_id()][\"callees\"]\n",
    "        kernel_name = list(callees.values())[0][\"executor\"].name()\n",
    "        if \"sgd\" in kernel_name:\n",
    "            sgd = True\n",
    "        shmem = kernel_name.split(',')[1].strip()\n",
    "        if sgd:\n",
    "            t = embedding_backward_sgd_predictor(peak_dram_bw, batch_size=B, num_embeddings=E, num_tables=T, bag_size=L, embedding_dim=D, rows_per_block=rows_per_block, shmem=shmem)\n",
    "        else:\n",
    "            t = embedding_backward_rowwise_adagrad_approx_predictor(peak_dram_bw, batch_size=B, num_embeddings=E, num_tables=T, bag_size=L, embedding_dim=D, rows_per_block=rows_per_block)\n",
    "#         print(\"Embedding backward\", t)\n",
    "    if op.name() == \"AddmmBackward\":\n",
    "        sizes = get_addmm_backward_size(op)\n",
    "        for size in sizes:\n",
    "            M, K, N = size\n",
    "            t += fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=1, M=M, N=N, K=K)\n",
    "#         print(\"AddmmBackward\", t)\n",
    "    if op.name() == \"BmmBackward0\":\n",
    "        sizes = get_bmm_backward_size(op)\n",
    "        for size in sizes:\n",
    "            batch_size, M, K, N = size\n",
    "            t += fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=batch_size, M=M, N=N, K=K)\n",
    "#         print(\"BmmBackward0\", t)\n",
    "    total_time += t\n",
    "print(\"total_time:\", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5829.673816702252"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "bento_obj_id": "139888250505680"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def fc_forward_predictor(peak_dram_bw, peak_throughput, df, **kwargs):\n",
    "#     def get_record(df, **kwargs):\n",
    "#         row_count = df.shape[0]\n",
    "#         condition = pd.Series([True] * row_count)\n",
    "#         for k, v in kwargs.items():\n",
    "#             condition = condition & (df[k] == v)\n",
    "#         return df[condition]\n",
    "\n",
    "#     def get_closest(df, **kwargs):\n",
    "#         no_match = {}\n",
    "#         row_count = df.shape[0]\n",
    "#         condition = pd.Series([True] * row_count)\n",
    "#         for k, v in kwargs.items():\n",
    "#             if v in df[k].unique():\n",
    "#                 condition = condition & (df[k] == v)\n",
    "#             else:\n",
    "#                 no_match[k] = v\n",
    "\n",
    "#         # With matched dimensions\n",
    "#         data_points = [(df[condition], {})]\n",
    "\n",
    "#         # For each of the non-matched dimension\n",
    "#         for k, v in no_match.items():\n",
    "#             tmp = []\n",
    "#             for dp, limits in data_points:\n",
    "#                 uni_val = sorted(dp[k].unique())\n",
    "\n",
    "#                 low, high = -1, -1\n",
    "#                 if v < uni_val[0]:\n",
    "#                     high = uni_val[0]\n",
    "#                 elif v > uni_val[-1]:\n",
    "#                     low = uni_val[-1]\n",
    "#                 else:\n",
    "#                     for idx in range(len(uni_val[:-1])):\n",
    "#                         if uni_val[idx] < v and uni_val[idx+1] > v:\n",
    "#                             high = uni_val[idx+1]\n",
    "#                             low = uni_val[idx]\n",
    "#                             break\n",
    "#                 assert not (low == -1 and high == -1)\n",
    "\n",
    "#                 less_tmp = dp[dp[k] == (low if low != -1 else uni_val[0])]\n",
    "#                 more_tmp = dp[dp[k] == (high if high != -1 else uni_val[-1])]\n",
    "#                 if low == -1:\n",
    "#                     less_tmp[k] = 0\n",
    "#                 if high == -1:\n",
    "#                     more_tmp[k] = sys.maxsize # Big enough for BW in GB/s or throughput in GFLOPS\n",
    "#                 tmp_limits = limits.copy()\n",
    "#                 tmp_limits[k] = (low, high)\n",
    "#                 tmp.append((less_tmp, tmp_limits))\n",
    "#                 tmp.append((more_tmp, tmp_limits))\n",
    "#             data_points = tmp\n",
    "\n",
    "#         return data_points\n",
    "\n",
    "#     #####################\n",
    "#     #       |     X |\n",
    "#     #    |==O=======|\n",
    "#     #    |  |       |\n",
    "#     #    |  |-------O-\n",
    "#     #    |     X     |\n",
    "#     #    O           |\n",
    "#     #    |===========O\n",
    "#     #    |  X        |\n",
    "#     #####################\n",
    "\n",
    "#     record = get_record(df, **kwargs)\n",
    "#     if not record.empty:\n",
    "#         return record[\"kernel_runtime\"].iloc[0]\n",
    "#     data_points = get_closest(df, **kwargs)\n",
    "\n",
    "#     effective_flops = 0.0\n",
    "#     effective_bw = 0.0\n",
    "#     batch_size = kwargs[\"batch_size\"]\n",
    "#     M = kwargs[\"M\"]\n",
    "#     N = kwargs[\"N\"]\n",
    "#     K = kwargs[\"K\"]\n",
    "#     for dp, limits in data_points:\n",
    "#         dp_flops_contrib = 0.0\n",
    "#         dp_bw_contrib = 0.0\n",
    "\n",
    "#         # An idea, if zero occurs, it's always the bottleneck. A zero dominates all peaks.\n",
    "#         zero_exists = False\n",
    "#         peak_exists = False\n",
    "#         for k, v in limits.items():\n",
    "#             metric = dp[k].iloc[0]\n",
    "#             if metric == 0:\n",
    "#                 zero_exists = True\n",
    "#             elif metric == sys.maxsize:\n",
    "#                 peak_exists = True\n",
    "\n",
    "#         for k, v in limits.items():\n",
    "#             low, high = v\n",
    "#             if high == -1: # Reaching the peak, taking average\n",
    "#                 ratio_l, ratio_h = 0.5, 0.5\n",
    "#             elif low == -1: # Reaching the bottom, set low as 0\n",
    "#                 ratio_l, ratio_h = kwargs[k] / high, (high - kwargs[k]) / high\n",
    "#             else: # Normal, weighted\n",
    "#                 ratio_l, ratio_h = (kwargs[k] - low) / (high - low), (high - kwargs[k]) / (high - low)\n",
    "#             # Edge cases: when more than one metric is MAX/0\n",
    "\n",
    "#             metric = dp[k].iloc[0]\n",
    "#             if zero_exists:\n",
    "#                 throughput = 0\n",
    "#                 dram_bw = 0\n",
    "#                 ratio = 0\n",
    "#             elif peak_exists:\n",
    "#                 throughput = peak_throughput\n",
    "#                 dram_bw = peak_dram_bw\n",
    "#                 ratio = ratio_h\n",
    "#             elif metric == low:\n",
    "#                 throughput = (dp[\"batch_size\"] * dp[\"M\"] * dp[\"N\"] * dp[\"K\"]).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 # GFLOPS\n",
    "#                 dram_bw = (dp[\"batch_size\"] * (dp[\"M\"] * dp[\"K\"] + dp[\"K\"] * dp[\"N\"] + dp[\"M\"] * dp[\"N\"])).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 * 4 # GB/s\n",
    "#                 ratio = ratio_l \n",
    "#             elif metric == high:\n",
    "#                 throughput = (dp[\"batch_size\"] * dp[\"M\"] * dp[\"N\"] * dp[\"K\"]).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 # GFLOPS\n",
    "#                 dram_bw = (dp[\"batch_size\"] * (dp[\"M\"] * dp[\"K\"] + dp[\"K\"] * dp[\"N\"] + dp[\"M\"] * dp[\"N\"])).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 * 4 # GB/s\n",
    "#                 ratio = ratio_h\n",
    "\n",
    "#             dp_flops_contrib += throughput * ratio\n",
    "#             dp_bw_contrib += dram_bw * ratio\n",
    "\n",
    "#         effective_flops += dp_flops_contrib / len(limits.items())\n",
    "#         effective_bw += dp_bw_contrib / len(limits.items())\n",
    "\n",
    "#     effective_flops /= len(data_points) / 2\n",
    "#     effective_bw /= len(data_points) / 2\n",
    "\n",
    "#     FLOP = kwargs[\"batch_size\"] * kwargs[\"M\"] * kwargs[\"N\"] * kwargs[\"K\"]\n",
    "#     DRAM_bytes = kwargs[\"batch_size\"] * (kwargs[\"M\"] * kwargs[\"K\"] + kwargs[\"K\"] * kwargs[\"N\"] + kwargs[\"M\"] * kwargs[\"N\"]) * 4\n",
    "#     predicted_runtime = max(FLOP / effective_flops, DRAM_bytes / effective_bw) / 1000\n",
    "\n",
    "#     return predicted_runtime\n",
    "\n",
    "# # 5829 vs 7548\n",
    "# fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=256, M=512, N=1000, K=400)"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "573647873313879"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200830-210251",
   "description": "Analyze a two-iteration trace file generated by ATC ; extract multi-level (module/op) runtime breakdown and major input output shapes.",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/zhongyilin/fbsource/fbcode/bento/kernels/local/zhongyilin/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "320128",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "643445799644945",
   "tags": "FBLSim,CEA,ATC,dyno,gputrace,trace",
   "tasks": "",
   "title": "Runtime Prediction"
  },
  "kernelspec": {
   "display_name": "zhongyi",
   "language": "python",
   "name": "zhongyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
