{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, sys\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "from pprint import pprint\n",
    "from utils import histogram, PM_HOME\n",
    "sys.path.insert(0, PM_HOME)\n",
    "from analysis.inference import *\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, error = infer_concat()\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memcpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, error = infer_memcpy()\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, error = infer_from_model(op_type=\"transpose\")\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, error = infer_from_model(op_type=\"tril\", backward=False)\n",
    "_ = histogram(error)\n",
    "_, error = infer_from_model(op_type=\"tril\", backward=True)\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Lookup Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sizes\n",
    "_, error = infer_el(backward=False, big=False, hit_rate_estimation=False)\n",
    "_ = histogram(error)\n",
    "_, error = infer_el(backward=False, big=False, hit_rate_estimation=True)\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big sizes\n",
    "_, error = infer_el(backward=False, big=True, hit_rate_estimation=False)\n",
    "_ = histogram(error)\n",
    "_, error = infer_el(backward=False, big=True, hit_rate_estimation=False)\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Lookup Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sizes\n",
    "_, error = infer_el(backward=True, big=False, hit_rate_estimation=False)\n",
    "_ = histogram(error)\n",
    "_, error = infer_el(backward=True, big=False, hit_rate_estimation=True)\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big sizes\n",
    "_, error = infer_el(backward=True, big=True, hit_rate_estimation=False)\n",
    "_ = histogram(error)\n",
    "_, error = infer_el(backward=True, big=True, hit_rate_estimation=True)\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, error = infer_from_model(op_type=\"fully_connected\")\n",
    "_ = histogram(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = max(t_Mem, t_MemLatency, t_Compute)\n",
    "# num_main_loop_iteration = K / blkK\n",
    "\n",
    "\n",
    "# t_Mem = t_Prologue + (t_MainLoop_Mem + t_Epilogue) * (# CTA per SM)\n",
    "#     t_Prologue = (t_DRAM_2_Regs_output) + (t_Regs_2_SMEM_output + t_SMEM_2_Regs_input) [t_GLS + t_SAS]\n",
    "#         t_DRAM_2_Regs_output = LAT_DRAM + (blkM * blkN) / (BW_DRAM / # SM) --------------- [TBD]\n",
    "#             LAT_DRAM --------------------------------- Look up the datasheet. [Good]\n",
    "# \n",
    "#         t_Regs_2_SMEM_output = LAT_SMEM + (blkM * blkN) / (BW_SMEM) ---------------------- [TBD]\n",
    "#             LAT_SMEM --------------------------------- Look up the datasheet. [Good]\n",
    "# \n",
    "#         t_SMEM_2_Regs_input = (blkWM + blkWN) * blkK * (# warps) / (BW_SMEM)\n",
    "#             blkWM,WN --------------------------------- [TBD]\n",
    "#             (# warps) -------------------------------- [TBD]\n",
    "# \n",
    "#         (# CTA) -------------------------------------- Calculated\n",
    "# \n",
    "#     t_Epilogue = (blkM * blkN) / BW_DRAM ------------------------------------------------- [Good]\n",
    "#     t_MainLoop_Mem = max(trf_L1, trf_L2, trf_DRAM) * num_main_loop_iteration --------------[Good]\n",
    "#         trf_L1,L2,DRAM ------------------------------- Calculated [TODO]\n",
    "\n",
    "\n",
    "# t_DRAM_LAT = t_Prologue + (tGLS + max(t_CS / blkK, t_SAS / blkK) * num_main_loop_iteration + t_Epilogue) * (# CTA per SM) / (# Active CTA)\n",
    "#     t_Prologue: [same as above]\n",
    "#     t_CS = (blkM * blkN * blkK) / BW_MAC\n",
    "#     t_SAS = (LAT_SMEM + (blkM * blkN) + (blkWM + blkWN) * blkK * (# warps)) / (BW_SMEM)\n",
    "#     t_Epilouge: [same as above]\n",
    "\n",
    "# t_Compute_or_SMEM = t_Prologue + (t_MainLoop_Compute + t_Epilogue) * (# CTA per SM)\n",
    "#     t_Prologue: [same as above]\n",
    "#     t_MainLoop_Compute = max(t_CS, t_SAS) * num_main_loop_iteration\n",
    "#         t_CS: [same as above]\n",
    "#         t_SAS: [same as above]\n",
    "#     t_Epilouge: [same as above]\n",
    "\n",
    "\n",
    "# blkM,N,K --------------------------------------------- Predicted from shape [TODO]\n",
    "# (# SM) ----------------------------------------------- Given\n",
    "# BW_L1/L2/DRAM/MAC ------------------------------------ Measured\n",
    "# (# CTA per SM) = (# CTA / # SM) ---------------------- [Good]\n",
    "# (# Active CTA per SM) -------------------------------- Predicted from kernel names and shapes [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision tree\n",
    "# from sklearn import tree\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# lb = LabelEncoder()\n",
    "\n",
    "# name_and_size = fc_data[(fc_data['kernel_name'] != 'gemv2T_kernel') & (fc_data['kernel_name'] != 'splitKreduce_kernel')][['batch_size', 'M', 'N', 'K', 'kernel_name']]\n",
    "# name_and_size[\"kernel_name_code\"] = lb.fit_transform(name_and_size[\"kernel_name\"])\n",
    "# X = name_and_size[['batch_size', 'M', 'N', 'K']]\n",
    "# y = name_and_size['kernel_name_code']\n",
    "# clf = clf.fit(X, y)\n",
    "# # plt.figure(figsize=(12, 8))\n",
    "# # tree.plot_tree(clf)\n",
    "# # plt.show()\n",
    "\n",
    "# def predict_kernel_name(*shape):\n",
    "#     n = clf.predict([*shape])\n",
    "#     return list(lb.inverse_transform(n))[0]\n",
    "# predict_kernel_name([1, 64, 64, 4096])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_blkMNK_from_kernel_name(kernel_name):\n",
    "#     splitted = kernel_name.split('_')\n",
    "#     x = splitted[2]\n",
    "#     blkN = int(x.split('x')[0])\n",
    "#     blkM = int(x.split('x')[1])\n",
    "#     # blkK is either 4 or 8. Assuming 4 for (<= 64) and 8 for others. No evidence. TODO: Confirm this.\n",
    "#     if blkM <= 64:\n",
    "#         blkK = 4\n",
    "#     else:\n",
    "#         blkK = 8\n",
    "#     return blkM, blkN, blkK\n",
    "\n",
    "# # batch_size, M, N, K = 1, 64, 64, 4096\n",
    "# # Assuming blkM,N,K are known. Find someone to confirm.\n",
    "# # A potential idea: use B, M, N, K to predict the kernel to be used.\n",
    "# def get_blks(batch_size, M, N, K, clf=None):\n",
    "#     if clf is None:\n",
    "#         row = trunc[(trunc['batch_size'] == batch_size) & (trunc['M'] == M) & (trunc['N'] == N) & (trunc['K'] == K)]\n",
    "#         if row.empty:\n",
    "#             return -1, -1, -1, -1, -1, -1\n",
    "#         kernel_name = str(row['kernel_name'])\n",
    "#     else:\n",
    "#         kernel_name = list(lb.inverse_transform(clf.predict([[batch_size, M, N, K]])))[0]\n",
    "#     blkM, blkN, blkK = get_blkMNK_from_kernel_name(kernel_name)\n",
    "        \n",
    "#     # Confirmed\n",
    "#     block_x = div_round_up(N, blkN)\n",
    "#     block_y = div_round_up(M, blkM)\n",
    "    \n",
    "#     # # Using batch_size as block_z for now. Usually works for BMM.\n",
    "#     # block_z = batch_size if batch_size > 1 else int(row['block_z'])\n",
    "#     block_z = batch_size\n",
    "\n",
    "#     return blkM, blkN, blkK, block_x, block_y, block_z\n",
    "\n",
    "# # Verify blkMNK and block_xyz correctness. TODO: Need a test set for prediction of blkM and blkN.\n",
    "# predicted_blks = bmm_data.apply(lambda x: get_blks(x['batch_size'], x['M'], x['N'], x['K'], clf), axis=1)\n",
    "# predicted_blks = pd.DataFrame(predicted_blks.tolist(), index=predicted_blks.index, columns =['blkM', 'blkN', 'blkK', 'block_x', 'block_y', 'block_z'])\n",
    "# actual_blkMNK = bmm_data.apply(lambda x: get_blkMNK_from_kernel_name(x['kernel_name']), axis=1)\n",
    "# actual_blkMNK = pd.DataFrame(actual_blkMNK.tolist(), index=actual_blkMNK.index, columns =['blkM', 'blkN', 'blkK'])\n",
    "\n",
    "# block_x_error = sum(predicted_blks['block_x'] != bmm_data['block_x']) / len(predicted_blks) * 100.0\n",
    "# block_y_error = sum(predicted_blks['block_y'] != bmm_data['block_y']) / len(predicted_blks) * 100.0\n",
    "# block_z_error = sum(predicted_blks['block_z'] != bmm_data['block_z']) / len(predicted_blks) * 100.0\n",
    "\n",
    "# blkM_error = sum(predicted_blks['blkM'] != actual_blkMNK['blkM']) / len(predicted_blks) * 100.0\n",
    "# blkN_error = sum(predicted_blks['blkN'] != actual_blkMNK['blkN']) / len(predicted_blks) * 100.0\n",
    "# blkK_error = sum(predicted_blks['blkK'] != actual_blkMNK['blkK']) / len(predicted_blks) * 100.0\n",
    "\n",
    "# print(\"Block xyz error rates: {}%, {}%, {}%\".format(block_x_error, block_y_error, block_z_error))\n",
    "# print(\"blk xyz error rates: {}%, {}%, {}%\".format(blkM_error, blkN_error, blkK_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kernels and regs/smem usages are mapped one by one\n",
    "# # Occupancy (CTA per SM): warps per SM, blocks per SM, regs per SM, SMEM per SM\n",
    "# # bmm: only 128x64 and 128x128 kernels\n",
    "# # addmm: includes all others\n",
    "\n",
    "# info = {}\n",
    "# # for kernel in fc_data['kernel_name'].unique():\n",
    "# #     print(kernel)\n",
    "# #     df = fc_data[fc_data['kernel_name'] == kernel]\n",
    "# #     print('df length:', len(df))\n",
    "# #     print(df['thread_x'].unique(), df['regs'].unique(), df['smem'].unique())\n",
    "# # print(\"************\")\n",
    "\n",
    "# for kernel in fc_data['kernel_name'].unique():\n",
    "#     df = fc_data[fc_data['kernel_name'] == kernel]\n",
    "#     num_thread_x = int(df['thread_x'].unique()[0])\n",
    "#     num_warps_per_CTA = num_thread_x // 32\n",
    "#     num_regs_per_thread = int(df['regs'].unique()[0])\n",
    "#     SMEM_per_CTA = int(df['smem'].unique()[0])\n",
    "#     blkM, blkN, _ = get_blkMNK_from_kernel_name(kernel)\n",
    "#     tmp = num_warps_per_CTA\n",
    "#     if kernel.endswith('sliced1x4_tn'):\n",
    "#         tmp = tmp // 4\n",
    "#     blkWM, blkWN = blkM, blkN\n",
    "#     while tmp > 1:\n",
    "#         if blkWM > blkWN:\n",
    "#             blkWM = blkWM // 2\n",
    "#         else:\n",
    "#             blkWN = blkWN // 2\n",
    "#         tmp = tmp // 2\n",
    "#     tmp = {}\n",
    "#     tmp['num_warps_per_CTA'] = num_warps_per_CTA\n",
    "#     tmp['num_active_CTA'] = min(maximum_warps_per_SM // num_warps_per_CTA, maximum_CTA_per_SM, regs_size // (num_thread_x * num_regs_per_thread), SMEM_size // SMEM_per_CTA)\n",
    "#     tmp['blkWM'] = blkWM\n",
    "#     tmp['blkWN'] = blkWN\n",
    "#     info[kernel] = tmp\n",
    "\n",
    "# #     print(kernel)\n",
    "# #     occ = df['achieved_occupancy']\n",
    "# #     histogram(occ / (tmp['num_active_CTA'] / maximum_CTA_per_SM), perc=False, bins=[0.0, 0.5, 0.8, 0.9, 1.0, 1.1, 1.2, 1.5, 2.0])\n",
    "# #     print(len(occ))\n",
    "\n",
    "# # occ = fc_data['achieved_occupancy']\n",
    "# # num_active_CTA_df = fc_data['kernel_name'].map(lambda x: info[x]['num_active_CTA'])\n",
    "# # histogram(occ / (num_active_CTA_df / maximum_CTA_per_SM), perc=False, bins=[0.0, 0.5, 0.8, 0.9, 1.0, 1.1, 1.2, 1.5, 2.0])\n",
    "# # print(len(occ))\n",
    "    \n",
    "# pprint(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # L2 traffic: almost never bound on this. Set to 0. Evidence: L2 utilization has never been higher than 2.\n",
    "# L2_traffic = 0\n",
    "\n",
    "# def get_DRAM_traffic(batch_size, M, N, K, blkM, blkN, num_CTA, num_active_CTA):\n",
    "#     # Num of waves\n",
    "#     num_waves = div_round_up(num_CTA // batch_size, num_SM * num_active_CTA) # Num of CTA waves per batch\n",
    "\n",
    "#     A = M * K * 4\n",
    "#     B = N * K * 4\n",
    "\n",
    "#     num_block_per_A_col = div_round_up(M, blkM) # Num of CTA block in a block column of A\n",
    "#     num_block_per_B_row = div_round_up(N, blkN) # Num of CTA block in a block row of B\n",
    "\n",
    "#     # Possibility 1: Along rows\n",
    "#     if num_SM * num_active_CTA < num_block_per_B_row:\n",
    "#         DRAM_traffic_row = (B * num_block_per_A_col + A + blkM * num_waves * K * 4)\n",
    "#     else:\n",
    "#         DRAM_traffic_row = (B * num_waves + A + blkM * num_waves * K * 4)\n",
    "\n",
    "#     # Possibility 2: Along columns\n",
    "#     if num_SM * num_active_CTA < num_block_per_A_col: # One CTA batch doesn't fill a full column of A\n",
    "#         DRAM_traffic_col = (A * num_block_per_B_row + B + blkN * num_waves * K * 4)\n",
    "#     else:\n",
    "#         DRAM_traffic_col = (A * num_waves + B + blkN * num_waves * K * 4)\n",
    "\n",
    "#     # Take the min of them\n",
    "#     DRAM_traffic = batch_size * min(DRAM_traffic_row, DRAM_traffic_col)\n",
    "\n",
    "#     return DRAM_traffic\n",
    "# # get_DRAM_traffic(batch_size=1, M=64, N=64, K=4096, blkM=blkM, blkN=blkN, num_CTA=4)\n",
    "# # dram_trf = get_DRAM_traffic(batch_size=512, M=1024, N=512, K=4096, blkM=64, blkN=128, num_CTA=32768)\n",
    "# # df = bmm_data[(bmm_data['batch_size'] == 512) & (bmm_data['M'] == 1024) & (bmm_data['N'] == 512) & (bmm_data['K'] == 4096)]\n",
    "# # print(dram_trf / float(df['dram_read_transactions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def estimate_runtime(batch_size, M, N, K, clf=None, use_th_peak=True):\n",
    "#     kernel_name = predict_kernel_name([batch_size, M, N, K])\n",
    "#     blkM, blkN, blkK, block_x, block_y, block_z = get_blks(batch_size, M, N, K, clf=clf) # TODO: fix block_z for batch_size = 1\n",
    "#     blkWM, blkWN = info[kernel_name]['blkWM'], info[kernel_name]['blkWN']\n",
    "#     num_CTA = block_x * block_y * block_z\n",
    "#     throughput = peak_throughput if use_th_peak else corrected_peak_throughput\n",
    "#     num_main_loop_iteration = div_round_up(num_CTA, num_SM)\n",
    "#     num_warps = info[kernel_name]['num_warps_per_CTA']\n",
    "#     num_active_CTA = info[kernel_name]['num_active_CTA']\n",
    "#     DRAM_traffic = get_DRAM_traffic(batch_size, M, N, K, blkM, blkN, num_CTA, num_active_CTA) # Total\n",
    "\n",
    "#     LAT_DRAM = 1029 / frequency # See https://arxiv.org/pdf/1804.06826.pdf page 20\n",
    "#     LAT_SMEM = 19 / frequency # See https://arxiv.org/pdf/1804.06826.pdf page 19\n",
    "# #     t_DRAM_2_Regs_output = LAT_DRAM + (DRAM_traffic / (block_x * block_y * block_z)) * 4 / (peak_DRAM_BW / num_SM) / 1000\n",
    "# #     t_Regs_2_SMEM_output = LAT_SMEM + (blkM * blkN) * 4 / (peak_SMEM_BW) / 1000 # Good\n",
    "# #     t_SMEM_2_Regs_input = (blkWM + blkWN) * blkK * 4 * (num_warps) / peak_SMEM_BW / 1000 # Good\n",
    "\n",
    "# #     t_GLS = t_DRAM_2_Regs_output + t_Regs_2_SMEM_output\n",
    "#     t_SAS = (blkWM + blkWN) * blkK * 4 * (num_warps) / peak_SMEM_BW / 1000\n",
    "#     t_CS = blkM * blkN * blkK / throughput / 1000\n",
    "\n",
    "#     t_Prologue = (LAT_DRAM + (blkM * blkN) * 4 / (peak_DRAM_BW / num_SM) / 1000) + \\\n",
    "#                     (LAT_SMEM + (blkM * blkN) * 4 / (peak_SMEM_BW / 2) / 1000) + \\\n",
    "#                     ((blkWM + blkWN) * blkK * 4 * (num_warps) / (peak_SMEM_BW / 2) / 1000)\n",
    "#     t_Epilogue = (blkM * blkN) * 4 / peak_DRAM_BW / 1000\n",
    "\n",
    "#     # Bounded by CS or SMEM\n",
    "#     t_MainLoop_Compute = max(t_CS, t_SAS) * div_round_up(K, blkK) # Good\n",
    "#     t_Compute_or_SMEM = t_Prologue + (t_MainLoop_Compute + t_Epilogue) * num_main_loop_iteration # Good\n",
    "\n",
    "#     # Bounded by latency\n",
    "#     t_MainLoop_Latency = max(t_CS, t_SAS) * num_active_CTA * div_round_up(K, blkK)\n",
    "#     t_Latency = t_Prologue + (DRAM_traffic / peak_DRAM_BW / 1000 + (t_MainLoop_Latency + t_Epilogue) * num_main_loop_iteration) / num_active_CTA\n",
    "\n",
    "#     # Bounded by memory traffic (only considering DRAM for now)\n",
    "#     t_MainLoop_Mem = max(L2_traffic / peak_L2_BW / 1000, DRAM_traffic / peak_DRAM_BW / 1000)\n",
    "#     t_Mem = t_Prologue + t_MainLoop_Mem + t_Epilogue * num_main_loop_iteration\n",
    "\n",
    "#     # Final time\n",
    "#     t_final = max(t_Compute_or_SMEM, t_Latency, t_Mem)\n",
    "    \n",
    "#     # Bound factor\n",
    "#     if t_final == t_Compute_or_SMEM:\n",
    "#         bound = 'compute'\n",
    "#     elif t_final == t_Latency:\n",
    "#         bound = 'latency'\n",
    "#     else:\n",
    "#         bound = 'memory'\n",
    "    \n",
    "#     return t_final, bound, DRAM_traffic\n",
    "# estimate_runtime(batch_size=1, M=64, N=64, K=4096, clf=clf, use_th_peak=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhongyi",
   "language": "python",
   "name": "zhongyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
