{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.core.display import display, HTML\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import pandas as pd\n",
    "import json, sys, re, os\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "# plt.rcParams['figure.max_open_warning'] = 50\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + \"/../../\")\n",
    "from analysis.utils import PM_HOME, GPU_NAME, CPU_EVENT_OVERHEAD, GPU_EVENT_OVERHEAD, KERNEL_LAUNCH_LENGTH\n",
    "from analysis.trace_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "model_name = \"DLRM_default\" # \"DLRM_MLPerf\"\n",
    "num_gpus = 1\n",
    "batch_size = 2048\n",
    "iters = 10\n",
    "\n",
    "trace_file = '{}/data/{}/e2e/{}/{}_{}.json'.format(PM_HOME, GPU_NAME, model_name, num_gpus, batch_size)\n",
    "print(trace_file)\n",
    "\n",
    "trimmed_trace_file = trim_trace_by_num_iter(trace_file, iters=iters, trimmed_file='/tmp/{}_{}_{}.json'.format(model_name, num_gpus, batch_size))\n",
    "with open(trimmed_trace_file) as f:\n",
    "    trace = json.load(f)\n",
    "\n",
    "print(trimmed_trace_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLRM with data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roots, cc, _, corrected_start_time, corrected_end_time, sum_skipped_intervals = process_event_hierarchy(trace['traceEvents'], skip_module=False)\n",
    "print('Num of events: {}, num of root events: {}, num of caller/callee pairs: {}'.format(len(trace['traceEvents']), len(roots), len(cc)))\n",
    "print('Sum of dataloading time: {}'.format(sum_skipped_intervals))\n",
    "print(\"Corrected start time: \", corrected_start_time, \", corrected end time: \", corrected_end_time)\n",
    "host_runtime = corrected_end_time - corrected_start_time - sum_skipped_intervals\n",
    "# ---\n",
    "# device_runtime, device_start_delay = get_device_runtime_and_start_delay(cc, corrected_start_time)\n",
    "# print(\"Device start delay: \", device_start_delay)\n",
    "# ---\n",
    "device_runtime = host_runtime\n",
    "# ---\n",
    "print(\"Host runtime: \", host_runtime)\n",
    "print(\"Device runtime: \", device_runtime)\n",
    "ops = []\n",
    "get_operators(roots, ops)\n",
    "QPS = 1000000 / host_runtime * iters * 2048\n",
    "print(f\"QPS: {QPS:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device runtime breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_device_runtime = get_device_runtime(ops, cc) # dict: op ex_id -> all its device calls and stats\n",
    "dt_breakdown = device_runtime_breakdown(roots, op_device_runtime, depth=0)\n",
    "truncate_count = 10\n",
    "flatten = {}\n",
    "for stream, v in dt_breakdown.items():\n",
    "    # print(\"Stream: {}\".format(stream))\n",
    "    flatten[stream] = {}\n",
    "    get_major_device_results(device_runtime, dt_breakdown[stream], flatten[stream])\n",
    "print(op_device_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stream, v in dt_breakdown.items():\n",
    "    print(\"Stream: {}\".format(stream))\n",
    "    print_major_device_results(device_runtime, dt_breakdown[stream], flatten[stream], truncate_count=truncate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from matplotlib import cm\n",
    "# cs=cm.Set1([1, 3, 2, 25, 4, 5, 6, 7, 8, 11])\n",
    "\n",
    "# def plot_pie_chart(flatten, key=\"total\", truncate_count=100, depth=0):\n",
    "#     d = flatten[key]\n",
    "    \n",
    "#     # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "#     stats = sorted(d[\"subs\"].items(), key=lambda x: x[1], reverse=True)\n",
    "#     labels = [x[0] for x in stats]\n",
    "#     runtime = [x[1] for x in stats]\n",
    "#     explode = np.zeros(len(runtime))\n",
    "#     if len(explode) > 2:\n",
    "#         explode[1] = 0.1\n",
    "\n",
    "#     fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "#     wedges, texts = ax1.pie(runtime, explode=explode, shadow=True, startangle=90, colors=cs)\n",
    "#     ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "#     ax1.set_title(key)\n",
    "    \n",
    "#     ax1.legend(wedges, zip(labels, [\"{:.2f}%\".format(r / d[\"runtime\"] * 100) for r in runtime]),\n",
    "#           title=\"Breakdown\",\n",
    "#           loc=\"center left\",\n",
    "#           bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "#           fontsize=14)\n",
    "    \n",
    "#     for label in labels:\n",
    "#         if label in flatten:\n",
    "#             plot_pie_chart(flatten, key=label, truncate_count=truncate_count, depth=depth+1)\n",
    "\n",
    "#     if depth == 0:\n",
    "#         plt.show()\n",
    "\n",
    "# for stream, v in flatten.items():\n",
    "#     print(\"########################\")\n",
    "#     print(\"STREAM: {}\".format(stream))\n",
    "#     print(\"########################\")\n",
    "#     plot_pie_chart(v, truncate_count=truncate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_name_to_legend(name):\n",
    "    if 'gemm' in name:\n",
    "        return 'gemm'\n",
    "    if 'gemv' in name:\n",
    "        return 'gemv'\n",
    "    if 'Memset' in name:\n",
    "        return 'Memset'\n",
    "    if 'Memcpy' in name:\n",
    "        return 'Memcpy'\n",
    "    if 'vectorized_elementwise' in name:\n",
    "        return 'elwt'\n",
    "    if 'unrolled_elementwise' in name:\n",
    "        return 'permute'\n",
    "    if 'embedding_forward' in name:\n",
    "        return 'ELF'\n",
    "    if 'embedding_backward' in name:\n",
    "        return 'ELB'\n",
    "    if 'splitK' in name:\n",
    "        return 'splitK'\n",
    "    if 'reduce_kernel' in name:\n",
    "        return 'reduce'\n",
    "    if 'CatArray' in name:\n",
    "        return 'CatCopy'\n",
    "    if 'indexing' in name:\n",
    "        return 'IdxBwd'\n",
    "    return name[:8]\n",
    "\n",
    "runtime_no_pf = -1\n",
    "log_file = \"{}/data/{}/e2e/{}/{}_{}.log\".format(PM_HOME, GPU_NAME, model_name, num_gpus, batch_size)\n",
    "if os.path.exists(log_file):\n",
    "    for line in open(log_file, 'r'):\n",
    "        if re.search(\"Overall per-batch\", line):\n",
    "            runtime_no_pf = float(line.split(' ')[4]) * 1000 * iters # us\n",
    "\n",
    "def plot_bar_chart(flatten, key=\"total\", truncate_count=100, depth=0):\n",
    "    per_op = {}\n",
    "    total = 0.0\n",
    "    for k, v in flatten.items():\n",
    "        if k == 'total' or 'DLRM ' in k[0] or 'module' in k[0]: # Skip all labels\n",
    "            continue\n",
    "        k0 = k[0] if '#' not in k[0] else k[0].split('#')[0]\n",
    "        if k0 not in per_op.keys():\n",
    "            per_op[k0] = 0.0\n",
    "        per_op[k0] += v['runtime']\n",
    "        total += v['runtime']\n",
    "    \n",
    "    t = {}\n",
    "    for k, v in flatten.items():\n",
    "        if k == 'total' or 'DLRM ' in k[0] or 'module' in k[0]: # Skip all labels\n",
    "            continue\n",
    "        k0 = k[0] if '#' not in k[0] else k[0].split('#')[0]\n",
    "        if k0 not in t.keys(): # Per Op\n",
    "            t[k0] = []\n",
    "        a = {}\n",
    "        total = 0.0\n",
    "        for kk, vv in v['subs'].items():\n",
    "            a[kk] = vv\n",
    "            total += vv\n",
    "        a = {x: y / total for x, y in a.items()}\n",
    "        t[k0].append(a)\n",
    "    so = sorted(t.items(), key=lambda x: per_op[x if isinstance(x, str) else x[0]], reverse=True)\n",
    "    pprint([len(s[1]) for s in so])\n",
    "    so = [list(x) for x in so]\n",
    "\n",
    "    for s in so:\n",
    "        keys = set()\n",
    "        to_be_deleted = []\n",
    "        # Find duplicate kernel combinations\n",
    "        for idx, b in enumerate(s[1]):\n",
    "            k = []\n",
    "            for x in b.keys():\n",
    "                if 'volta_sgemm' in x[0]:\n",
    "                    xx = 'volta_sgemm'\n",
    "                elif 'maxwell_sgemm' in x[0]:\n",
    "                    xx = 'maxwell_sgemm'\n",
    "                elif isinstance(x, str):\n",
    "                    xx = x\n",
    "                else:\n",
    "                    xx = x[0]\n",
    "                k.append(xx)\n",
    "            k = tuple(sorted(k))\n",
    "            if k in keys:\n",
    "                to_be_deleted.append(idx)\n",
    "            else:\n",
    "                keys.add(k)\n",
    "        # Delete duplicate kernel combinations\n",
    "        for idx in sorted(to_be_deleted, reverse=True):\n",
    "            del s[1][idx]\n",
    "    pprint([len(s[1]) for s in so])\n",
    "\n",
    "    nrows, ncols = 3, 8\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=8, figsize=(20, 13))\n",
    "    xx, yy = 0, 0\n",
    "    rects, texts = [], []\n",
    "    for s in so:\n",
    "        num_variants = len(s[1])\n",
    "        for idx, b in enumerate(s[1]):\n",
    "            ax = plt.subplot(nrows, ncols, 1+xx+idx+yy*ncols)\n",
    "            p = sorted(b.items(), key=lambda x: x[1], reverse=True)\n",
    "            kernel_name = [kernel_name_to_legend(x[0] if isinstance(x[0], str) else x[0][0]) for x in p]\n",
    "            perc = [x[1] for x in p]\n",
    "            df = pd.DataFrame([perc], columns=kernel_name)\n",
    "            _ = df.plot(stacked=True, kind='bar', ax=ax, cmap='tab20b')\n",
    "            ax.get_legend().remove()\n",
    "            if xx+idx == 0:\n",
    "                ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "                ax.set_yticklabels(['{:,.0%}'.format(x) for x in [0, 0.2, 0.4, 0.6, 0.8, 1.0]])\n",
    "            else:\n",
    "                ax.set_yticks([])\n",
    "                ax.set_yticklabels([])\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_ylim((0.0, 1.0))\n",
    "            ax.legend(bbox_to_anchor=(1.02, 1.04), loc='upper left', frameon=False, fontsize=14)\n",
    "            if idx == (len(s[1]) - 1) / 2:\n",
    "                ax.set_title(s[0], loc='left', fontsize=(16 if len(s[0]) < 20 else 12))\n",
    "            \n",
    "        # Borders\n",
    "        llc_x = 1.0 / ncols * (xx) + (0.014 if xx != 0 else 0)\n",
    "        llc_y = 1.0 / nrows * (nrows-yy-1) + 0.01\n",
    "        rx = 1.0 / ncols * (num_variants) + (0.014 if xx == 0 else 0)\n",
    "        ry = 1.0 / nrows\n",
    "        rects.append(plt.Rectangle(\n",
    "            (llc_x, llc_y), rx, ry, fill=False, color=\"k\", lw=2, zorder=1000, transform=fig.transFigure, figure=fig\n",
    "        ))\n",
    "\n",
    "        # Subplot position\n",
    "        xx += num_variants\n",
    "        if xx >= ncols:\n",
    "            xx = 0\n",
    "            yy += 1\n",
    "\n",
    "    # Hardcoded for now\n",
    "    axes[2,7].set_axis_off()\n",
    "    \n",
    "    fig.patches.extend(rects)\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['figure.figsize'] = [20, 13]\n",
    "    plt.savefig('{}/data/{}/e2e/{}/dominating_op_breakdown.pdf'.format(PM_HOME, GPU_NAME, model_name), bbox_inches='tight')\n",
    "    plt.savefig('{}/data/{}/e2e/{}/dominating_op_breakdown.png'.format(PM_HOME, GPU_NAME, model_name), bbox_inches='tight')\n",
    "\n",
    "for stream, v in flatten.items():\n",
    "    print(\"########################\")\n",
    "    print(\"STREAM: {}\".format(stream))\n",
    "    print(\"########################\")\n",
    "    plot_bar_chart(v, truncate_count=truncate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Type 1 overhead: between two op calls\n",
    "# Type 2 overhead: before the first device call, op-specific\n",
    "# Type 3 overhead: after the last device call, op-specific\n",
    "# Type 4 overhead: kernel launches themselves, kernel-launch-type-specific\n",
    "# Type 5 overhead: sum of gaps between kernel launches, op-specific\n",
    "overheads = {'independent': {}}\n",
    "overheads['independent']['t1'] = [] # Independent from names\n",
    "overheads['independent']['t4'] = {} # Independent from names\n",
    "launches_dict = {}\n",
    "\n",
    "for i, op in enumerate(ops):\n",
    "    name = op.name()\n",
    "    if name not in overheads.keys():\n",
    "        overheads[name] = {}\n",
    "\n",
    "    if 't2' not in overheads[name].keys():\n",
    "        overheads[name]['t2'] = []\n",
    "    if 't3' not in overheads[name].keys():\n",
    "        overheads[name]['t3'] = []\n",
    "    if 't5' not in overheads[name].keys():\n",
    "        overheads[name]['t5'] = []\n",
    "\n",
    "    sub_event_count = get_sub_event_count(op)\n",
    "    # Get the number of events before each kernel launch (to subtract corresponding amount of CPU overheads from estimated time)\n",
    "    tmp_launches = get_event_all_kernel_launches(op)\n",
    "    launches = []\n",
    "    count = 0\n",
    "    for x, y in tmp_launches:\n",
    "        count += y\n",
    "        if x.name() in [\"cudaMemcpyAsync\", \"cudaLaunchKernel\"]:\n",
    "            launches.append((x, count))\n",
    "            count = 0\n",
    "\n",
    "    if len(launches) > 0:\n",
    "        overheads[name]['t2'].append(launches[0][0].start_time() - op.start_time() - launches[0][1] * CPU_EVENT_OVERHEAD) # T2 has all overheads before the first launch\n",
    "        trailing_sub_event_count = sub_event_count - sum([y+1 for _, y in launches]) # And kernel launches themselves\n",
    "        overheads[name]['t3'].append(op.end_time() - launches[-1][0].end_time() - trailing_sub_event_count * CPU_EVENT_OVERHEAD) # T3 has all overheads after the last launch\n",
    "        if len(launches) > 1:\n",
    "            overheads[name]['t5'].extend([launches[i][0].start_time() - launches[i-1][0].end_time() - launches[i][1] * CPU_EVENT_OVERHEAD for i in range(1, len(launches))]) # T5 has all overheads between each pair of launches\n",
    "        else:\n",
    "            overheads[name]['t5'].append(0)\n",
    "\n",
    "        # T4 is launch-type-dependent\n",
    "        for x, _ in launches:\n",
    "            if x.name() not in overheads['independent']['t4']:\n",
    "                overheads['independent']['t4'][x.name()] = []\n",
    "            overheads['independent']['t4'][x.name()].append(KERNEL_LAUNCH_LENGTH - CPU_EVENT_OVERHEAD - GPU_EVENT_OVERHEAD) # T4 has 1 overhead\n",
    "\n",
    "        if op.name() not in launches_dict.keys():\n",
    "            launches_dict[op.name()] = []\n",
    "            for x, _ in launches:\n",
    "                launches_dict[op.name()].append(x.name())\n",
    "    else:\n",
    "        if name not in overheads.keys():\n",
    "            overheads[name] = {}\n",
    "        # If an op doesn't have kernel calls it has only one T5 overhead representing its CPU duration\n",
    "        if 't5' not in overheads[name].keys():\n",
    "            overheads[name]['t5'] = []\n",
    "        if name == \"aten::to\":\n",
    "            continue # Some aten::to doesn't have children\n",
    "        else:\n",
    "            overheads[name]['t5'].append(op.duration() - sub_event_count * CPU_EVENT_OVERHEAD) # Remove cpu overhead for all sub events\n",
    "\n",
    "    if i > 0:\n",
    "        prev_op = ops[i-1]\n",
    "\n",
    "        # Only consider adjacent ops under the SAME MODULE\n",
    "        if prev_op.parent != op.parent:\n",
    "            continue\n",
    "\n",
    "        gap = op.start_time() - prev_op.end_time()\n",
    "        if gap < 200: # Skip dataloading gaps\n",
    "            overheads['independent']['t1'].append(gap - CPU_EVENT_OVERHEAD) # Some pairs of ops are actually inserted by a runtime call which has been filtered from ops. TODO: fix it.\n",
    "\n",
    "# # T1: mean ~= 21, std ~= 20\n",
    "# from analysis.utils import histogram\n",
    "# histogram(overheads['independent']['t1'], perc=False, bins=[0, 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 100000])\n",
    "# print(np.mean(overheads['independent']['t1']), np.std(overheads['independent']['t1']))\n",
    "\n",
    "# T2, T3, T5\n",
    "t2 = {k: (np.mean(v['t2']), np.std(v['t2'])) for k, v in overheads.items() if k != 'independent' and len(v['t2']) > 0}\n",
    "# pprint(t2)\n",
    "t3 = {k: (np.mean(v['t3']), np.std(v['t3'])) for k, v in overheads.items() if k != 'independent' and len(v['t3']) > 0}\n",
    "# pprint(t3)\n",
    "t5 = {k: (np.mean(v['t5']), np.std(v['t5'])) for k, v in overheads.items() if k != 'independent' and len(v['t5']) > 0}\n",
    "# pprint(t5)\n",
    "\n",
    "# # T4\n",
    "# for t, l in overheads['independent']['t4'].items():\n",
    "#     print(t, np.mean(l), np.std(l))\n",
    "\n",
    "o = {\n",
    "    \"t1\": (np.mean(overheads['independent']['t1']), np.std(overheads['independent']['t1'])),\n",
    "    \"t2\": t2,\n",
    "    \"t3\": t3,\n",
    "    \"t4\": {\n",
    "        t: (np.mean(l), np.std(l)) for t, l in overheads['independent']['t4'].items()\n",
    "    },\n",
    "    \"t5\": t5,\n",
    "    \"launches\": launches_dict\n",
    "}\n",
    "\n",
    "# with open(\"overheads_{}.json\".format(model_name), \"w\") as f:\n",
    "#     json.dump(o, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multistream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Not finished yet\n",
    "# all_kernels = []\n",
    "# for _, c in cc.items():\n",
    "#     for _, v in c[\"callees\"].items():\n",
    "#         if v[\"executor\"] is not None:\n",
    "#             all_kernels.append(v[\"executor\"])\n",
    "# all_kernels = sorted(all_kernels, key=lambda x: x.start_time())\n",
    "\n",
    "# idle_time = 0\n",
    "# last_end = all_kernels[0].start_time() + all_kernels[0].duration()\n",
    "# overlapped = 0\n",
    "# for k in all_kernels:\n",
    "#     if k.start_time() > last_end:\n",
    "#         idle_time += k.start_time() - last_end\n",
    "#         last_end = k.start_time() + k.duration()\n",
    "#     else:\n",
    "#         last_end = max(last_end, k.start_time() + k.duration())\n",
    "#         overlapped += min(last_end, k.start_time() + k.duration()) - k.start_time()\n",
    "\n",
    "# print(\"device_runtime\", device_runtime)\n",
    "# print(\"idle_time:\", idle_time)\n",
    "# print(\"overlapped_time\", overlapped)"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "312645203253544"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200830-210251",
   "description": "Analyze a two-iteration trace file generated by ATC ; extract multi-level (module/op) runtime breakdown and major input output shapes.",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/zhongyilin/fbsource/fbcode/bento/kernels/local/zhongyilin/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "303205",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "1425096571211434",
   "tags": "FBLSim,CEA,ATC,dyno,gputrace,trace",
   "tasks": "",
   "title": "GPU Trace Analysis"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('zhongyi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "02372877dc9d511b3b18d0e90d7fc10386e618ab92d8e9830d393832c733bc2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
