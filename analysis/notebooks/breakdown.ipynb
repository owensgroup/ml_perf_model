{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import json, sys, re, os, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.max_open_warning'] = 50\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + \"/../../\")\n",
    "from analysis.utils import PM_HOME, GPU_NAME, op_name_contains\n",
    "from analysis.trace_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "num_gpus = 4\n",
    "batch_size = 4096\n",
    "iters = 30\n",
    "model_names = [\"DLRM_Heavy_EL\", \"DLRM_Light_EL\", \"GPT2\"]\n",
    "\n",
    "workloads = {\n",
    "    \"DLRM_Heavy_EL\": '{}/data/{}/e2e/DLRM_open_source/2022/44-247-742-494-148-1-595-558-16-109-550-511-704-583-206-564-316-71-572-125-106-592-584-142-546-582-591-586-574-736-716-253-267-41-587-102/f/size_lookup_greedy/barrier_bucketed_allreduce/25/{}_{}_distributed_[0-{}].json'.format(PM_HOME, GPU_NAME, num_gpus, batch_size, num_gpus),\n",
    "    \"DLRM_Light_EL\": '{}/data/{}/e2e/DLRM_open_source/2021/417-307-201-412-722-106-536-234-20-523-373-231-791-140-719-309-107-216-209-336-638-69-711-206-740-655-58-337-475-136-550-56-53-425-530-808-280-595/f/size_lookup_greedy/barrier_bucketed_allreduce/25/{}_{}_distributed_[0-{}].json'.format(PM_HOME, GPU_NAME, num_gpus, batch_size, num_gpus),\n",
    "    \"GPT2\": '{}/data/{}/e2e/gpt2/barrier_bucketed_allreduce/25/4_64_distributed_[0-{}].json'.format(PM_HOME, GPU_NAME, num_gpus),\n",
    "}\n",
    "\n",
    "traces = {\n",
    "    \"DLRM_Heavy_EL\": [],\n",
    "    \"DLRM_Light_EL\": [],\n",
    "    \"GPT2\": [],\n",
    "}\n",
    "\n",
    "actual_time = {\n",
    "    \"DLRM_Heavy_EL\": [],\n",
    "    \"DLRM_Light_EL\": [],\n",
    "    \"GPT2\": [],\n",
    "}\n",
    "\n",
    "for model_name in workloads.keys():\n",
    "    trace_files = glob.glob(workloads[model_name])\n",
    "\n",
    "    for tf in trace_files:\n",
    "        log_file = os.path.splitext(tf)[0][:-2] + '.log'\n",
    "        runtime_no_pf = -1\n",
    "        if os.path.exists(log_file):\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch\", line):\n",
    "                    runtime_no_pf = float(line.split(' ')[4]) * 1000 * iters # us\n",
    "                    actual_time[model_name].append(runtime_no_pf)\n",
    "                    break\n",
    "\n",
    "        trimmed_trace_file = os.path.splitext(tf)[0] + '_trimmed_{}.json'.format(iters)\n",
    "        with open(trimmed_trace_file) as f:\n",
    "            trace = json.load(f)\n",
    "            traces[model_name].append(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's impossible that a DL model doesn't have a linear, no?\n",
    "def is_comp_stream(tmp):\n",
    "    return any([(\"aten::linear\" in x[0]) for x in tmp.keys()])\n",
    "\n",
    "ranks = {}\n",
    "for model_name in model_names:\n",
    "    ranks[model_name] = []\n",
    "    for idx, trace in enumerate(traces[model_name]):\n",
    "        module_marker = \"DLRM \" if \"DLRM\" in model_name else \"## Forward ##\"\n",
    "        roots, cc, streams, corrected_start_time, corrected_end_time, sum_skipped_intervals = process_event_hierarchy(trace['traceEvents'], skip_module=False, module_marker=module_marker)\n",
    "        host_runtime = corrected_end_time - corrected_start_time - sum_skipped_intervals\n",
    "        device_runtime = host_runtime\n",
    "        ops = []\n",
    "        get_operators(roots, ops)\n",
    "        op_device_runtime = get_device_runtime(ops, cc) # dict: op ex_id -> all its device calls and stats\n",
    "        dt_breakdown = device_runtime_breakdown(roots, op_device_runtime, depth=0)\n",
    "        flatten = {}\n",
    "        comp_count = 0\n",
    "        comm_count = 0\n",
    "        for stream, v in dt_breakdown.items():\n",
    "            tmp = {}\n",
    "            get_major_device_results(device_runtime, dt_breakdown[stream], tmp)\n",
    "            if is_comp_stream(tmp):\n",
    "                s = \"computation_{}\".format(comp_count)\n",
    "                comp_count += 1\n",
    "            else:\n",
    "                s = \"communication_{}\".format(comm_count)\n",
    "                comm_count += 1\n",
    "            flatten[s] = tmp\n",
    "        ranks[model_name].append((model_name, flatten, actual_time[model_name][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\"Embedding Lookup\", \"GEMM\", \"Memory\", \"Communication\", \"Others\", \"Idle\"]\n",
    "f1 = lambda x: op_name_contains(x, [\n",
    "    \"fbgemm\", \"torch::autograd::CppNode<SplitLookupFunction_sgd_Op>\",\n",
    "])\n",
    "f2 = lambda x: op_name_contains(x, [\n",
    "    \"aten::linear\", \"aten::addmm\", \"AddmmBackward\", \\\n",
    "    \"aten::bmm\", \"BmmBackward\", \\\n",
    "    \"aten::matmul\", \"MmBackward\",\n",
    "])\n",
    "f3 = lambda x: op_name_contains(x, [\n",
    "    \"aten::to\", \"aten::cat\", \"Transpose\", \"transpose\",\n",
    "])\n",
    "def get_type(x):\n",
    "    if f1(x): return \"Embedding Lookup\"\n",
    "    if f2(x): return \"GEMM\"\n",
    "    if f3(x): return \"Memory\"\n",
    "    return \"Others\"\n",
    "\n",
    "def legend_helper(tmp, *args):\n",
    "    if isinstance(tmp, plt.Figure):\n",
    "        handles, labels = [list(chain.from_iterable(seq)) for seq in zip(*(\n",
    "            ax.get_legend_handles_labels() for ax in tmp.axes\n",
    "        ))]\n",
    "    elif isinstance(tmp, list):\n",
    "        handles, labels = [list(chain.from_iterable(seq)) for seq in zip(*(\n",
    "            ax.get_legend_handles_labels() for ax in tmp\n",
    "        ))]\n",
    "    else:\n",
    "        handles, labels = [list(chain.from_iterable(seq)) for seq in zip(*(\n",
    "            ax.get_legend_handles_labels() for ax in chain([tmp], args)\n",
    "        ))]\n",
    "        \n",
    "    return {\n",
    "        'handles': handles,\n",
    "        'labels': labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_bar_chart(ranks):\n",
    "    ncols = len(model_names)\n",
    "    nrows = num_gpus * 2\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12 * len(model_names) - 1, 9)) # 2 Streams per model\n",
    "    cmap = plt.get_cmap('tab20b', 6) # 6 colors: 5 types + idle\n",
    "    colors = cmap(np.linspace(0, 1, 6))\n",
    "    colors[-1, :] = np.array([0.5, 0.5, 0.5, 1]) # Grey\n",
    "    handles, labels = None, None\n",
    "    for idz, (model_name, tts) in enumerate(ranks.items()):\n",
    "        for idy, t in enumerate(tts):\n",
    "            _, d, runtime_no_pf = t\n",
    "            for idx, (stream, flatten) in enumerate(d.items()):\n",
    "                ax = axes[idy * 2 + idx][idz]\n",
    "                is_communication = \"communication\" in stream\n",
    "                per_op = {}\n",
    "                total = 0.0\n",
    "                # ranks[\"GPT2\"][0][1][\"computation_0\"][\"total\"][\"subs\"]\n",
    "                for k, v in flatten[\"total\"][\"subs\"].items():\n",
    "                    k0 = k[0]\n",
    "                    if k0 not in per_op.keys():\n",
    "                        per_op[k0] = 0.0\n",
    "                    per_op[k0] += v\n",
    "                    total += v\n",
    "\n",
    "                tmp = sorted(per_op.items(), key=lambda x: x[1], reverse=True)\n",
    "                op = [x[0] for x in tmp]\n",
    "                p = [x[1] / total for x in tmp]\n",
    "                df0 = pd.DataFrame({\n",
    "                    'Active time': [flatten['total']['runtime'] / runtime_no_pf],\n",
    "                    'Idle time': [1 - flatten['total']['runtime'] / runtime_no_pf],\n",
    "                })\n",
    "                tmp_df = pd.DataFrame({\n",
    "                    \"type\": [\"Communication\"] * len(op) if is_communication else [get_type(x) for x in op],\n",
    "                    \"perc\": p,\n",
    "                }).groupby(['type'], as_index=False).sum()\n",
    "                df = pd.DataFrame([tmp_df[\"perc\"].tolist()], columns=tmp_df[\"type\"].tolist())\n",
    "                active_time = df0['Active time'].item()\n",
    "                idle_time = df0['Idle time'].item()\n",
    "                df = df * active_time\n",
    "                df['Idle'] = idle_time\n",
    "\n",
    "                new_colors = [colors[TYPES.index(x)] for x in df.columns.values.tolist()]\n",
    "                new_cmp = ListedColormap(new_colors)\n",
    "                df.plot(stacked=True, legend=False, kind='barh', width=0.05, cmap=new_cmp, ax=ax)\n",
    "                ax.set_xlim((0.0, 1.0))\n",
    "                ax.set_yticks([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_ylim((-0.03, 0.03))\n",
    "                ax.grid(axis='x')\n",
    "                ax.set_title(\"Communication\" if is_communication else \"Computation\", fontsize=16)\n",
    "                if idy * 2 + idx == nrows - 1:\n",
    "                    ax.set_xticks(ax.get_xticks())\n",
    "                    ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()], fontsize=16)\n",
    "                else:\n",
    "                    ax.tick_params(labelbottom = False, bottom = False)\n",
    "\n",
    "                if idz == 0 and idy == 0:\n",
    "                    h, l = ax.get_legend_handles_labels()\n",
    "                    if handles is None:\n",
    "                        handles, labels = h[:-1], l[:-1]\n",
    "                    else:\n",
    "                        handles += h\n",
    "                        labels += l\n",
    "            ax.text(-0.02, 0.04, \"Rank {}\".format(idy), horizontalalignment='center', verticalalignment='center', size=16, rotation=90)\n",
    "            rec = Rectangle((-0.04, -0.035), 1.05, 0.19, fill=False, lw=1.5, linestyle=\"dotted\")\n",
    "            rec.set_clip_on(False)\n",
    "            ax.add_patch(rec)\n",
    "\n",
    "        ax.text(0.5, 0.75, \"{} ({:.2f} ms/iter)\".format(model_name, max(actual_time[model_name]) / 1000.0 / iters), horizontalalignment='center', verticalalignment='center', size=18)\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=6, bbox_to_anchor=(0.5, 0.01), frameon=False, fontsize=18)\n",
    "    plt.subplots_adjust(hspace=0.6, wspace=0.2)\n",
    "    plt.savefig('{}/data/{}/e2e/multi_gpu_time_breakdown.pdf'.format(PM_HOME, GPU_NAME), bbox_inches='tight')\n",
    "    plt.savefig('{}/data/{}/e2e/multi_gpu_time_breakdown.png'.format(PM_HOME, GPU_NAME), bbox_inches='tight')\n",
    "\n",
    "plot_bar_chart(ranks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "num_gpus = 1\n",
    "batch_size = 2048\n",
    "iters = 10\n",
    "\n",
    "flattens = []\n",
    "for model_name in ['DLRM_default', 'DLRM_MLPerf', 'DLRM_DDP']:\n",
    "    module_marker = \"DLRM \" if \"DLRM\" in model_name else \"## Forward ##\"\n",
    "    trace_file = '{}/data/{}/e2e/{}/{}_{}.json'.format(PM_HOME, GPU_NAME, model_name, num_gpus, batch_size)\n",
    "\n",
    "    runtime_no_pf = -1\n",
    "    log_file = \"{}/data/{}/e2e/{}/{}_{}.log\".format(PM_HOME, GPU_NAME, model_name, num_gpus, batch_size)\n",
    "    if os.path.exists(log_file):\n",
    "        for line in open(log_file, 'r'):\n",
    "            if re.search(\"Overall per-batch\", line):\n",
    "                runtime_no_pf = float(line.split(' ')[4]) * 1000 * iters # us\n",
    "\n",
    "    trimmed_trace_file = trim_trace_by_num_iter(trace_file, iters=iters, trimmed_file='/tmp/{}_{}_{}'.format(model_name, num_gpus, batch_size))\n",
    "    with open(trimmed_trace_file) as f:\n",
    "        trace = json.load(f)\n",
    "\n",
    "    roots, cc, corrected_start_time, corrected_end_time, sum_skipped_intervals = process_event_hierarchy(trace['traceEvents'], skip_module=False, module_marker=module_marker)\n",
    "    host_runtime = corrected_end_time - corrected_start_time - sum_skipped_intervals\n",
    "    device_runtime = host_runtime\n",
    "    ops = []\n",
    "    get_operators(roots, ops)\n",
    "    op_device_runtime = get_device_runtime(ops, cc) # dict: op ex_id -> all its device calls and stats\n",
    "    dt_breakdown = device_runtime_breakdown(roots, op_device_runtime, depth=0)\n",
    "    flatten = {}\n",
    "    for stream, v in dt_breakdown.items():\n",
    "        flatten[stream] = {}\n",
    "        get_major_device_results(device_runtime, dt_breakdown[stream], flatten[stream])\n",
    "    flattens.append((model_name, flatten, runtime_no_pf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(flattens, truncate=20):\n",
    "    fig, axes = plt.subplots(nrows=len(flattens), ncols=1, figsize=(12, 5))\n",
    "\n",
    "    cmap = plt.get_cmap('tab20b', truncate+1)\n",
    "    gray = np.array([0.5, 0.5, 0.5, 1])\n",
    "    colors = cmap(np.linspace(0, 1, truncate+1))\n",
    "    colors[-1, :] = gray\n",
    "    legend_dict = None\n",
    "\n",
    "    for idx, tuple in enumerate(flattens):\n",
    "        model_name, flatten, runtime_no_pf = tuple\n",
    "        flatten = flatten[7]\n",
    "        per_op = {}\n",
    "        total = 0.0\n",
    "        for k, v in flatten.items():\n",
    "            if k == 'total' or 'DLRM ' in k[0] or 'module' in k[0]: # Skip all labels\n",
    "                continue\n",
    "            k0 = k[0] if '#' not in k[0] else k[0].split('#')[0]\n",
    "            if k0 not in per_op.keys():\n",
    "                per_op[k0] = 0.0\n",
    "            per_op[k0] += v['runtime']\n",
    "            total += v['runtime']\n",
    "            \n",
    "        tmp = sorted(per_op.items(), key=lambda x: x[1], reverse=True)\n",
    "        op = [x[0] for x in tmp]\n",
    "        p = [x[1] / total for x in tmp]\n",
    "        df0 = pd.DataFrame({\n",
    "            'Active time': [flatten['total']['runtime'] / runtime_no_pf],\n",
    "            'Idle time': [1 - flatten['total']['runtime'] / runtime_no_pf]\n",
    "        })\n",
    "        if len(p) > truncate:\n",
    "            p[truncate-1] = sum(p[(truncate-1):])\n",
    "            p = p[:truncate]\n",
    "            op[truncate-1] = \"others\"\n",
    "            op = op[:truncate]\n",
    "        df = pd.DataFrame([p], columns=op)\n",
    "        active_time = df0['Active time'].item()\n",
    "        idle_time = df0['Idle time'].item()\n",
    "        df = df * active_time\n",
    "        df['Idle'] = idle_time\n",
    "        \n",
    "        if idx == 0:\n",
    "            new_colors = colors\n",
    "            legend_dict = {k: v for k, v in zip(df.columns.values.tolist(), colors)}\n",
    "        else:\n",
    "            assert legend_dict is not None\n",
    "            columns = df.columns.values.tolist()\n",
    "            new_colors = [legend_dict[c] if c in legend_dict.keys() else plt.get_cmap('tab20b')(truncate + idx) for c in columns]\n",
    "\n",
    "        new_cmp = ListedColormap(new_colors)\n",
    "        df.plot(stacked=True, legend=False, kind='barh', width=0.05, cmap=new_cmp, ax=axes[idx])\n",
    "        vals = axes[idx].get_xticks()\n",
    "        axes[idx].set_xticklabels(['{:,.0%}'.format(x) for x in vals])\n",
    "        axes[idx].set_xlim((0.0, 1.0))\n",
    "        axes[idx].set_yticks([])\n",
    "        axes[idx].set_yticklabels([])\n",
    "        axes[idx].set_ylim((-0.03, 0.03))\n",
    "        axes[idx].set_title(model_name, fontsize=14)\n",
    "\n",
    "        ax2 = axes[idx].twiny()\n",
    "        ax2.set_xticks([x * active_time for x in np.arange(0, 1.2, 0.2)])\n",
    "        ax2.set_xbound(axes[idx].get_xbound())\n",
    "        ax2.set_xticklabels(['{:,.0%}'.format(x) for x in np.arange(0, 1.2, 0.2)])\n",
    "\n",
    "    handels, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handels, labels, loc=\"lower center\", ncol=5, bbox_to_anchor=(0.5, -0.3), frameon=False, fontsize=10.5)\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['figure.figsize'] = [12, 5]\n",
    "    plt.savefig('{}/data/{}/e2e/active_time_breakdown.pdf'.format(PM_HOME, GPU_NAME, model_name), bbox_inches='tight')\n",
    "    plt.savefig('{}/data/{}/e2e/active_time_breakdown.png'.format(PM_HOME, GPU_NAME, model_name), bbox_inches='tight')\n",
    "\n",
    "plot_bar_chart(flattens)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02372877dc9d511b3b18d0e90d7fc10386e618ab92d8e9830d393832c733bc2e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('zhongyi': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
