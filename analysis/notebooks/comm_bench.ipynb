{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, \"/usr/local/cuda/targets/x86_64-linux/lib/stubs/\") # \n",
    "from analysis.utils import histogram, abs_err, gmae, GPU_COUNT\n",
    "from analysis.memory_bw_utils import *\n",
    "\n",
    "superscript = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "collectives = ['all_to_allv', 'all_reduce']\n",
    "num_of_collectives = len(collectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce05fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU_COUNT = 4\n",
    "dir_prefix = \"../../3rdparty/param/train/comms/pt/bench_results\"\n",
    "data = process_param_data(\n",
    "    prefix=dir_prefix,\n",
    "    collectives=collectives,\n",
    "    num_gpus=GPU_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 18))\n",
    "for idx, collective in enumerate(collectives):\n",
    "    ax = fig.add_subplot(num_of_collectives, 2, idx * 2 + 1)\n",
    "    ax.set_title('{} latency'.format(collective))\n",
    "    ax.plot(data[collective]['size'], data[collective]['latency'], marker='o')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim([1e1, 1e6])\n",
    "    ax.set_xticks([2**i for i in range(2, 33)])\n",
    "    ax.set_xticklabels([\"2{}\".format(str(j).translate(superscript)) for j in range(2, 33)])\n",
    "\n",
    "    ax = fig.add_subplot(num_of_collectives, 2, idx * 2 + 2)\n",
    "    ax.set_title('{} BW'.format(collective))\n",
    "    ax.plot(data[collective]['size'], data[collective]['alg_bw'], marker='o')\n",
    "    ax.plot(data[collective]['size'], data[collective]['bus_bw'], marker='o')\n",
    "    ax.legend(['alg_bw', 'bus_bw'])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim([1e-3, 1e3])\n",
    "    ax.set_xticks([2**i for i in range(2, 33)])\n",
    "    ax.set_xticklabels([\"2{}\".format(str(j).translate(superscript)) for j in range(2, 33)])\n",
    "\n",
    "fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax1 = fig.add_subplot(3, 1, 1)\n",
    "# ax2 = fig.add_subplot(3, 1, 2)\n",
    "# ax3 = fig.add_subplot(3, 1, 3)\n",
    "# for idx, collective in enumerate(collectives):\n",
    "    \n",
    "#     ax1.plot(data[collective]['size'], data[collective]['latency'], marker='o')\n",
    "#     ax1.set_xscale('log')\n",
    "#     ax1.set_yscale('log')\n",
    "#     ax1.set_ylim([1e1, 1e6])\n",
    "#     ax1.set_xticks([2**i for i in range(2, 33)])\n",
    "#     ax1.set_xticklabels([\"2{}\".format(str(j).translate(superscript)) for j in range(2, 33)])\n",
    "#     ax1.set_xlabel(\"Message size (bytes)\", fontsize=12)\n",
    "#     ax1.set_ylabel(\"Latency (us)\", fontsize=12)\n",
    "\n",
    "#     ax2.plot(data[collective]['size'], data[collective]['bus_bw'], marker='o')\n",
    "#     ax2.set_xscale('log')\n",
    "#     ax2.set_yscale('log')\n",
    "#     ax2.set_ylim([1e-3, 1e2])\n",
    "#     ax2.set_xticks([2**i for i in range(2, 33)])\n",
    "#     ax2.set_xticklabels([\"2{}\".format(str(j).translate(superscript)) for j in range(2, 33)])\n",
    "#     ax2.set_xlabel(\"Message size (bytes)\", fontsize=12)\n",
    "#     ax2.set_ylabel(\"Bus BW (GB/s)\", fontsize=12)\n",
    "\n",
    "#     ax3.plot(data[collective]['size'], data[collective]['alg_bw'], marker='o')\n",
    "#     ax3.set_xscale('log')\n",
    "#     ax3.set_yscale('log')\n",
    "#     ax3.set_ylim([1e-3, 1e2])\n",
    "#     ax3.set_xticks([2**i for i in range(2, 33)])\n",
    "#     ax3.set_xticklabels([\"2{}\".format(str(j).translate(superscript)) for j in range(2, 33)])\n",
    "#     ax3.set_xlabel(\"Message size (bytes)\", fontsize=12)\n",
    "#     ax3.set_ylabel(\"Alg BW (GB/s)\", fontsize=12)\n",
    "\n",
    "# ax1.legend(collectives)\n",
    "# ax2.legend(collectives)\n",
    "# ax3.legend(collectives)\n",
    "# fig.suptitle(\"Communication collectives microbenchmark on a quad-V100 DGX-1\", fontsize=14)\n",
    "# fig.tight_layout(pad=0.5)\n",
    "# plt.savefig('../../3rdparty/param/train/comms/pt/latency_bw.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada41dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BW curve fitting. Input: total message size in bytes, output: BW in GB/s.\n",
    "mem_chs = {}\n",
    "sigmoid_params = {}\n",
    "fig, axs = plt.subplots(1, len(collectives), figsize=(8*len(collectives), 6))\n",
    "for idx, collective in enumerate(collectives):\n",
    "    ax = axs[idx] if len(collectives) > 1 else axs\n",
    "    mem_chs[collective] = get_memory_characteristics(data[collective])\n",
    "    sigmoid_params[collective] = fit_sigmoid_bw_predictor(data[collective], mem_chs[collective])\n",
    "    d = {\n",
    "        \"mul_factor\": MUL_FACTOR_FUNCS[collective](GPU_COUNT),\n",
    "        \"mem_ch\": mem_chs[collective],\n",
    "        \"sigmoid_param\": sigmoid_params[collective],\n",
    "    }\n",
    "    f1 = lambda x: predict_bus_bw(x, **d)\n",
    "    ax.plot(data[collective]['size'], data[collective]['bus_bw'])\n",
    "    ax.plot(data[collective]['size'], data[collective]['size'].apply(f1))\n",
    "    ax.axvline(2 ** mem_chs[collective][\"ln_p\"], linestyle='--', color='green')\n",
    "    ax.axvline(2 ** mem_chs[collective][\"sats_p\"], linestyle='--', color='green')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(collective)\n",
    "    ax.set_ylabel('Latency (us)', va='center', rotation=90, fontsize=14)\n",
    "    ax.set_xlabel(\"Message Size (in bytes)\", fontsize=14)\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    # print(data[collective]['bus_bw'])\n",
    "    # print(data[collective]['size'].apply(f1))\n",
    "\n",
    "    # # Prediction\n",
    "    # print(\"----- {} -----\".format(collective))\n",
    "    # for idx, size in enumerate(data[collective]['size']):\n",
    "    #     f_mul_factor = MUL_FACTOR_FUNCS[collective]\n",
    "    #     f_sigmoid_bw = sigmoid_params[collective](GPU_COUNT)\n",
    "    #     print(\"{:.2f}, {:.2f}, {:.2f}\".format(\n",
    "    #         data[collective]['latency'][idx],\n",
    "    #         predict_linear(size, f_mul_factor, *mem_chs[collective]),\n",
    "    #         predict_data_movement_time(size, f_mul_factor, mem_ch, sigmoid_param),\n",
    "    #     ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f9a27dc",
   "metadata": {},
   "source": [
    "Current limitations:\n",
    "- Is there a way to directly get the min/max BW from device connection configuration w/o benchmarking? Can the bus BW be derived from the algo BW which seems to follow a pattern (50, 75, 87.5 GB/s)?\n",
    "\n",
    "e.g. 4 GPUs, all_to_all, each GPU sends 1/4 elements to each of the other GPUs\n",
    "- --b/--e (in bytes per rank): 16, 32, 64...\n",
    "- allSizes (in bytes per rank): 16, 32, 64...\n",
    "- memSize / size (B) in printed results (in bytes per rank): 16, 32, 64...\n",
    "- num-elements in printed results (in elements COMMUNICATED per rank-pair): 1, 2, 4...\n",
    "\n",
    "commsParams.element_size: 4 (float)\n",
    "comm_fn: backendFuncs.collectiveFunc[commsParams.collective]\n",
    "comms.py comm op line 1202 calls runColl line 258\n",
    "--z/commsParamsHolderBase's blockingFlag/~asyncOp 1: non-blocking, 0: blocking\n",
    "gatherBenchTime line 767: gather bench time stored in tensors on each device to a list of tensors on rank 0.\n",
    "\n",
    "param pytorch_dist_backend: all_to_all line 163 calls dist.all_to_all_single line 170, wait function called at line 389\n",
    "dlrm extend_distributed: alltoall line 597 calls All2All_Req line 404 calls dist.all_to_all_single line 429 (list of local tensors concatenated and flatten to 1D)\n",
    "\n",
    "e.g. batched_emb\n",
    "dist.all_to_all_single: input_split_sizes (how tables are distributed to devices, e.g. 13 tables and [2,3,3,5] on 4 GPUs), output_split_sizes (how batches are distributed to devices; set to None for equal distribution, e.g. batch size 2048 -> 512 per GPU)\n",
    "common case: input_split_sizes not None, output_split_sizes None\n",
    "\n",
    "- reduce scatter: memSize measures the INPUT size in bytes per rank (equal to total OUTPUT size on all devices)\n",
    "- all gather: memSize measures the OUTPUT size in bytes per rank (equal to total INPUT size on all devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_ch = mem_chs['all_to_allv']\n",
    "sigmoid_param = sigmoid_params[\"all_to_allv\"]\n",
    "mul_factor = MUL_FACTOR_FUNCS[\"all_to_allv\"](GPU_COUNT)\n",
    "df = process_general_a2a_param_data(\n",
    "    prefix=dir_prefix,\n",
    "    num_gpus=GPU_COUNT,\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration\n",
    "def get_adjusted_size(s, t):\n",
    "    splitted = s.split(',')\n",
    "    B = int(splitted[0]) // GPU_COUNT\n",
    "    D = int(splitted[2])\n",
    "    tables = [int(t) for t in splitted[1].split('-')]\n",
    "    if t == \"sum\":\n",
    "        T = sum(tables)\n",
    "    elif t == \"max_of_max\":\n",
    "        T = max([max(sum(tables) - t, t * (GPU_COUNT-1)) for t in tables])\n",
    "    elif t == \"max_of_sum\":\n",
    "        T = max([(sum(tables) - t + t * (GPU_COUNT-1)) for t in tables])\n",
    "    else:\n",
    "        raise Exception(\"Unrecognized max_type\")\n",
    "    return B * T * D * 4 # float32\n",
    "\n",
    "for tt in ['sum', 'max_of_max', 'max_of_sum']:\n",
    "    adjusted_size = df['btd'].apply(get_adjusted_size, args=(tt,))\n",
    "    y1 = adjusted_size.apply(predict_data_movement_time, args=(mul_factor, mem_ch, sigmoid_param))\n",
    "    error1 = abs_err(y1, df['latency'])\n",
    "    print(\"All to allv ({}): GMAE: {:.2f}%, mean: {:.2f}%, std: {:.2f}%\".format(tt, gmae(error1) * 100.0, error1.mean() * 100.0, error1.std() * 100.0))\n",
    "    _ = histogram(error1)\n",
    "    sorted_df = pd.DataFrame({\n",
    "        \"size\": adjusted_size,\n",
    "        \"latency\": df[\"latency\"],\n",
    "    }).sort_values(['size'], ascending=True)\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(sorted_df['size'], sorted_df['latency'], s=1)\n",
    "    ax.plot(sorted_df['size'], sorted_df['size'].apply(predict_data_movement_time, args=(mul_factor, mem_ch, sigmoid_param)), color='orange', linewidth=4)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "    ax.axvline(2 ** mem_ch[\"ln_p\"], linestyle='--', color='green')\n",
    "    ax.axvline(2 ** mem_ch[\"sats_p\"], linestyle='--', color='green')\n",
    "    ax.set_ylabel('Latency (us)', va='center', rotation=90, fontsize=14)\n",
    "    ax.set_xlabel(\"Message Size (in bytes)\", fontsize=14)\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    if tt == \"max_of_max\":\n",
    "        plt.savefig('./a2a.pdf', bbox_inches='tight')\n",
    "        plt.savefig('./a2a.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94dbb46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhongyi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
