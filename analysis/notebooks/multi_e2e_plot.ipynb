{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats.mstats import gmean\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, sys, glob\n",
    "\n",
    "plt.rcParams['figure.max_open_warning'] = 50\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "PM_HOME = os.getcwd() + \"/../../\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU DLRM E2E Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "batch_sizes = [512, 1024, 2048, 4096]\n",
    "num_gpus = 2\n",
    "trimmed_iters = 30\n",
    "gpus = [\"A100\"]\n",
    "errors = {}\n",
    "actual_time = {}\n",
    "for gpu in gpus:\n",
    "    errors[gpu] = {}\n",
    "    actual_time[gpu] = {}\n",
    "    dirs = glob.glob('{}/data/{}/e2e/DLRM_open_source/*/*/f/size_lookup_greedy/barrier_bucketed_allreduce/25'.format(PM_HOME, gpu))\n",
    "    ff = filter(lambda f: os.path.isdir(f), dirs)\n",
    "    for prefix in ff:\n",
    "        task = prefix.split('/')[-5]\n",
    "        if task not in errors[gpu].keys():\n",
    "            errors[gpu][task] = []\n",
    "        if task not in actual_time[gpu].keys():\n",
    "            actual_time[gpu][task] = []\n",
    "        for batch_size in batch_sizes:\n",
    "            log_file = '{}/{}_{}_distributed.log'.format(prefix, num_gpus, batch_size)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            assert os.path.exists(log_file), \"{} does not exist\".format(log_file)\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch training time\", line):\n",
    "                    actual_time[gpu][task].append(float(line.split(' ')[4])) # In ms\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, num_gpus, batch_size, trimmed_iters)\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        active_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        kernel_only_error = float(line.split(' ')[4].rstrip(',%\\n'))\n",
    "                        shared_e2e_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        errors[gpu][task].append([active_error, kernel_only_error, shared_e2e_error])\n",
    "            prediction_file = '{}/{}_{}_distributed_prediction_{}.log'.format(prefix, num_gpus, batch_size, trimmed_iters)\n",
    "            if os.path.exists(prediction_file):\n",
    "                for line in open(prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        e2e_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        errors[gpu][task][-1].append(e2e_error)\n",
    "# active, kernel_only, shared_e2e, (e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "overall_active = [abs(c[0]) for device in errors.keys() for task in errors[device].keys() for c in errors[device][task]]\n",
    "# overall_total = [abs(c[1]) for device in errors.keys() for task in errors[device].keys() for c in errors[device][task]]\n",
    "error_active_devices = {\n",
    "    device: [abs(c[0]) for task in errors[device].keys() for c in errors[device][task]] for device in errors.keys()\n",
    "}\n",
    "# error_total_devices = {\n",
    "#     device: [abs(c[1]) for task in errors[device].keys() for c in errors[device][task]] for device in errors.keys()\n",
    "# }\n",
    "\n",
    "# print(\"Overall GPU (geo, min, max): {:.2f} & {:.2f} & {:.2f}\".format(gmean(overall_active), min(overall_active), max(overall_active)))\n",
    "# print(\"Overall total (geo, min, max): {:.2f} & {:.2f} & {:.2f}\".format(gmean(overall_total), min(overall_total), max(overall_total)))\n",
    "\n",
    "# for device in error_active_devices.keys():\n",
    "#     print(\"{} GPU (geo, min, max): {:.2f} & {:.2f} & {:.2f}\".format(device, gmean(error_active_devices[device]), min(error_active_devices[device]), max(error_active_devices[device])))\n",
    "#     print(\"{} total (geo, min, max): {:.2f} & {:.2f} & {:.2f}\".format(device, gmean(error_total_devices[device]), min(error_total_devices[device]), max(error_total_devices[device])))\n",
    "\n",
    "\n",
    "overall_shared_total = [abs(c[2]) for device in errors.keys() for task in errors[device].keys() for c in errors[device][task]]\n",
    "error_shared_total_devices = {\n",
    "    device: [abs(c[2]) for task in errors[device].keys() for c in errors[device][task]] for device in errors.keys()\n",
    "}\n",
    "\n",
    "# print(\"Active & \\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_active), min(overall_active), max(overall_active)))\n",
    "# # print(\"E2E & \\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_total), min(overall_total), max(overall_total)))\n",
    "# print(\"Shared E2E & \\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_shared_total), min(overall_shared_total), max(overall_shared_total)))\n",
    "\n",
    "# For Latex\n",
    "s1 = [\"Active & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_active), min(overall_active), max(overall_active))]\n",
    "# s2 = [\"E2E & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_total), min(overall_total), max(overall_total))]\n",
    "s3 = [\"Shared E2E & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_shared_total), min(overall_shared_total), max(overall_shared_total))]\n",
    "for device in error_active_devices.keys():\n",
    "    s1.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_active_devices[device]), min(error_active_devices[device]), max(error_active_devices[device])))\n",
    "    # s2.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_total_devices[device]), min(error_total_devices[device]), max(error_total_devices[device])))\n",
    "    s3.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_shared_total_devices[device]), min(error_shared_total_devices[device]), max(error_shared_total_devices[device])))\n",
    "print(' & '.join(s1) + \"\\\\\\\\\")\n",
    "# print(' & '.join(s2) + \"\\\\\\\\\")\n",
    "print(' & '.join(s3) + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [512, 1024, 2048, 4096]\n",
    "fig, axes = plt.subplots(nrows=len(gpus), ncols=1, figsize=(15, 7))\n",
    "count = 0\n",
    "w = 0.5\n",
    "task_gap = 6\n",
    "yticks = [-30, -20, -10, 0, 10, 20]\n",
    "yticks2 = [2.5, 5, 7.5, 10, 12.5, 15]\n",
    "yticklabels = [\"{:.0f}%\".format(x) for x in yticks]\n",
    "for device, error in errors.items():\n",
    "    ax = axes\n",
    "    ax2 = ax.twinx()\n",
    "    e1 = [] # active\n",
    "    eb = [] # baseline\n",
    "    e2 = [] # shared\n",
    "    e3 = [] # standalone\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    xb = []\n",
    "    x3 = []\n",
    "    xt = []\n",
    "    yt = []\n",
    "    batch_width = 0\n",
    "    tasks = []\n",
    "    xticks = []\n",
    "    for task, e in error.items():\n",
    "        tasks.append(task)\n",
    "        xtt = []\n",
    "        ytt = []\n",
    "        batch_width += 2\n",
    "        batch_gap = 0\n",
    "        first = -1\n",
    "        last = -1\n",
    "        for idx, ee in enumerate(e):\n",
    "            e1.append(ee[0])\n",
    "            eb.append(ee[1])\n",
    "            e2.append(ee[2])\n",
    "            if len(ee) == 4:\n",
    "                e3.append(ee[3])\n",
    "                x1.append(task_gap * batch_width - 2 + batch_gap - w * 2)\n",
    "                x2.append(task_gap * batch_width - 2 + batch_gap - w)\n",
    "                x3.append(task_gap * batch_width - 2 + batch_gap)\n",
    "                xb.append(task_gap * batch_width - 2 + batch_gap + w)\n",
    "            else:\n",
    "                x1.append(task_gap * batch_width - 2 + batch_gap - w)\n",
    "                x2.append(task_gap * batch_width - 2 + batch_gap)\n",
    "                xb.append(task_gap * batch_width - 2 + batch_gap + w)\n",
    "            if idx == 0:\n",
    "                first = x1[-1]\n",
    "            batch_gap += 2\n",
    "            xtt.append(task_gap * batch_width - 2 + batch_gap - w * 4)\n",
    "        # ax.text(task_gap * c - 1.6, 13, task, fontsize=12, transform=ax.transData)\n",
    "        ax2.plot(xtt, actual_time[device][task], color=plt.get_cmap(\"tab10\")(1), marker=\"s\")\n",
    "        last = xb[-1]\n",
    "        xticks.append((first + last) / 2)\n",
    "\n",
    "    ax.bar(x2, e2, width=w, color=plt.get_cmap(\"Blues\")(180), label='Prediction')\n",
    "    ax.bar(xb, eb, width=w, color=plt.get_cmap(\"Greys\")(120), label='Baseline')\n",
    "    if e3:\n",
    "        ax.bar(x3, e3, width=w, color=plt.get_cmap(\"Blues\")(240), label='E2E')\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([\"Task_{}\".format(x) for x in range(len(tasks))], fontsize=12)\n",
    "    ax.set_title(device, fontsize=15)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_ylim(yticks[0]-5, yticks[-1]+5)\n",
    "    xes = [zip(x1, e1), zip(xb, eb), zip(x2, e2)]\n",
    "    if e3:\n",
    "        xes.insert(-2, zip(x3, e3))\n",
    "    for xe in xes:\n",
    "        for x, e in xe:\n",
    "            if e < yticks[0]-5:\n",
    "                ax.text(x+0.3, yticks[0]+5, \"{:.1f}%\".format(e), horizontalalignment='center', verticalalignment='center', size=12)\n",
    "    ax2.set_yticks(yticks2)\n",
    "    count += 1\n",
    "\n",
    "handles, labels = (axes[-1] if isinstance(axes, list) else axes).get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=[0.8, 0.95], ncol=len(labels))\n",
    "fig.text(-0.01, 0.5, 'Prediction Error', va='center', rotation=90, fontsize=14)\n",
    "fig.text(0.99, 0.5, 'Iteration Time (ms)', va='center', rotation=-90, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('{}/data/multi_gpu_e2e.pdf'.format(PM_HOME), bbox_inches='tight')\n",
    "plt.savefig('{}/data/multi_gpu_e2e.png'.format(PM_HOME), bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharding Config Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "batch_sizes = [4096]\n",
    "num_gpus = 2\n",
    "trimmed_iters = 30\n",
    "gpus = [\"A100\"]\n",
    "# errors = {}\n",
    "actual_time = {}\n",
    "predicted_time = {}\n",
    "for gpu in gpus:\n",
    "    # errors[gpu] = {}\n",
    "    actual_time[gpu] = {}\n",
    "    predicted_time[gpu] = {}\n",
    "    dirs = glob.glob('{}/data/{}/e2e/DLRM_open_source/*/*/f/*/barrier_bucketed_allreduce/25'.format(PM_HOME, gpu))\n",
    "    ff = filter(lambda f: os.path.isdir(f), dirs)\n",
    "    for prefix in ff:\n",
    "        sharder = prefix.split('/')[-3]\n",
    "        task = prefix.split('/')[-5]\n",
    "        # if task not in errors[gpu].keys():\n",
    "        #     errors[gpu][task] = {}\n",
    "        if task not in actual_time[gpu].keys():\n",
    "            actual_time[gpu][task] = {}\n",
    "        if task not in predicted_time[gpu].keys():\n",
    "            predicted_time[gpu][task] = {}\n",
    "        for batch_size in batch_sizes:\n",
    "            log_file = '{}/{}_{}_distributed.log'.format(prefix, num_gpus, batch_size)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            assert os.path.exists(log_file), \"{} does not exist\".format(log_file)\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, num_gpus, batch_size, trimmed_iters)\n",
    "            if not os.path.exists(shared_prediction_file):\n",
    "                continue\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch training time\", line):\n",
    "                    actual_time[gpu][task][sharder] = float(line.split(' ')[4]) # In ms\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    predicted_time[gpu][task][sharder] = float(line.split(' ')[2].strip(',')) / 1e3 # In ms\n",
    "                    break\n",
    "            # x = []\n",
    "            # if os.path.exists(shared_prediction_file):\n",
    "            #     for line in open(shared_prediction_file, 'r'):\n",
    "            #         if re.search(\"Prediction error:\", line):\n",
    "            #             active_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "            #             kernel_only_error = float(line.split(' ')[4].rstrip(',%\\n'))\n",
    "            #             shared_e2e_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "            #             x = [active_error, kernel_only_error, shared_e2e_error]\n",
    "            # prediction_file = '{}/{}_{}_distributed_prediction_{}.log'.format(prefix, num_gpus, batch_size, trimmed_iters)\n",
    "            # if os.path.exists(prediction_file):\n",
    "            #     for line in open(prediction_file, 'r'):\n",
    "            #         if re.search(\"Prediction error:\", line):\n",
    "            #             e2e_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "            #             x.append(e2e_error)\n",
    "            # errors[gpu][task][sharder] = tuple(x)\n",
    "# active, kernel_only, shared_e2e, (e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(gpus), ncols=1, figsize=(20, 7))\n",
    "count = 0\n",
    "w = 0.5\n",
    "task_gap = 10\n",
    "yticks = [-30, -20, -10, 0, 10, 20]\n",
    "yticks2 = [5, 7.5, 10, 12.5, 15]\n",
    "yticklabels = [\"{:.0f}%\".format(x) for x in yticks]\n",
    "for idx, gpu in enumerate(gpus):\n",
    "    ax = axes if len(gpus) == 1 else axes[idx]\n",
    "    batch_width = 0\n",
    "    for task, e in predicted_time[gpu].items():\n",
    "        xtt = []\n",
    "        batch_width += 2\n",
    "        sharder_gap = 0\n",
    "        for sharder, ee in e.items():\n",
    "            sharder_gap += 1.5\n",
    "            xtt.append(task_gap * batch_width - 2 + sharder_gap - w * 4)\n",
    "        ax.plot(xtt, [t for _, t in actual_time[gpu][task].items()], color=plt.get_cmap(\"tab10\")(1), marker=\"s\")\n",
    "        ax.plot(xtt, [t for _, t in predicted_time[gpu][task].items()], color=plt.get_cmap(\"Blues\")(180), marker=\"s\")\n",
    "    ax.set_yticks(yticks2)\n",
    "\n",
    "handles, labels = (axes[-1] if isinstance(axes, list) else axes).get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=[0.7, 0.95], ncol=len(labels))\n",
    "fig.text(0.45, -0.01, \"Batch Size\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('{}/data/selection.pdf'.format(PM_HOME), bbox_inches='tight')\n",
    "plt.savefig('{}/data/selection.png'.format(PM_HOME), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhongyi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
