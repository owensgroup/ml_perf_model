{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats.mstats import gmean\n",
    "from pprint import pprint\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, sys, glob\n",
    "\n",
    "plt.rcParams['figure.max_open_warning'] = 50\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "PM_HOME = os.getcwd() + \"/../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 4\n",
    "# GPU_NAME = \"GV100\"\n",
    "gpus = [\"GV100\", \"A100\"]\n",
    "nodes = {\n",
    "    \"GV100\": \"bowser\",\n",
    "    \"A100\": \"wario\",\n",
    "}\n",
    "tasks = {\n",
    "    \"GV100\": None,\n",
    "    \"A100\": None,\n",
    "}\n",
    "batch_sizes = [512, 1024, 2048, 4096]\n",
    "nlp_batch_sizes = [8, 16, 32, 64]\n",
    "trimmed_iters = 30\n",
    "home_template = \"/home/m092926/{}/Documents/ml_perf_model\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU DLRM E2E Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "errors = {}\n",
    "actual_time = {}\n",
    "for gpu in gpus:\n",
    "    home = home_template.format(nodes[gpu])\n",
    "    with open(\"{}/benchmark/tasks_{}x{}_permanent.txt\".format(home, GPU_COUNT, gpu), 'r') as f:\n",
    "        tasks[gpu] = [line.strip() for line in f.readlines()]\n",
    "    errors[gpu] = []\n",
    "    actual_time[gpu] = []\n",
    "    for task in tasks[gpu]:\n",
    "        tmp = task.split(',')\n",
    "        prefix = \"{}/data/{}/e2e/DLRM_open_source/{}/{}/f/size_lookup_greedy/barrier_bucketed_allreduce/25\".format(home, gpu, tmp[0], tmp[1])\n",
    "        tmp1 = []\n",
    "        tmp2 = []\n",
    "        for batch_size in batch_sizes:\n",
    "            log_file = '{}/{}_{}_distributed.log'.format(prefix, GPU_COUNT, batch_size)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            assert os.path.exists(log_file), \"{} does not exist\".format(log_file)\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch training time\", line):\n",
    "                    tmp2.append(float(line.split(' ')[4])) # In ms\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        tmp1.append([shared_e2e_error, active_error])\n",
    "                    if re.search(\"Baseline error:\", line):\n",
    "                        baseline_shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        baseline_active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        tmp1[-1].extend([baseline_shared_e2e_error, baseline_active_error])\n",
    "            prediction_file = '{}/{}_{}_distributed_prediction_{}.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            if os.path.exists(prediction_file):\n",
    "                for line in open(prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        tmp1[-1].extend([e2e_error])\n",
    "        errors[gpu].append(tmp1)\n",
    "        actual_time[gpu].append(tmp2)\n",
    "# shared_e2e, active, baseline, active_baseline, (e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "overall_shared_total = [abs(c[0]) for gpu in errors.keys() for task in errors[gpu] for c in task]\n",
    "error_shared_total = {\n",
    "    gpu: [abs(c[0]) for task in errors[gpu] for c in task] for gpu in errors.keys()\n",
    "}\n",
    "overall_active = [abs(c[1]) for gpu in errors.keys() for task in errors[gpu] for c in task]\n",
    "error_active = {\n",
    "    gpu: [abs(c[1]) for task in errors[gpu] for c in task] for gpu in errors.keys()\n",
    "}\n",
    "overall_baseline_shared_total = [abs(c[2]) for gpu in errors.keys() for task in errors[gpu] for c in task]\n",
    "error_baseline_shared_total = {\n",
    "    gpu: [abs(c[2]) for task in errors[gpu] for c in task] for gpu in errors.keys()\n",
    "}\n",
    "\n",
    "# For Latex\n",
    "s1 = [\"Shared E2E & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_shared_total), min(overall_shared_total), max(overall_shared_total))]\n",
    "s2 = [\"Active & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_active), min(overall_active), max(overall_active))]\n",
    "s3 = [\"Baseline E2E & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_baseline_shared_total), min(overall_baseline_shared_total), max(overall_baseline_shared_total))]\n",
    "for gpu in error_active.keys():\n",
    "    s1.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_shared_total[gpu]), min(error_shared_total[gpu]), max(error_shared_total[gpu])))\n",
    "    s2.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_active[gpu]), min(error_active[gpu]), max(error_active[gpu])))\n",
    "    s3.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_baseline_shared_total[gpu]), min(error_baseline_shared_total[gpu]), max(error_baseline_shared_total[gpu])))\n",
    "print(' & '.join(s1) + \"\\\\\\\\\")\n",
    "print(' & '.join(s2) + \"\\\\\\\\\")\n",
    "print(' & '.join(s3) + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.5\n",
    "task_gap = 6\n",
    "for gpu, error in errors.items():\n",
    "    yticks1 = [-20, -10, 0, 10, 20]\n",
    "    yticks2 = [-70, -50]\n",
    "    yticks3 = [6, 8.5, 11, 13.5, 16]\n",
    "    yticks4 = [0, 2.5]\n",
    "    yticklabels1 = [\"{:.0f}%\".format(x) for x in yticks1]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    gs = GridSpec(2, 2, height_ratios=[4, 1])\n",
    "    ax1 = fig.add_subplot(gs.new_subplotspec((0, 0), colspan=2))\n",
    "    ax2 = fig.add_subplot(gs.new_subplotspec((1, 0), colspan=2))\n",
    "    ax3 = ax1.twinx()\n",
    "    ax4 = ax2.twinx()\n",
    "\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.set_xticks([])\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.tick_params(labeltop=\"off\")\n",
    "    ax2.xaxis.tick_bottom()\n",
    "    ax3.spines['bottom'].set_visible(False)\n",
    "    ax3.set_xticks([])\n",
    "    ax4.spines['top'].set_visible(False)\n",
    "    ax4.tick_params(labeltop=\"off\")\n",
    "    ax4.xaxis.tick_bottom()\n",
    "\n",
    "    e0 = [] # shared\n",
    "    e1 = [] # active\n",
    "    eb = [] # baseline\n",
    "    e3 = [] # standalone\n",
    "    x0 = []\n",
    "    x1 = []\n",
    "    xb = []\n",
    "    x3 = []\n",
    "    xt = []\n",
    "    yt = []\n",
    "    batch_width = 0\n",
    "    xticks = []\n",
    "    for idy, e in enumerate(error):\n",
    "        xtt = []\n",
    "        ytt = []\n",
    "        batch_width += 2\n",
    "        batch_gap = 0\n",
    "        first = -1\n",
    "        last = -1\n",
    "        for idx, ee in enumerate(e):\n",
    "            e0.append(ee[0])\n",
    "            e1.append(ee[1])\n",
    "            eb.append(ee[2])\n",
    "            e3.append(ee[3])\n",
    "            x0.append(task_gap * batch_width - 2 + batch_gap - w * 2)\n",
    "            xb.append(task_gap * batch_width - 2 + batch_gap - w)\n",
    "            x1.append(task_gap * batch_width - 2 + batch_gap)\n",
    "            x3.append(task_gap * batch_width - 2 + batch_gap + w)\n",
    "            if idx == 0:\n",
    "                first = x1[-1]\n",
    "            batch_gap += 2\n",
    "            xtt.append(task_gap * batch_width - 2 + batch_gap - w * 4)\n",
    "        uhhh = ax3.plot(xtt, actual_time[gpu][idy], color=plt.get_cmap(\"tab10\")(1), marker=\"s\")\n",
    "        last = xb[-1]\n",
    "        xticks.append((first + last) / 2)\n",
    "\n",
    "    ax1.bar(x0, e0, width=w, color=plt.get_cmap(\"Blues\")(180), label='Prediction')\n",
    "    ax1.bar(xb, eb, width=w, color=plt.get_cmap(\"Greys\")(120), label='Baseline')\n",
    "    ax2.bar(xb, eb, width=w, color=plt.get_cmap(\"Greys\")(120), label='Baseline')\n",
    "\n",
    "    ax1.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax2.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax3.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax4.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax1.set_ylim(yticks1[0]-w*16, yticks1[-1]+w*16)\n",
    "    ax2.set_ylim(yticks2[0]-w*4, yticks2[-1]+w*4)\n",
    "    ax3.set_ylim(yticks3[0]-w*4, yticks3[-1]+w*4)\n",
    "    ax4.set_ylim(yticks4[0]-w, yticks4[-1]+w)\n",
    "\n",
    "    ax1.set_yticks(yticks1)\n",
    "    ax2.set_yticks(yticks2)\n",
    "    ax2.set_xticks(xticks)\n",
    "    ax2.set_xticklabels([\"Task_{}\".format(x) for x in range(len(tasks[gpu]))], fontsize=12)\n",
    "    heavy_label_idx = list(range(len(tasks[gpu])//4)) + list(range((len(tasks[gpu])//4*2), (len(tasks[gpu])//4*3)))\n",
    "    for i in heavy_label_idx:\n",
    "        ax2.get_xticklabels()[i].set_color(\"Purple\")\n",
    "    ax3.set_yticks(yticks3)\n",
    "    ax4.set_yticks(yticks4)\n",
    "\n",
    "    ax1.set_title(\"{}x{}\".format(GPU_COUNT, gpu), fontsize=15)\n",
    "    ax1.grid(axis='y')\n",
    "    xes = [zip(x0, e0), zip(xb, eb)]\n",
    "    count = 0\n",
    "    for xe in xes:\n",
    "        for x, e in xe:\n",
    "            if yticks2[-1]+w*4 < e < yticks1[0]-w*16:\n",
    "                ax1.text(x+0.3, yticks1[0]+4*(count % 3 - 1), \"{:.1f}%\".format(e), horizontalalignment='center', verticalalignment='center', size=12)\n",
    "                count += 1\n",
    "    axis_break1 = yticks2[-1] + w*4\n",
    "    axis_break2 = yticks1[0] - w*16\n",
    "    l = 2  # \"break\" line length\n",
    "    x_min = min(x0) - task_gap\n",
    "    x_max = max(xb) + task_gap\n",
    "    kwargs = dict(color=\"k\", clip_on=False, linewidth=1)\n",
    "    ax1.plot((x_min - l, x_min + l), (axis_break2, axis_break2), **kwargs) # top-left\n",
    "    ax1.plot((x_max - l, x_max + l), (axis_break2, axis_break2), **kwargs) # top-right\n",
    "    ax2.plot((x_min - l, x_min + l), (axis_break1, axis_break1), **kwargs) # bottom-left\n",
    "    ax2.plot((x_max - l, x_max + l), (axis_break1, axis_break1), **kwargs) # bottom-right\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles += uhhh\n",
    "    labels += [\"Actual Time\"]\n",
    "    fig.legend(handles, labels, loc=[0.74, 0.88], ncol=len(labels))\n",
    "    fig.text(-0.01, 0.5, 'Prediction Error', va='center', rotation=90, fontsize=14)\n",
    "    fig.text(0.99, 0.5, 'Iteration Time (ms)', va='center', rotation=-90, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}/data/multi_e2e_{}.pdf'.format(PM_HOME, gpu.lower()), bbox_inches='tight')\n",
    "    plt.savefig('{}/data/multi_e2e_{}.png'.format(PM_HOME, gpu.lower()), bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "other_errors = {}\n",
    "other_time = {}\n",
    "models = [\"BERT\", \"GPT2\", \"XLNet\"]\n",
    "xticklabels = []\n",
    "for gpu in gpus:\n",
    "    home = home_template.format(nodes[gpu])\n",
    "    other_errors[gpu] = {}\n",
    "    other_time[gpu] = {}\n",
    "    for model in models:\n",
    "        prefix = '{}/data/{}/e2e/{}/barrier_bucketed_allreduce/25'.format(home, gpu, model.lower())\n",
    "        if model not in other_errors[gpu].keys():\n",
    "            other_errors[gpu][model] = []\n",
    "        if model not in other_time[gpu].keys():\n",
    "            other_time[gpu][model] = []\n",
    "        for batch_size in nlp_batch_sizes:\n",
    "            if model == \"XLNet\":\n",
    "                batch_size = batch_size // 2\n",
    "            if (model == \"BERT\" and batch_size == 64) or (model == \"XLNet\" and batch_size == 32): # OOM\n",
    "                continue\n",
    "            xticklabels.append(batch_size)\n",
    "            log_file = '{}/{}_{}_distributed.log'.format(prefix, GPU_COUNT, batch_size)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch training time\", line):\n",
    "                    other_time[gpu][model].append(float(line.split(' ')[4])) # In ms\n",
    "                    break\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        other_errors[gpu][model].append([shared_e2e_error, active_error])\n",
    "                    if re.search(\"Baseline error:\", line):\n",
    "                        baseline_shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        baseline_active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        other_errors[gpu][model][-1].extend([baseline_shared_e2e_error, baseline_active_error])\n",
    "            prediction_file = '{}/{}_{}_distributed_prediction_{}.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            if os.path.exists(prediction_file):\n",
    "                for line in open(prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        e2e_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        other_errors[gpu][model][-1].append(e2e_error)\n",
    "# shared_e2e, active, baseline, active_baseline, (e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gap = 0.9\n",
    "nlp_xticklabels = xticklabels[:(len(xticklabels)//2)]\n",
    "nlp_yticks1 = [-40, -30, -20, -10, 0, 10]\n",
    "nlp_yticks2 = [0, 350, 700, 1050, 1400, 1750]\n",
    "yticklabels = [\"{:.0f}%\".format(x) for x in nlp_yticks1]\n",
    "bar_width = 0.2\n",
    "for gpu, error in other_errors.items():\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.gca()\n",
    "    ax2 = ax.twinx()\n",
    "    e1 = []\n",
    "    e2 = []\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    c = 0\n",
    "    for model, e in error.items():\n",
    "        model_gap = batch_gap * len(e) + 0.5\n",
    "        xtt = []\n",
    "        cc = 0\n",
    "        for ee in e:\n",
    "            e1.append(ee[0]) # Prediction\n",
    "            e2.append(ee[2]) # Baseline\n",
    "            x1.append(c - 2 + cc - bar_width)\n",
    "            x2.append(c - 2 + cc)\n",
    "            xtt.append(c - 2 + cc - (bar_width // 2))\n",
    "            cc += batch_gap\n",
    "        c += model_gap\n",
    "\n",
    "        ax.text((xtt[0] + xtt[-1] - batch_gap) / 2, nlp_yticks1[-1]+2, model, fontsize=12, transform=ax.transData)\n",
    "        uhhh = ax2.plot(xtt, other_time[gpu][model], color=plt.get_cmap(\"tab10\")(1), marker=\"s\")\n",
    "\n",
    "    ax.bar(x1, e1, width=bar_width, color=plt.get_cmap(\"Blues\")(180), label=\"Prediction\")\n",
    "    ax.bar(x2, e2, width=bar_width, color=plt.get_cmap(\"Greys\")(120), label=\"Baseline\")\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xticks([(x+y)/2 for x,y in zip(x1,x2)])\n",
    "    ax.set_xticklabels(nlp_xticklabels, fontsize=12)\n",
    "    ax.set_title(\"{}x{}\".format(GPU_COUNT, gpu), fontsize=15)\n",
    "    ax.set_yticks(nlp_yticks1)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_ylim(nlp_yticks1[0]-5, nlp_yticks1[-1]+5)\n",
    "    for xe in [zip(x1, e1), zip(x2, e2)]:\n",
    "        for x, e in xe:\n",
    "            if e > nlp_yticks1[-1]+5:\n",
    "                ax.text(x-0.1, nlp_yticks1[-1]-2, \"{:.1f}%\".format(e), horizontalalignment='center', verticalalignment='center', size=12)\n",
    "    ax2.set_ylim(nlp_yticks2[0]-175, nlp_yticks2[-1]+175)\n",
    "    ax2.set_yticks(nlp_yticks2)\n",
    "    ax2.set_yticklabels([str(x) for x in nlp_yticks2])\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles += uhhh\n",
    "    labels += [\"Actual Time\"]\n",
    "    fig.legend(handles, labels, loc=[0.08, 0.16], ncol=len(labels))\n",
    "    fig.text(-0.01, 0.5, 'Prediction Error', va='center', rotation=90, fontsize=14)\n",
    "    fig.text(0.99, 0.5, 'Iteration Time (ms)', va='center', rotation=-90, fontsize=14)\n",
    "    fig.text(0.45, -0.01, \"Batch Size\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}/data/multi_e2e_others_{}.pdf'.format(PM_HOME, gpu.lower()), bbox_inches='tight')\n",
    "    plt.savefig('{}/data/multi_e2e_others_{}.png'.format(PM_HOME, gpu.lower()), bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharding Config Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "batch_size = 4096\n",
    "selection_actual_time = {}\n",
    "selection_predicted_time = {}\n",
    "for gpu in gpus:\n",
    "    home = home_template.format(nodes[gpu])\n",
    "    selection_actual_time[gpu] = []\n",
    "    selection_predicted_time[gpu] = []\n",
    "    for idx, task in enumerate(tasks[gpu]):\n",
    "        if idx % 10 >= 5: # Heavy tasks only\n",
    "            continue\n",
    "        tmp = task.split(',')\n",
    "        tmp1 = {}\n",
    "        tmp2 = {}\n",
    "        dirs = glob.glob('{}/data/{}/e2e/DLRM_open_source/{}/{}/f/*/barrier_bucketed_allreduce/25'.format(home, gpu, tmp[0], tmp[1]))\n",
    "        ff = filter(lambda f: os.path.isdir(f), dirs)\n",
    "        for prefix in ff:\n",
    "            sharder = prefix.split('/')[-3]\n",
    "            log_file = '{}/{}_{}_distributed.log'.format(prefix, GPU_COUNT, batch_size)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            assert os.path.exists(log_file), \"{} does not exist\".format(log_file)\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            if not os.path.exists(shared_prediction_file):\n",
    "                continue\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch training time\", line):\n",
    "                    tmp1[sharder] = float(line.split(' ')[4]) # In ms\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    tmp2[sharder] = float(line.split(' ')[2].strip(',')) / 1e3 # In ms\n",
    "                    break\n",
    "        selection_actual_time[gpu].append(tmp1)\n",
    "        selection_predicted_time[gpu].append(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "w = 0.5\n",
    "task_gap = 10\n",
    "yticks = [6, 9, 12, 15, 18, 21]\n",
    "for idy, gpu in enumerate(gpus):\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    ax = fig.gca()\n",
    "    batch_width = 0\n",
    "    xticks = []\n",
    "    for idx, e in enumerate(selection_predicted_time[gpu]):\n",
    "        xtt = []\n",
    "        batch_width += 2\n",
    "        sharder_gap = 0\n",
    "        for sharder, ee in e.items():\n",
    "            sharder_gap += 1.5\n",
    "            xtt.append(task_gap * batch_width - 2 + sharder_gap - w * 4)\n",
    "        h1, = ax.plot(xtt, [t for _, t in selection_predicted_time[gpu][idx].items()], color=plt.get_cmap(\"Blues\")(180), marker=\"s\", label='Prediction')\n",
    "        h2, = ax.plot(xtt, [t for _, t in selection_actual_time[gpu][idx].items()], color=plt.get_cmap(\"tab10\")(1), marker=\"s\", label='Actual Time')\n",
    "        xticks.append((xtt[0] + xtt[-1]) / 2)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([\"Task_{}\".format(x) for x in range(len(tasks[gpu])) if x % 10 < 5], fontsize=12)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_title(\"{}x{}\".format(GPU_COUNT, gpu), fontsize=15)\n",
    "    ax.grid(axis='y')\n",
    "    fig.legend(handles=[h1, h2], loc=[0.78, 0.85], ncols=2)\n",
    "    fig.text(0.1, 0.5, 'Iteration Time (ms)', va='center', rotation=90, fontsize=14)\n",
    "    plt.savefig('{}/data/config_selection_{}.pdf'.format(PM_HOME, gpu.lower()), bbox_inches='tight')\n",
    "    plt.savefig('{}/data/config_selection_{}.png'.format(PM_HOME, gpu.lower()), bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eg_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "eg_comm = {}\n",
    "actual_eg_comm = {}\n",
    "for gpu in gpus:\n",
    "    home = home_template.format(nodes[gpu])\n",
    "    with open(\"{}/benchmark/tasks_{}x{}_permanent.txt\".format(home, GPU_COUNT, gpu), 'r') as f:\n",
    "        tasks[gpu] = [line.strip() for line in f.readlines()]\n",
    "    eg_comm[gpu] = []\n",
    "    actual_eg_comm[gpu] = []\n",
    "    for task in tasks[gpu]:\n",
    "        tmp = task.split(',')\n",
    "        prefix = \"{}/data/{}/e2e/DLRM_open_source/{}/{}/f/size_lookup_greedy/barrier_bucketed_allreduce/25\".format(home, gpu, tmp[0], tmp[1])\n",
    "        tmp1 = []\n",
    "        tmp2 = []\n",
    "        for batch_size in batch_sizes:\n",
    "            log_file = '{}/{}_{}_distributed_0_summary_{}.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            # assert os.path.exists(log_file), \"{} does not exist\".format(log_file)\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"eg_comm\", line):\n",
    "                    tmp2.append(float(line.split(' ')[-1]))\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    if re.search(\"eg_comm:\", line):\n",
    "                        tmp1.append([float(line.split(' ')[-1])])\n",
    "            prediction_file = '{}/{}_{}_distributed_prediction_{}.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            if os.path.exists(prediction_file):\n",
    "                for line in open(prediction_file, 'r'):\n",
    "                    if re.search(\"eg_comm:\", line):\n",
    "                        tmp1[-1].extend([float(line.split(' ')[-1])])\n",
    "        eg_comm[gpu].append(tmp1)\n",
    "        actual_eg_comm[gpu].append(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_eg_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhongyi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
