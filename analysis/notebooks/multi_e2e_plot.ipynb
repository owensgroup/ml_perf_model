{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats.mstats import gmean\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, sys, glob\n",
    "\n",
    "plt.rcParams['figure.max_open_warning'] = 50\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "PM_HOME = os.getcwd() + \"/../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 4\n",
    "GPU_NAME = \"GV100\"\n",
    "gpus = [GPU_NAME]\n",
    "batch_sizes = [512, 1024, 2048, 4096]\n",
    "nlp_batch_sizes = [8, 16, 32, 64]\n",
    "trimmed_iters = 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU DLRM E2E Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "errors = {}\n",
    "actual_time = {}\n",
    "for gpu in gpus:\n",
    "    errors[gpu] = {}\n",
    "    actual_time[gpu] = {}\n",
    "    dirs = glob.glob('{}/data/{}/e2e/DLRM_open_source/*/*/f/size_lookup_greedy/barrier_bucketed_allreduce/25'.format(PM_HOME, gpu))\n",
    "    ff = filter(lambda f: os.path.isdir(f), dirs)\n",
    "    for prefix in ff:\n",
    "        task = \"{},{}\".format(prefix.split('/')[-6], prefix.split('/')[-5])\n",
    "        if task not in errors[gpu].keys():\n",
    "            errors[gpu][task] = []\n",
    "        if task not in actual_time[gpu].keys():\n",
    "            actual_time[gpu][task] = []\n",
    "        for batch_size in batch_sizes:\n",
    "            log_file = '{}/{}_{}_distributed.log'.format(prefix, GPU_COUNT, batch_size)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            assert os.path.exists(log_file), \"{} does not exist\".format(log_file)\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch training time\", line):\n",
    "                    actual_time[gpu][task].append(float(line.split(' ')[4])) # In ms\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        errors[gpu][task].append([shared_e2e_error, active_error])\n",
    "                    if re.search(\"Baseline error:\", line):\n",
    "                        baseline_shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        baseline_active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        errors[gpu][task][-1].extend([baseline_shared_e2e_error, baseline_active_error])\n",
    "            prediction_file = '{}/{}_{}_distributed_prediction_{}.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            if os.path.exists(prediction_file):\n",
    "                for line in open(prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        errors[gpu][task][-1].extend([e2e_error])\n",
    "# shared_e2e, active, baseline, active_baseline, (e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "overall_shared_total = [abs(c[0]) for device in errors.keys() for task in errors[device].keys() for c in errors[device][task]]\n",
    "error_shared_total = {\n",
    "    device: [abs(c[0]) for task in errors[device].keys() for c in errors[device][task]] for device in errors.keys()\n",
    "}\n",
    "overall_active = [abs(c[1]) for device in errors.keys() for task in errors[device].keys() for c in errors[device][task]]\n",
    "error_active = {\n",
    "    device: [abs(c[1]) for task in errors[device].keys() for c in errors[device][task]] for device in errors.keys()\n",
    "}\n",
    "overall_baseline_shared_total = [abs(c[2]) for device in errors.keys() for task in errors[device].keys() for c in errors[device][task]]\n",
    "error_baseline_shared_total = {\n",
    "    device: [abs(c[2]) for task in errors[device].keys() for c in errors[device][task]] for device in errors.keys()\n",
    "}\n",
    "\n",
    "# For Latex\n",
    "s1 = [\"Shared E2E & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_shared_total), min(overall_shared_total), max(overall_shared_total))]\n",
    "s2 = [\"Active & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_active), min(overall_active), max(overall_active))]\n",
    "s3 = [\"Baseline E2E & \\\\bf{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(overall_baseline_shared_total), min(overall_baseline_shared_total), max(overall_baseline_shared_total))]\n",
    "for device in error_active.keys():\n",
    "    s1.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_shared_total[device]), min(error_shared_total[device]), max(error_shared_total[device])))\n",
    "    s2.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_active[device]), min(error_active[device]), max(error_active[device])))\n",
    "    s3.append(\"{:.2f}\\% & {:.2f}\\% & {:.2f}\\%\".format(gmean(error_baseline_shared_total[device]), min(error_baseline_shared_total[device]), max(error_baseline_shared_total[device])))\n",
    "print(' & '.join(s1) + \"\\\\\\\\\")\n",
    "print(' & '.join(s2) + \"\\\\\\\\\")\n",
    "print(' & '.join(s3) + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "w = 0.5\n",
    "task_gap = 6\n",
    "for device, error in errors.items():\n",
    "    yticks1 = [-20, -10, 0, 10, 20]\n",
    "    yticks2 = [-70, -50]\n",
    "    yticks3 = [6, 8.5, 11, 13.5, 16]\n",
    "    yticks4 = [0, 2.5]\n",
    "    yticklabels1 = [\"{:.0f}%\".format(x) for x in yticks1]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    gs = GridSpec(2, 2, height_ratios=[4, 1])\n",
    "    ax1 = fig.add_subplot(gs.new_subplotspec((0, 0), colspan=2))\n",
    "    ax2 = fig.add_subplot(gs.new_subplotspec((1, 0), colspan=2))\n",
    "    ax3 = ax1.twinx()\n",
    "    ax4 = ax2.twinx()\n",
    "\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.set_xticks([])\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.tick_params(labeltop=\"off\")\n",
    "    ax2.xaxis.tick_bottom()\n",
    "    ax3.spines['bottom'].set_visible(False)\n",
    "    ax3.set_xticks([])\n",
    "    ax4.spines['top'].set_visible(False)\n",
    "    ax4.tick_params(labeltop=\"off\")\n",
    "    ax4.xaxis.tick_bottom()\n",
    "\n",
    "    e0 = [] # shared\n",
    "    e1 = [] # active\n",
    "    eb = [] # baseline\n",
    "    e3 = [] # standalone\n",
    "    x0 = []\n",
    "    x1 = []\n",
    "    xb = []\n",
    "    x3 = []\n",
    "    xt = []\n",
    "    yt = []\n",
    "    batch_width = 0\n",
    "    tasks = []\n",
    "    xticks = []\n",
    "    for task, e in error.items():\n",
    "        tasks.append(task)\n",
    "        xtt = []\n",
    "        ytt = []\n",
    "        batch_width += 2\n",
    "        batch_gap = 0\n",
    "        first = -1\n",
    "        last = -1\n",
    "        for idx, ee in enumerate(e):\n",
    "            e0.append(ee[0])\n",
    "            e1.append(ee[1])\n",
    "            eb.append(ee[2])\n",
    "            e3.append(ee[3])\n",
    "            x0.append(task_gap * batch_width - 2 + batch_gap - w * 2)\n",
    "            xb.append(task_gap * batch_width - 2 + batch_gap - w)\n",
    "            x1.append(task_gap * batch_width - 2 + batch_gap)\n",
    "            x3.append(task_gap * batch_width - 2 + batch_gap + w)\n",
    "            if idx == 0:\n",
    "                first = x1[-1]\n",
    "            batch_gap += 2\n",
    "            xtt.append(task_gap * batch_width - 2 + batch_gap - w * 4)\n",
    "        uhhh = ax3.plot(xtt, actual_time[device][task], color=plt.get_cmap(\"tab10\")(1), marker=\"s\")\n",
    "        last = xb[-1]\n",
    "        xticks.append((first + last) / 2)\n",
    "\n",
    "    ax1.bar(x0, e0, width=w, color=plt.get_cmap(\"Blues\")(180), label='Prediction')\n",
    "    ax1.bar(xb, eb, width=w, color=plt.get_cmap(\"Greys\")(120), label='Baseline')\n",
    "    ax2.bar(xb, eb, width=w, color=plt.get_cmap(\"Greys\")(120), label='Baseline')\n",
    "\n",
    "    ax1.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax2.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax3.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax4.set_xlim(min(x0) - task_gap, max(xb) + task_gap)\n",
    "    ax1.set_ylim(yticks1[0]-w*16, yticks1[-1]+w*16)\n",
    "    ax2.set_ylim(yticks2[0]-w*4, yticks2[-1]+w*4)\n",
    "    ax3.set_ylim(yticks3[0]-w*4, yticks3[-1]+w*4)\n",
    "    ax4.set_ylim(yticks4[0]-w, yticks4[-1]+w)\n",
    "\n",
    "    ax1.set_yticks(yticks1)\n",
    "    ax2.set_yticks(yticks2)\n",
    "    ax2.set_xticks(xticks)\n",
    "    ax2.set_xticklabels([\"Task_{}\".format(x) for x in range(len(tasks))], fontsize=12, rotation=25)\n",
    "    ax3.set_yticks(yticks3)\n",
    "    ax4.set_yticks(yticks4)\n",
    "\n",
    "    ax1.set_title(device, fontsize=15)\n",
    "    ax1.grid(axis='y')\n",
    "    xes = [zip(x0, e0), zip(xb, eb)]\n",
    "    sign = 1\n",
    "    for xe in xes:\n",
    "        for x, e in xe:\n",
    "            if yticks2[-1]+w*4 < e < yticks1[0]-w*16:\n",
    "                ax1.text(x+0.3, yticks1[0]+3*sign, \"{:.1f}%\".format(e), horizontalalignment='center', verticalalignment='center', size=12)\n",
    "                sign = -sign\n",
    "    axis_break1 = yticks2[-1] + w*4\n",
    "    axis_break2 = yticks1[0] - w*16\n",
    "    l = 2  # \"break\" line length\n",
    "    x_min = min(x0) - task_gap\n",
    "    x_max = max(xb) + task_gap\n",
    "    kwargs = dict(color=\"k\", clip_on=False, linewidth=1)\n",
    "    ax1.plot((x_min - l, x_min + l), (axis_break2, axis_break2), **kwargs)# top-left\n",
    "    ax1.plot((x_max - l, x_max + l), (axis_break2, axis_break2), **kwargs)# top-right\n",
    "    ax2.plot((x_min - l, x_min + l), (axis_break1, axis_break1), **kwargs)# bottom-left\n",
    "    ax2.plot((x_max - l, x_max + l), (axis_break1, axis_break1), **kwargs)# bottom-right\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles += uhhh\n",
    "    labels += [\"Actual Time\"]\n",
    "    fig.legend(handles, labels, loc=[0.7, 0.88], ncol=len(labels))\n",
    "    fig.text(-0.01, 0.5, 'Prediction Error', va='center', rotation=90, fontsize=14)\n",
    "    fig.text(0.99, 0.5, 'Iteration Time (ms)', va='center', rotation=-90, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}/data/multi_gpu_e2e.pdf'.format(PM_HOME), bbox_inches='tight')\n",
    "    plt.savefig('{}/data/multi_gpu_e2e.png'.format(PM_HOME), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(idx, t) for idx, t in enumerate(tasks)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "other_errors = {}\n",
    "other_time = {}\n",
    "models = [\"bert\", \"gpt2\"]\n",
    "for gpu in gpus:\n",
    "    other_errors[gpu] = {}\n",
    "    other_time[gpu] = {}\n",
    "    for model in models:\n",
    "        prefix = '{}/data/{}/e2e/{}/barrier_bucketed_allreduce/25'.format(PM_HOME, gpu, model)\n",
    "        if model not in errors[gpu].keys():\n",
    "            other_errors[gpu][model] = []\n",
    "        if model not in actual_time[gpu].keys():\n",
    "            other_time[gpu][model] = []\n",
    "        for batch_size in nlp_batch_sizes:\n",
    "            if (model == \"bert\" and batch_size == 64): # OOM\n",
    "                continue\n",
    "            log_file = '{}/{}_{}_distributed.log'.format(prefix, GPU_COUNT, batch_size)\n",
    "            if not os.path.exists(log_file):\n",
    "                continue\n",
    "            for line in open(log_file, 'r'):\n",
    "                if re.search(\"Overall per-batch training time\", line):\n",
    "                    other_time[gpu][model].append(float(line.split(' ')[4])) # In ms\n",
    "                    break\n",
    "            shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "            if os.path.exists(shared_prediction_file):\n",
    "                for line in open(shared_prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        other_errors[gpu][model].append([shared_e2e_error, active_error])\n",
    "                    if re.search(\"Baseline error:\", line):\n",
    "                        baseline_shared_e2e_error = float(line.split(' ')[2].rstrip(',%\\n'))\n",
    "                        baseline_active_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        other_errors[gpu][model][-1].extend([baseline_shared_e2e_error, baseline_active_error])\n",
    "            prediction_file = '{}/{}_{}_distributed_prediction_{}.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "            if os.path.exists(prediction_file):\n",
    "                for line in open(prediction_file, 'r'):\n",
    "                    if re.search(\"Prediction error:\", line):\n",
    "                        e2e_error = float(line.split(' ')[3].rstrip(',%\\n'))\n",
    "                        other_errors[gpu][model][-1].append(e2e_error)\n",
    "# shared_e2e, active, baseline, active_baseline, (e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.gca()\n",
    "model_gap = 3.2\n",
    "nlp_xticklabels = nlp_batch_sizes[:-1] + nlp_batch_sizes\n",
    "nlp_yticks1 = [-30, -20, -10, 0, 10]\n",
    "nlp_yticks2 = [200, 400, 600, 800, 1000]\n",
    "yticklabels = [\"{:.0f}%\".format(x) for x in nlp_yticks1]\n",
    "for device, error in other_errors.items():\n",
    "    ax2 = ax.twinx()\n",
    "    e1 = []\n",
    "    e2 = []\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    c = 0\n",
    "    for model, e in error.items():\n",
    "        xtt = []\n",
    "        c += 0.5\n",
    "        cc = 0\n",
    "        for ee in e:\n",
    "            e1.append(ee[0])\n",
    "            e2.append(ee[2])\n",
    "            x1.append(model_gap * c - 2 + cc - 0.1)\n",
    "            x2.append(model_gap * c - 2 + cc)\n",
    "            cc += 0.5\n",
    "            xtt.append(model_gap * c - 2 + cc - 0.5)\n",
    "\n",
    "        ax.text((xtt[0] + xtt[-1]) / 2, nlp_yticks1[-1]+2, model, fontsize=12, transform=ax.transData)\n",
    "        ax2.plot(xtt, other_time[device][model], color=plt.get_cmap(\"tab10\")(1), marker=\"s\")\n",
    "\n",
    "    ax.bar(x1, e1, width=0.1, color=plt.get_cmap(\"Blues\")(180), label=\"Prediction\")\n",
    "    ax.bar(x2, e2, width=0.1, color=plt.get_cmap(\"Greys\")(120), label=\"Baseline\")\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xticks([(x+y)/2 for x,y in zip(x1,x2)])\n",
    "    ax.set_xticklabels(nlp_xticklabels, fontsize=12)\n",
    "    ax.set_title(device, fontsize=15)\n",
    "    ax.set_yticks(nlp_yticks1)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_ylim(nlp_yticks1[0]-5, nlp_yticks1[-1]+5)\n",
    "    for xe in [zip(x1, e1), zip(x2, e2)]:\n",
    "        for x, e in xe:\n",
    "            if e > nlp_yticks1[-1]+5:\n",
    "                ax.text(x-0.1, nlp_yticks1[-1]-2, \"{:.1f}%\".format(e), horizontalalignment='center', verticalalignment='center', size=12)\n",
    "    ax2.set_ylim(nlp_yticks2[0]-100, nlp_yticks2[-1]+100)\n",
    "    ax2.set_yticks(nlp_yticks2)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles += uhhh\n",
    "labels += [\"Actual Time\"]\n",
    "fig.legend(handles, labels, loc=[0.08, 0.16], ncol=len(labels))\n",
    "fig.text(-0.01, 0.5, 'Prediction Error', va='center', rotation=90, fontsize=14)\n",
    "fig.text(0.99, 0.5, 'Iteration Time (ms)', va='center', rotation=-90, fontsize=14)\n",
    "fig.text(0.45, -0.01, \"Batch Size\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('{}/data/multi_e2e_others.pdf'.format(PM_HOME), bbox_inches='tight')\n",
    "plt.savefig('{}/data/multi_e2e_others.png'.format(PM_HOME), bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharding Config Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically get errors from logs\n",
    "batch_size = 4096\n",
    "# errors = {}\n",
    "actual_time = {}\n",
    "predicted_time = {}\n",
    "for gpu in gpus:\n",
    "    # errors[gpu] = {}\n",
    "    actual_time[gpu] = {}\n",
    "    predicted_time[gpu] = {}\n",
    "    dirs = glob.glob('{}/data/{}/e2e/DLRM_open_source/*/*/f/*/barrier_bucketed_allreduce/25'.format(PM_HOME, gpu))\n",
    "    ff = filter(lambda f: os.path.isdir(f), dirs)\n",
    "    for prefix in ff:\n",
    "        sharder = prefix.split('/')[-3]\n",
    "        task = \"{},{}\".format(prefix.split('/')[-6], prefix.split('/')[-5])\n",
    "        # if task not in errors[gpu].keys():\n",
    "        #     errors[gpu][task] = {}\n",
    "        if task not in actual_time[gpu].keys():\n",
    "            actual_time[gpu][task] = {}\n",
    "        if task not in predicted_time[gpu].keys():\n",
    "            predicted_time[gpu][task] = {}\n",
    "        log_file = '{}/{}_{}_distributed.log'.format(prefix, GPU_COUNT, batch_size)\n",
    "        if not os.path.exists(log_file):\n",
    "            continue\n",
    "        assert os.path.exists(log_file), \"{} does not exist\".format(log_file)\n",
    "        shared_prediction_file = '{}/{}_{}_distributed_prediction_{}_shared.log'.format(prefix, GPU_COUNT, batch_size, trimmed_iters)\n",
    "        if not os.path.exists(shared_prediction_file):\n",
    "            continue\n",
    "        assert os.path.exists(shared_prediction_file), \"{} does not exist\".format(shared_prediction_file)\n",
    "        for line in open(log_file, 'r'):\n",
    "            if re.search(\"Overall per-batch training time\", line):\n",
    "                actual_time[gpu][task][sharder] = float(line.split(' ')[4]) # In ms\n",
    "        if os.path.exists(shared_prediction_file):\n",
    "            for line in open(shared_prediction_file, 'r'):\n",
    "                predicted_time[gpu][task][sharder] = float(line.split(' ')[2].strip(',')) / 1e3 # In ms\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(gpus), ncols=1, figsize=(20, 7))\n",
    "count = 0\n",
    "w = 0.5\n",
    "task_gap = 10\n",
    "yticks = [-30, -20, -10, 0, 10, 20]\n",
    "yticks2 = [5, 7.5, 10, 12.5, 15]\n",
    "yticklabels = [\"{:.0f}%\".format(x) for x in yticks]\n",
    "for idx, gpu in enumerate(gpus):\n",
    "    ax = axes if len(gpus) == 1 else axes[idx]\n",
    "    batch_width = 0\n",
    "    for task, e in predicted_time[gpu].items():\n",
    "        xtt = []\n",
    "        batch_width += 2\n",
    "        sharder_gap = 0\n",
    "        for sharder, ee in e.items():\n",
    "            sharder_gap += 1.5\n",
    "            xtt.append(task_gap * batch_width - 2 + sharder_gap - w * 4)\n",
    "        ax1.plot(xtt, [t for _, t in actual_time[gpu][task].items()], color=plt.get_cmap(\"tab10\")(1), marker=\"s\")\n",
    "        ax1.plot(xtt, [t for _, t in predicted_time[gpu][task].items()], color=plt.get_cmap(\"Blues\")(180), marker=\"s\")\n",
    "    ax1.set_yticks(yticks2)\n",
    "\n",
    "handles, labels = (axes[-1] if isinstance(axes, list) else axes).get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=[0.7, 0.95], ncol=len(labels))\n",
    "fig.text(0.45, -0.01, \"Batch Size\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('{}/data/selection.pdf'.format(PM_HOME), bbox_inches='tight')\n",
    "plt.savefig('{}/data/selection.png'.format(PM_HOME), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhongyi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
