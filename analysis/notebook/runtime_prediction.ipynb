{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "bento_obj_id": "139888653724304"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from aiplatform.monitoring.atc import antipattern_detection, trace_utils\n",
    "from pprint import pprint\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from IPython.core.display import display, HTML\n",
    "from scipy.stats.mstats import gmean \n",
    "import argparse, logging, tempfile, json, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.rcParams['figure.max_open_warning'] = 100\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "def div_round_up(x, y):\n",
    "    return (x + y - 1) // y\n",
    "\n",
    "def error_mean_std(predicted, actual):\n",
    "    error = (predicted - actual).abs() / actual\n",
    "    return error.mean(), error.std()\n",
    "\n",
    "def p2f(x):\n",
    "    return float(x.strip('%')) / 100\n",
    "\n",
    "def remove_bw_suffix(x):\n",
    "    if x.endswith('GB/s'):\n",
    "        return float(x.strip('GB/s'))\n",
    "    elif x.endswith('MB/s'):\n",
    "        return float(x.strip('MB/s')) / 1000\n",
    "    # B/s\n",
    "    return float(x.strip('B/s')) / 1000000000\n",
    "\n",
    "def strip_util(x):\n",
    "    return float(x.strip('(').strip(')'))\n",
    "\n",
    "def preprocessing(stats):\n",
    "    stats = stats.dropna()\n",
    "    return stats\n",
    "\n",
    "def histogram(df, buckets, percentage=True):\n",
    "    for idx, bk in enumerate(buckets):\n",
    "        if idx > 0:\n",
    "            if percentage:\n",
    "                print(\"{}-{}%, {:.2f}%\".format(buckets[idx-1] * 100, bk * 100, 100 * len(df[(df.abs() < bk) & (df.abs() > buckets[idx-1])]) / len(df)))\n",
    "            else:\n",
    "                print(\"{}-{}, {:.2f}%\".format(buckets[idx-1], bk, 100 * len(df[(df < bk) & (df > buckets[idx-1])]) / len(df)))\n",
    "                \n",
    "# From Louis. Trim a long trace so that it eases the ATC processing\n",
    "def trim_trace(file_name, start, end):\n",
    "    assert (0 <= start and start <= 1 and 0 <= end and end <= 1 and start <= end)\n",
    "    with open(file_name) as trace_file:\n",
    "        trace = json.load(trace_file)\n",
    "        min_time = sys.maxsize\n",
    "        max_time = 0\n",
    "\n",
    "        for event in trace:\n",
    "            # print(event['ts'])\n",
    "            min_time = min(min_time, event['ts'])\n",
    "            max_time = max(max_time, event['ts'])\n",
    "\n",
    "        print(\"time range: {} {}\".format(min_time, max_time))\n",
    "        time_range = max_time - min_time\n",
    "        offset_start = start * time_range\n",
    "        offset_end = end * time_range\n",
    "        # offset from the start to the trimmed end\n",
    "        max_time = min_time + offset_end\n",
    "        # move the min time to the offset start\n",
    "        min_time += offset_start\n",
    "        print(\"trimmed time range: {} {}\".format(min_time, max_time))\n",
    "        trimmed_trace = [x for x in trace if x['ts'] > min_time and x['ts'] < max_time]\n",
    "        with open(\"trace_trimmed.json\", 'w') as out_file:\n",
    "            json.dump(trimmed_trace, out_file)\n",
    "\n",
    "# Code copied from //aiplatform/monitoring/atc\n",
    "def run_ATC():\n",
    "    # Initiate the logger\n",
    "    FORMAT = \"[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s\"\n",
    "    logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
    "    logger: logging.Logger = logging.getLogger(\"atc\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Disable logging if necessary\n",
    "    logging.disable(sys.maxsize)\n",
    "\n",
    "    base_trace = \"./trace_trimmed.json\"\n",
    "\n",
    "    trace = trace_utils.load_trace_json_file(base_trace)\n",
    "    base_trace_dir: str = tempfile.mkdtemp(prefix=\"base-trace_\")\n",
    "    (\n",
    "        iteration_start_base,\n",
    "        iteration_end_base,\n",
    "        all_events_base,\n",
    "        per_process_events_base,\n",
    "        per_thread_events_base,\n",
    "    ) = trace_utils.parse_trace_json(trace, base_trace_dir)\n",
    "    trace_utils.extract_insights_from_trace(\n",
    "        base_trace_dir,\n",
    "        all_events_base,\n",
    "        per_process_events_base,\n",
    "        per_thread_events_base,\n",
    "        iteration_start_base,\n",
    "        iteration_end_base,\n",
    "    )\n",
    "\n",
    "    antipattern_detection.save_all_antipatterns(\n",
    "        all_events_base,\n",
    "        per_process_events_base,\n",
    "        per_thread_events_base,\n",
    "        output_dir=base_trace_dir,\n",
    "    )\n",
    "    logger.info(\"output directory for base trace: {}\".format(base_trace_dir))\n",
    "\n",
    "    return base_trace_dir\n",
    "\n",
    "def list_to_tuple(lst):\n",
    "    return tuple(list_to_tuple(l) if isinstance(l, list) else l for l in lst) if lst is not None else None\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, e, dummy=False):\n",
    "        if dummy:\n",
    "            self.event = {\n",
    "                \"name\": \"dummy\",\n",
    "                \"ts\": -1,\n",
    "                \"dur\": -1,\n",
    "                \"cat\": \"Runtime\",\n",
    "                \"args\": {}\n",
    "            }\n",
    "        else:\n",
    "            assert (type(e) == dict)\n",
    "            self.event = e\n",
    "        self.parent = None\n",
    "        self.children = []\n",
    "        self.has_device_calls = False\n",
    "    def __str__(self):\n",
    "        return json.dumps(self.event, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "#     def __repr__(self):\n",
    "#         return json.dumps(self.event, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    def start_time(self):\n",
    "        if \"ts\" not in self.event.keys():\n",
    "            return None\n",
    "        return self.event[\"ts\"]\n",
    "    def duration(self):\n",
    "        if \"dur\" not in self.event.keys():\n",
    "            return None\n",
    "        return self.event[\"dur\"]\n",
    "    def category(self):\n",
    "        if \"cat\" not in self.event.keys():\n",
    "            raise TypeError(\"Unknown event type!\")\n",
    "        return self.event[\"cat\"]\n",
    "    def name(self):\n",
    "        if \"name\" not in self.event.keys():\n",
    "            raise TypeError(\"Name lost!\")\n",
    "        return self.event[\"name\"]\n",
    "    def is_sub_of(self, other):\n",
    "        assert (self.start_time() is not None and \\\n",
    "                self.duration() is not None and \\\n",
    "                other.start_time() is not None and \\\n",
    "                other.duration() is not None)\n",
    "        ls = other.start_time()\n",
    "        le = other.start_time() + other.duration()\n",
    "        es = self.start_time()\n",
    "        ee = self.start_time() + self.duration()\n",
    "        return ls <= es and le >= ee\n",
    "    def input_shape(self):\n",
    "        if \"args\" not in self.event.keys() or \"Input dims\" not in self.event[\"args\"].keys():\n",
    "            return (-1,)\n",
    "        return list_to_tuple(self.event[\"args\"][\"Input dims\"])\n",
    "    def output_shape(self):\n",
    "        if \"args\" not in self.event.keys() or \"Output dims\" not in self.event[\"args\"].keys():\n",
    "            return (-1,)\n",
    "        return list_to_tuple(self.event[\"args\"][\"Output dims\"])\n",
    "    def external_id(self):\n",
    "        if \"args\" not in self.event.keys():\n",
    "            return None\n",
    "\n",
    "        if (\"External id\" not in self.event[\"args\"].keys() and \\\n",
    "             \"external id\" not in self.event[\"args\"].keys()):\n",
    "            raise TypeError(\"External id lost!\")\n",
    "        \n",
    "        if self.category() == \"Operator\":\n",
    "            return self.event[\"args\"][\"External id\"]\n",
    "        else:\n",
    "            return self.event[\"args\"][\"external id\"]\n",
    "    def correlation_id(self):\n",
    "        if \"args\" not in self.event.keys() or self.category() == \"Operator\":\n",
    "            return None\n",
    "\n",
    "        if (\"correlation\" not in self.event[\"args\"].keys()):\n",
    "            raise TypeError(\"Correlation id lost!\")\n",
    "        return self.event[\"args\"][\"correlation\"]\n",
    "    def device(self):\n",
    "        if \"args\" not in self.event.keys() or \\\n",
    "            (\"Device\" not in self.event[\"args\"].keys() and \\\n",
    "            \"device\" not in self.event[\"args\"].keys()):\n",
    "            return None\n",
    "        if \"Device\" in self.event[\"args\"].keys():\n",
    "            return self.event[\"args\"][\"Device\"]\n",
    "        else:\n",
    "            return self.event[\"args\"][\"device\"]\n",
    "    def stream(self):\n",
    "        if \"args\" not in self.event.keys() or \"stream\" not in self.event[\"args\"].keys():\n",
    "            return None\n",
    "        return self.event[\"args\"][\"stream\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a forest to represent the event hierarchy as well as a data structure to hold the relation between ops and device calls\n",
    "########## cc #########\n",
    "# {\n",
    "#     ex_id1 : {\n",
    "#         caller: - (an op that has one or multiple device calls)\n",
    "#         callees: {\n",
    "#             cr_id1: {\n",
    "#                 launcher: - (cudaKernelLaunch)\n",
    "#                 executor: - (device kernel)\n",
    "#             }\n",
    "#             ...\n",
    "#         }\n",
    "#     }\n",
    "#     ...\n",
    "# }\n",
    "def process_event_hierarchy(two, skip_module=False, module_marker=\"## \"):\n",
    "    \n",
    "    # Get the \"grandest child\" event of a given leaf\n",
    "    # e.g. |------------ A --------------| The leaf event in the frontier currently being accessed\n",
    "    #         |------------B-----------|\n",
    "    #            |-----C------| The current \"grandest child\" of A, since D hasn't been added as A's child yet\n",
    "    #               |---D---| The event currently being processed\n",
    "    def get_grandest_child_event(leaf, event, depth=1):\n",
    "        if not event.is_sub_of(leaf):\n",
    "            return None\n",
    "        ret = leaf\n",
    "        for c in leaf.children:\n",
    "            grandest = get_grandest_child_event(c, event, depth+1)\n",
    "            if grandest is not None:\n",
    "                ret = grandest\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    roots = [] # All the root events that have no parents\n",
    "    leaves = [] # The event frontier of the processing\n",
    "    unaccounted = [] # Unaccounted events (not being used now)\n",
    "    cc = {} # caller / callee: key = external id, value = { caller event, callee events }\n",
    "    \n",
    "    # Sort the event lists and remove all events without a duration\n",
    "    duration_none = [e for e in two if \"dur\" not in e.keys()]\n",
    "    sorted_events = [Event(e) for e in two if e not in duration_none]\n",
    "    sorted_events = sorted(sorted_events, key=lambda x: (x.start_time(), -x.duration()))\n",
    "    \n",
    "    # Remove all leftovers from the last iteration and next iteration\n",
    "    start_idx = 0\n",
    "    end_idx = len(sorted_events) - 1\n",
    "    corrected_start_time = sorted_events[0].start_time()\n",
    "    corrected_end_time = sorted_events[-1].start_time()\n",
    "    # Start the analysis from the first module detected, if module is not to be skipped\n",
    "    for idx, x in enumerate(sorted_events):\n",
    "        ######## IMPORTANT ########\n",
    "        # Find the start of an iteration started with \"##\" without \":\". The first module should be \"## zero_grad ##\" though, \n",
    "        # but the current ATC code couldn't start the extraction exactly at there. \n",
    "        # Change TORCH_AUTOGRAD_GRAPHROOT in ATC's trace_utils.py does the trick\n",
    "        if not skip_module and x.name().startswith(module_marker) and \":\" not in x.name():\n",
    "            # The actual start time is the start time of the profiler enter call right before \"zero_grad\"\n",
    "            for idy, y in enumerate(reversed(sorted_events[:idx])):\n",
    "                if y.name() == \"profiler::_record_function_enter\":\n",
    "                    start_idx = idx - idy\n",
    "                    corrected_start_time = y.start_time()\n",
    "                    break\n",
    "            break\n",
    "\n",
    "    # End the analysis at the last event that has a duration. Set the corrected end time later.\n",
    "    for idx, x in enumerate(reversed(sorted_events)):\n",
    "        if x.duration() is not None:\n",
    "            end_idx = idx\n",
    "            break\n",
    "    sorted_events = sorted_events[start_idx:(len(sorted_events) - 1 - end_idx)]\n",
    "\n",
    "    for x in sorted_events:\n",
    "        # Get start, duration and end time of the current event\n",
    "        event_start = x.start_time()\n",
    "        event_duration = x.duration()\n",
    "        external_id = x.external_id()\n",
    "        correlation_id = x.correlation_id()\n",
    "\n",
    "        # Runtime events e.g. cudaLaunchKernel counted as host events\n",
    "        if x.category() == \"Operator\" or x.category() == \"Runtime\":\n",
    "            if event_start is None or event_duration is None:\n",
    "                print(\"Unaccounted event: {}\".format(x.event))\n",
    "                unaccounted.append(x)\n",
    "                continue\n",
    "            # Put all OPERATOR events with no device info into unaccounted (0 means None in the trace file)\n",
    "            # This usually work for events like aten::pin_memory, etc\n",
    "            if x.device() == 0:\n",
    "                unaccounted.append(x)\n",
    "                continue\n",
    "                \n",
    "            event_end = event_start + event_duration\n",
    "            corrected_end_time = max(event_end, corrected_end_time)\n",
    "\n",
    "            # Find parent of the current event from the frontier\n",
    "            parent_found = False\n",
    "            to_add_root = None\n",
    "            to_add_leaf = None\n",
    "            for l in leaves:\n",
    "                leaf_start = l.start_time()\n",
    "                leaf_end = leaf_start + l.duration()\n",
    "\n",
    "                # The current event is sub to leaf\n",
    "                if event_end <= leaf_end:\n",
    "                    # Add this event to the GRANDEST CHILD of the leaf that can sub it\n",
    "                    grandest = get_grandest_child_event(l, x)\n",
    "                    x.parent = grandest\n",
    "                    grandest.children.append(x)\n",
    "                    to_add_leaf = x\n",
    "                    parent_found = True\n",
    "                    break\n",
    "                # The current event has no overlap with leaf\n",
    "                elif event_start >= leaf_end:\n",
    "                    continue\n",
    "                # Crossover shouldn't happen\n",
    "                else:\n",
    "                    pprint(str(x))\n",
    "                    raise ValueError(\"\\tCrossover happens!\")\n",
    "\n",
    "            # New root and leaf\n",
    "            if not parent_found:\n",
    "                to_add_root = x\n",
    "                to_add_leaf = x\n",
    "            if to_add_root:\n",
    "                roots.append(to_add_root)\n",
    "            if to_add_leaf:\n",
    "                leaves.append(to_add_leaf)\n",
    "            \n",
    "            # Add op to caller or unaccounted\n",
    "            if x.category() == \"Operator\":\n",
    "                if external_id != 0:\n",
    "                    if external_id not in cc.keys():\n",
    "                        cc[external_id] = {}  \n",
    "                    cc[external_id][\"caller\"] = x\n",
    "                    cc[external_id][\"callees\"] = {}\n",
    "            else: # Runtime\n",
    "                if external_id != 0 and correlation_id != 0: # Not consider some events without ex_id and cr_id, e.g. cudaEventCreateWithFlags\n",
    "                    if external_id not in cc.keys():\n",
    "                        cc[external_id] = {}\n",
    "                    if \"caller\" not in cc[external_id].keys():\n",
    "                        cc[external_id][\"caller\"] = None\n",
    "                    if \"callees\" not in cc[external_id].keys():\n",
    "                        cc[external_id][\"callees\"] = {}\n",
    "                    if correlation_id not in cc[external_id][\"callees\"].keys():\n",
    "                        cc[external_id][\"callees\"][correlation_id] = {}\n",
    "                        cc[external_id][\"callees\"][correlation_id][\"launcher\"] = None\n",
    "                        cc[external_id][\"callees\"][correlation_id][\"executor\"] = None\n",
    "                    cc[external_id][\"callees\"][correlation_id][\"launcher\"] = x\n",
    "        else:\n",
    "            # Skip modules if needed\n",
    "            if (skip_module and x.name().startswith(module_marker)):\n",
    "                continue\n",
    "            else: # \"cat\" = \"Memcpy\" or \"Kernel\", i.e. callee\n",
    "                if external_id != 0 and correlation_id != 0: # Doesn't consider some events without ex_id and cr_id, e.g. cudaEventCreateWithFlags\n",
    "                    if external_id not in cc.keys():\n",
    "                        cc[external_id] = {}\n",
    "                    if \"caller\" not in cc[external_id].keys():\n",
    "                        cc[external_id][\"caller\"] = None\n",
    "                    if \"callees\" not in cc[external_id].keys():\n",
    "                        cc[external_id][\"callees\"] = {}\n",
    "                    if correlation_id not in cc[external_id][\"callees\"].keys():\n",
    "                        cc[external_id][\"callees\"][correlation_id] = {}\n",
    "                        cc[external_id][\"callees\"][correlation_id][\"launcher\"] = None\n",
    "                        cc[external_id][\"callees\"][correlation_id][\"executor\"] = None\n",
    "                    cc[external_id][\"callees\"][correlation_id][\"executor\"] = x\n",
    "            \n",
    "    # Set the corrected_end_time to be the last event's end time\n",
    "    for x in reversed(roots):\n",
    "        if x.duration() is not None:\n",
    "            corrected_end_time = x.start_time() + x.duration()\n",
    "            break\n",
    "            \n",
    "    # Update 'has_device_calls' for all events in the tree\n",
    "    def update_has_device_calls(roots):\n",
    "        for r in roots:\n",
    "            ex_id = r.external_id()\n",
    "            if len(r.children) == 0:\n",
    "                if ex_id in cc.keys() and len(cc[ex_id][\"callees\"].keys()) != 0:\n",
    "                    for k, v in cc[ex_id][\"callees\"].items():\n",
    "                        if v[\"executor\"] is not None:\n",
    "                            r.has_device_calls = True\n",
    "            else:\n",
    "                update_has_device_calls(r.children)\n",
    "                for c in r.children:\n",
    "                    if c.has_device_calls:\n",
    "                        r.has_device_calls = True\n",
    "    update_has_device_calls(roots)\n",
    "\n",
    "    return roots, cc, corrected_start_time, corrected_end_time\n",
    "\n",
    "# Get root operators, not including modules\n",
    "def get_operators(roots, ops):\n",
    "    for r in roots:\n",
    "        # Is an operator, and\n",
    "        # Not a module or submodule, and\n",
    "        # (Parent is a module, or, is simply a root operator)\n",
    "        if r.category() == \"Operator\" and\\\n",
    "            (not r.name().startswith(\"## \")) and ((\\\n",
    "            r.parent is not None and\\\n",
    "            r.parent.name().startswith(\"## \")\\\n",
    "        ) or (\\\n",
    "            r.parent is None\\\n",
    "        )) :\n",
    "            ops.append(r)\n",
    "        else:\n",
    "            get_operators(r.children, ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "trace_file = \"./libgpumon_activities_425511.json\"\n",
    "trim_trace(trace_file, 0.90, 1.0)\n",
    "base_trace_dir = run_ATC()\n",
    "print(base_trace_dir)\n",
    "with open(base_trace_dir + \"/two_iteration_trace.json\") as two:\n",
    "    two_iteration_stats = json.load(two)\n",
    "\n",
    "ops = []\n",
    "roots, cc, corrected_start_time, corrected_end_time = process_event_hierarchy(two_iteration_stats, skip_module=False)\n",
    "get_operators(roots, ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizes of C*B and C'*A\n",
    "def get_addmm_backward_size(op):\n",
    "    sizes = []\n",
    "    for x in op.children:\n",
    "        if x.name() == \"mm\":\n",
    "            size = x.input_shape()\n",
    "            sizes.append((size[0][0], size[0][1], size[1][1]))\n",
    "    return sizes\n",
    "\n",
    "# Never seen BmmBackward0 having only one bmm. Possible though.\n",
    "def get_bmm_backward_size(op):\n",
    "    sizes = []\n",
    "    for x in op.children:\n",
    "        if x.name() == \"bmm\":\n",
    "            size = x.input_shape()\n",
    "            sizes.append((size[0][0], size[0][1], size[0][2], size[1][2]))\n",
    "    return sizes\n",
    "\n",
    "# Not working in new traces as the output_nr op calls are removed\n",
    "def get_embedding_lookup_forward_size(op):\n",
    "    sizes = []\n",
    "    rows_per_block = None\n",
    "    for x in op.children:\n",
    "        if x.name() == \"output_nr\":\n",
    "            sizes.append(x.input_shape())\n",
    "        if x.name() == \"cudaLaunchKernel\":\n",
    "            ex = cc[op.external_id()][\"callees\"][x.correlation_id()][\"executor\"]\n",
    "            rows_per_block = ex.event[\"args\"][\"block\"][1]\n",
    "    D = int(sizes[0][0][1])\n",
    "    T = int(sizes[1][0][0])\n",
    "    E = int(sizes[0][0][0] / T)\n",
    "    B = int((sizes[3][0][0] - 1) / T)\n",
    "    L = int(sizes[2][0][0] / B / T)\n",
    "    return B, E, T, L, D, rows_per_block\n",
    "\n",
    "# Not working in new traces as the size op calls are removed\n",
    "def get_embedding_lookup_backward_size(op):\n",
    "    sizes = []\n",
    "    rows_per_block = -1\n",
    "    for x in op.children:\n",
    "        if x.name() == \"size\":\n",
    "            sizes.append(x.input_shape())\n",
    "        if x.name() == \"cudaLaunchKernel\":\n",
    "            ex = cc[op.external_id()][\"callees\"][x.correlation_id()][\"executor\"]\n",
    "            rows_per_block = ex.event[\"args\"][\"block\"][1]\n",
    "    T = int(sizes[0][0][0])\n",
    "    E = int(sizes[1][0][0] / T)\n",
    "    D = int(sizes[1][0][1])\n",
    "    B = int((sizes[2][0][0] - 1) / T)\n",
    "    return B, E, T, _, D, rows_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_stats = pd.read_csv(\"./fully_connected_forward.csv\", delimiter=',')\n",
    "fc_stats = preprocessing(fc_stats)\n",
    "fc_stats = fc_stats[fc_stats[\"kernel_name\"].str.startswith(\"volta\")].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_size = 6 * 1024 * 1024 * 4\n",
    "num_SM = 80\n",
    "peak_dram_bw = 809 # GB/s\n",
    "peak_l2_bw = 2888 # GB/s\n",
    "peak_throughput = 12200 # GFLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4340.7676440049445"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "bento_obj_id": "139888146244208"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_forward_predictor(peak_dram_bw, peak_l2_bw, L2_size, num_SM, **kwargs):\n",
    "    # hit_rate = C(X, L) / C(E, L), X = avg_num_rows_per_table\n",
    "    def hit_rate(X, E, L):\n",
    "        ret = 1.0\n",
    "        e = E\n",
    "        x = X\n",
    "        for idx in range(L):\n",
    "            ret *= x / e\n",
    "            x -= 1\n",
    "            e -= 1\n",
    "        return ret\n",
    "\n",
    "    # Average number of rows per table in L2\n",
    "    y = kwargs\n",
    "    num_total_warps = y[\"batch_size\"] * y[\"num_tables\"] # Total warp number of the kernel\n",
    "    num_warps_per_sm = y[\"rows_per_block\"] # Number of warps per sm\n",
    "    num_warps_simul = num_SM * num_warps_per_sm # Total number of warps simultaneously running on the device\n",
    "    num_tables_simul = (num_warps_simul + y[\"batch_size\"] - 1) // y[\"batch_size\"] # Number of tables simultaneously being accessed on the device\n",
    "    avg_table_size = min(L2_size // num_tables_simul, y[\"num_embeddings\"] * y[\"embedding_dim\"] * 4) # Average table size that reside on the device\n",
    "    indices_size = 0 if y[\"shmem\"] else div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "    avg_num_rows_per_table = (avg_table_size - indices_size) // 4 // y[\"embedding_dim\"]\n",
    "\n",
    "    # Hit rate\n",
    "    hr = hit_rate(avg_num_rows_per_table, y[\"num_embeddings\"], y[\"batch_size\"])\n",
    "    \n",
    "    # num_thread_x\n",
    "    num_thread_x = max(y[\"embedding_dim\"] / 4, 1024 / y[\"rows_per_block\"])\n",
    "\n",
    "    # Traffics\n",
    "    table_offsets_traffic = 32\n",
    "    offsets_traffic = 32\n",
    "    if y[\"shmem\"]:\n",
    "        indices_dram_traffic = div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "        indices_l2_traffic = 0\n",
    "    else: # no_shmem\n",
    "        indices_dram_traffic = div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "        indices_l2_traffic = y[\"embedding_dim\"] // (4 * num_thread_x) * div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "    table_traffic = y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32)\n",
    "    output_traffic = (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32)\n",
    "\n",
    "    # avg_table_size all as dram traffic\n",
    "    # 21, 26, 13, 7, 4, 4, 24, (0.2 ± 0.21)\n",
    "    total_l2_traffic = ((table_offsets_traffic + offsets_traffic + indices_l2_traffic) * y[\"batch_size\"] + \\\n",
    "                        hr * (table_traffic * y[\"batch_size\"] - avg_table_size)) * y[\"num_tables\"]\n",
    "    total_dram_traffic = ((indices_dram_traffic + output_traffic) * y[\"batch_size\"] + \\\n",
    "                          (1 - hr) * (table_traffic * y[\"batch_size\"] - avg_table_size) + avg_table_size) * y[\"num_tables\"]\n",
    "\n",
    "    return max(total_dram_traffic / peak_dram_bw / 1000.0, total_l2_traffic / peak_l2_bw / 1000.0)\n",
    "        \n",
    "# e.g. 4340 vs 4789\n",
    "embedding_forward_predictor(peak_dram_bw, peak_l2_bw, L2_size, num_SM, batch_size=4096, num_embeddings=500000, num_tables=197, bag_size=32, embedding_dim=32, rows_per_block=128, shmem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86291.71294932015"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "bento_obj_id": "139888145467952"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_backward_sgd_predictor(peak_dram_bw, **kwargs):\n",
    "    y = kwargs\n",
    "    if y[\"shmem\"]: # 40% GMAE...\n",
    "        indices_traffic = div_round_up(y[\"bag_size\"] * 4, 32) * 32\n",
    "        grad_output_traffic = div_round_up(y[\"embedding_dim\"] * 4, 32) * 32\n",
    "    else: # backward_sgd_no_shmem\n",
    "        indices_traffic = y[\"bag_size\"] * 32\n",
    "        grad_output_traffic = (y[\"bag_size\"] * div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) * 2\n",
    "\n",
    "    # Traffic per warp = t_offsets + t_table_offsets + t_indices + t_weights + t_grad_outputs\n",
    "    total_traffic_per_warp = 32 + \\\n",
    "                            32 + \\\n",
    "                            indices_traffic + \\\n",
    "                            2 * y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) + \\\n",
    "                            grad_output_traffic\n",
    "\n",
    "    # Traffic = warp * traffic per warp\n",
    "    total_traffic = y[\"batch_size\"] * y[\"num_tables\"] * total_traffic_per_warp\n",
    "\n",
    "    # Total compute throughput\n",
    "    mac_per_warp = y[\"bag_size\"] * 4 * (y[\"embedding_dim\"] // 4)\n",
    "    total_mac = y[\"batch_size\"] * y[\"num_tables\"] * mac_per_warp\n",
    "\n",
    "    return max(total_traffic / peak_dram_bw / 1000, total_mac / peak_throughput / 1000)\n",
    "\n",
    "# e.g 86291 vs 77508\n",
    "embedding_backward_sgd_predictor(peak_dram_bw, batch_size=2048, num_embeddings=200000, num_tables=128, bag_size=128, embedding_dim=128, rows_per_block=32, shmem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64226.252103831896"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "bento_obj_id": "139888145468304"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_backward_rowwise_adagrad_approx_predictor(peak_dram_bw, **kwargs):\n",
    "    y = kwargs\n",
    "    \n",
    "    # Traffic = warp * traffic per warp\n",
    "    total_traffic_per_warp = 32 + \\\n",
    "                            32 + \\\n",
    "                            2 * (div_round_up(y[\"bag_size\"] * 4, 32) * 32) + \\\n",
    "                            y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) + \\\n",
    "                            (y[\"bag_size\"] + 1) * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32) + \\\n",
    "                            y[\"bag_size\"] * (div_round_up(y[\"embedding_dim\"] * 4, 32) * 32)\n",
    "    total_traffic = y[\"batch_size\"] * y[\"num_tables\"] * total_traffic_per_warp\n",
    "\n",
    "    return total_traffic / peak_dram_bw / 1000.0\n",
    "\n",
    "# e.g 64226 vs 63304\n",
    "embedding_backward_rowwise_adagrad_approx_predictor(peak_dram_bw, batch_size=2048, num_embeddings=200000, num_tables=128, bag_size=128, embedding_dim=128, rows_per_block=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5829.673816702252"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "bento_obj_id": "139888250505680"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fc_forward_predictor(peak_dram_bw, peak_throughput, df, **kwargs):\n",
    "    def get_record(df, **kwargs):\n",
    "        row_count = df.shape[0]\n",
    "        condition = pd.Series([True] * row_count)\n",
    "        for k, v in kwargs.items():\n",
    "            condition = condition & (df[k] == v)\n",
    "        return df[condition]\n",
    "\n",
    "    def get_closest(df, **kwargs):\n",
    "        no_match = {}\n",
    "        row_count = df.shape[0]\n",
    "        condition = pd.Series([True] * row_count)\n",
    "        for k, v in kwargs.items():\n",
    "            if v in df[k].unique():\n",
    "                condition = condition & (df[k] == v)\n",
    "            else:\n",
    "                no_match[k] = v\n",
    "\n",
    "        # With matched dimensions\n",
    "        data_points = [(df[condition], {})]\n",
    "\n",
    "        # For each of the non-matched dimension\n",
    "        for k, v in no_match.items():\n",
    "            tmp = []\n",
    "            for dp, limits in data_points:\n",
    "                uni_val = sorted(dp[k].unique())\n",
    "\n",
    "                low, high = -1, -1\n",
    "                if v < uni_val[0]:\n",
    "                    high = uni_val[0]\n",
    "                elif v > uni_val[-1]:\n",
    "                    low = uni_val[-1]\n",
    "                else:\n",
    "                    for idx in range(len(uni_val[:-1])):\n",
    "                        if uni_val[idx] < v and uni_val[idx+1] > v:\n",
    "                            high = uni_val[idx+1]\n",
    "                            low = uni_val[idx]\n",
    "                            break\n",
    "                assert not (low == -1 and high == -1)\n",
    "\n",
    "                less_tmp = dp[dp[k] == (low if low != -1 else uni_val[0])]\n",
    "                more_tmp = dp[dp[k] == (high if high != -1 else uni_val[-1])]\n",
    "                if low == -1:\n",
    "                    less_tmp[k] = 0\n",
    "                if high == -1:\n",
    "                    more_tmp[k] = sys.maxsize # Big enough for BW in GB/s or throughput in GFLOPS\n",
    "                tmp_limits = limits.copy()\n",
    "                tmp_limits[k] = (low, high)\n",
    "                tmp.append((less_tmp, tmp_limits))\n",
    "                tmp.append((more_tmp, tmp_limits))\n",
    "            data_points = tmp\n",
    "\n",
    "        return data_points\n",
    "\n",
    "    #####################\n",
    "    #       |     X |\n",
    "    #    |==O=======|\n",
    "    #    |  |       |\n",
    "    #    |  |-------O-\n",
    "    #    |     X     |\n",
    "    #    O           |\n",
    "    #    |===========O\n",
    "    #    |  X        |\n",
    "    #####################\n",
    "\n",
    "    record = get_record(df, **kwargs)\n",
    "    if not record.empty:\n",
    "        return record[\"kernel_runtime\"].iloc[0]\n",
    "    data_points = get_closest(df, **kwargs)\n",
    "\n",
    "    effective_flops = 0.0\n",
    "    effective_bw = 0.0\n",
    "    batch_size = kwargs[\"batch_size\"]\n",
    "    M = kwargs[\"M\"]\n",
    "    N = kwargs[\"N\"]\n",
    "    K = kwargs[\"K\"]\n",
    "    for dp, limits in data_points:\n",
    "        dp_flops_contrib = 0.0\n",
    "        dp_bw_contrib = 0.0\n",
    "\n",
    "        # An idea, if zero occurs, it's always the bottleneck. A zero dominates all peaks.\n",
    "        zero_exists = False\n",
    "        peak_exists = False\n",
    "        for k, v in limits.items():\n",
    "            metric = dp[k].iloc[0]\n",
    "            if metric == 0:\n",
    "                zero_exists = True\n",
    "            elif metric == sys.maxsize:\n",
    "                peak_exists = True\n",
    "\n",
    "        for k, v in limits.items():\n",
    "            low, high = v\n",
    "            if high == -1: # Reaching the peak, taking average\n",
    "                ratio_l, ratio_h = 0.5, 0.5\n",
    "            elif low == -1: # Reaching the bottom, set low as 0\n",
    "                ratio_l, ratio_h = kwargs[k] / high, (high - kwargs[k]) / high\n",
    "            else: # Normal, weighted\n",
    "                ratio_l, ratio_h = (kwargs[k] - low) / (high - low), (high - kwargs[k]) / (high - low)\n",
    "            # Edge cases: when more than one metric is MAX/0\n",
    "\n",
    "            metric = dp[k].iloc[0]\n",
    "            if zero_exists:\n",
    "                throughput = 0\n",
    "                dram_bw = 0\n",
    "                ratio = 0\n",
    "            elif peak_exists:\n",
    "                throughput = peak_throughput\n",
    "                dram_bw = peak_dram_bw\n",
    "                ratio = ratio_h\n",
    "            elif metric == low:\n",
    "                throughput = (dp[\"batch_size\"] * dp[\"M\"] * dp[\"N\"] * dp[\"K\"]).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 # GFLOPS\n",
    "                dram_bw = (dp[\"batch_size\"] * (dp[\"M\"] * dp[\"K\"] + dp[\"K\"] * dp[\"N\"] + dp[\"M\"] * dp[\"N\"])).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 * 4 # GB/s\n",
    "                ratio = ratio_l \n",
    "            elif metric == high:\n",
    "                throughput = (dp[\"batch_size\"] * dp[\"M\"] * dp[\"N\"] * dp[\"K\"]).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 # GFLOPS\n",
    "                dram_bw = (dp[\"batch_size\"] * (dp[\"M\"] * dp[\"K\"] + dp[\"K\"] * dp[\"N\"] + dp[\"M\"] * dp[\"N\"])).iloc[0] / dp[\"kernel_runtime\"].iloc[0] / 1000 * 4 # GB/s\n",
    "                ratio = ratio_h\n",
    "\n",
    "            dp_flops_contrib += throughput * ratio\n",
    "            dp_bw_contrib += dram_bw * ratio\n",
    "\n",
    "        effective_flops += dp_flops_contrib / len(limits.items())\n",
    "        effective_bw += dp_bw_contrib / len(limits.items())\n",
    "\n",
    "    effective_flops /= len(data_points) / 2\n",
    "    effective_bw /= len(data_points) / 2\n",
    "\n",
    "    FLOP = kwargs[\"batch_size\"] * kwargs[\"M\"] * kwargs[\"N\"] * kwargs[\"K\"]\n",
    "    DRAM_bytes = kwargs[\"batch_size\"] * (kwargs[\"M\"] * kwargs[\"K\"] + kwargs[\"K\"] * kwargs[\"N\"] + kwargs[\"M\"] * kwargs[\"N\"]) * 4\n",
    "    predicted_runtime = max(FLOP / effective_flops, DRAM_bytes / effective_bw) / 1000\n",
    "\n",
    "    return predicted_runtime\n",
    "\n",
    "# 5829 vs 7548\n",
    "fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=256, M=512, N=1000, K=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_time: 12117.71546046202\n"
     ]
    }
   ],
   "source": [
    "total_time = 0.0\n",
    "for op in ops:\n",
    "    t = 0.0\n",
    "    if op.name() == \"addmm\":\n",
    "        size = op.input_shape()\n",
    "        M, K, N = size[1][0], size[1][1], size[2][1]\n",
    "        t = fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=1, M=M, N=N, K=K)\n",
    "#         print(\"addmm\", M, N, K, t)\n",
    "    if op.name() == \"bmm\":\n",
    "        size = op.input_shape()\n",
    "        batch_size, M, K, N = size[0][0], size[0][1], size[0][2], size[1][2]\n",
    "        t = fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=batch_size, M=M, N=N, K=K)\n",
    "#         print(\"bmm\", batch_size, M, N, K, t)\n",
    "    if op.name() == \"LookupFunction\":\n",
    "        B, E, T, L, D, rows_per_block = get_embedding_lookup_forward_size(op)\n",
    "        lks = []\n",
    "        for c in op.children:\n",
    "            if c.name() == \"cudaLaunchKernel\":\n",
    "                lks.append(c)\n",
    "        callees = cc[lks[0].external_id()][\"callees\"]\n",
    "        shmem = list(callees.values())[0][\"executor\"].name().split(',')[1].strip()\n",
    "        t = embedding_forward_predictor(peak_dram_bw, peak_l2_bw, L2_size, num_SM, batch_size=B, num_embeddings=E, num_tables=T, bag_size=L, embedding_dim=D, rows_per_block=rows_per_block, shmem=shmem)\n",
    "#         print(\"Embedding forward\", t)\n",
    "    if op.name() == \"LookupFunctionBackward\":\n",
    "        B, E, T, _, D, rows_per_block = get_embedding_lookup_backward_size(op)\n",
    "        L = 38 # TODO: Cannot get it from trace. Hard code it.\n",
    "        sgd = False\n",
    "        lks = []\n",
    "        for c in op.children:\n",
    "            if c.name() == \"cudaLaunchKernel\":\n",
    "                lks.append(c)\n",
    "        callees = cc[lks[0].external_id()][\"callees\"]\n",
    "        kernel_name = list(callees.values())[0][\"executor\"].name()\n",
    "        if \"sgd\" in kernel_name:\n",
    "            sgd = True\n",
    "        shmem = kernel_name.split(',')[1].strip()\n",
    "        if sgd:\n",
    "            t = embedding_backward_sgd_predictor(peak_dram_bw, batch_size=B, num_embeddings=E, num_tables=T, bag_size=L, embedding_dim=D, rows_per_block=rows_per_block, shmem=shmem)\n",
    "        else:\n",
    "            t = embedding_backward_rowwise_adagrad_approx_predictor(peak_dram_bw, batch_size=B, num_embeddings=E, num_tables=T, bag_size=L, embedding_dim=D, rows_per_block=rows_per_block)\n",
    "#         print(\"Embedding backward\", t)\n",
    "    if op.name() == \"AddmmBackward\":\n",
    "        sizes = get_addmm_backward_size(op)\n",
    "        for size in sizes:\n",
    "            M, K, N = size\n",
    "            t += fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=1, M=M, N=N, K=K)\n",
    "#         print(\"AddmmBackward\", t)\n",
    "    if op.name() == \"BmmBackward0\":\n",
    "        sizes = get_bmm_backward_size(op)\n",
    "        for size in sizes:\n",
    "            batch_size, M, K, N = size\n",
    "            t += fc_forward_predictor(peak_dram_bw, peak_throughput, fc_stats, batch_size=batch_size, M=M, N=N, K=K)\n",
    "#         print(\"BmmBackward0\", t)\n",
    "    total_time += t\n",
    "print(\"total_time:\", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "573647873313879"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200830-210251",
   "description": "Analyze a two-iteration trace file generated by ATC ; extract multi-level (module/op) runtime breakdown and major input output shapes.",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/zhongyilin/fbsource/fbcode/bento/kernels/local/zhongyilin/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "320128",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "643445799644945",
   "tags": "FBLSim,CEA,ATC,dyno,gputrace,trace",
   "tasks": "",
   "title": "Runtime Prediction"
  },
  "kernelspec": {
   "display_name": "zhongyilin (local)",
   "language": "python",
   "name": "zhongyilin_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
