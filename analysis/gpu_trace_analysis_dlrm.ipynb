{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.core.display import display, HTML\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import pandas as pd\n",
    "import argparse, logging, tempfile, json, sys, pandas\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.max_open_warning'] = 50\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from utils import PM_HOME\n",
    "sys.path.insert(0, PM_HOME)\n",
    "from analysis.trace_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "model_name = \"MLPerf_1\"\n",
    "# model_name = \"DLRM_default_1\"\n",
    "\n",
    "trace_file = '../data/{}.json'.format(model_name)\n",
    "# trace_file = '../data/MLPerf_1_prof_cpu.json' # Not supported: only Kineto has the right trace file structure (schemaVersion etc) and external ID in events\n",
    "# trace_file = '../data/MLPerf_1_prof_cuda.json' # Same as above\n",
    "# trace_file = '../data/MLPerf_1_prof_cuda_kineto.json' # Good\n",
    "# trace_file = '../data/MLPerf_1_prof_kineto.json' # No device events\n",
    "# trace_file = '../data/MLPerf_1_prof_new.json' # Good\n",
    "iters = 10\n",
    "\n",
    "trimmed_trace_file = trim_trace_by_num_iter(trace_file, iters=iters)\n",
    "with open(trimmed_trace_file) as f:\n",
    "    trace = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLRM with data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of events: 18610, num of root events: 30, num of caller/callee pairs: 6289\n",
      "Sum of dataloading time: 34507\n",
      "Corrected start time:  1620971976952391 , corrected end time:  1620971977079261\n",
      "Host runtime:  92363\n",
      "Device runtime:  92363\n",
      "QPS: 221733.81\n"
     ]
    }
   ],
   "source": [
    "roots, cc, corrected_start_time, corrected_end_time, sum_skipped_intervals = process_event_hierarchy(trace['traceEvents'], skip_module=False, module_marker=\"DLRM \")\n",
    "print('Num of events: {}, num of root events: {}, num of caller/callee pairs: {}'.format(len(trace['traceEvents']), len(roots), len(cc)))\n",
    "print('Sum of dataloading time: {}'.format(sum_skipped_intervals))\n",
    "print(\"Corrected start time: \", corrected_start_time, \", corrected end time: \", corrected_end_time)\n",
    "host_runtime = corrected_end_time - corrected_start_time - sum_skipped_intervals\n",
    "# ---\n",
    "# device_runtime, device_start_delay = get_device_runtime_and_start_delay(cc, corrected_start_time)\n",
    "# print(\"Device start delay: \", device_start_delay)\n",
    "# ---\n",
    "device_runtime = host_runtime\n",
    "# ---\n",
    "print(\"Host runtime: \", host_runtime)\n",
    "print(\"Device runtime: \", device_runtime)\n",
    "ops = []\n",
    "get_operators(roots, ops)\n",
    "QPS = 1000000 / host_runtime * iters * 2048\n",
    "print(f\"QPS: {QPS:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operators(roots, ops):\n",
    "    for r in roots:\n",
    "        # Is an operator, and\n",
    "        # Not a module or submodule, and\n",
    "        # (Parent is a module, or, is simply a root operator)\n",
    "        if r.category() == \"Operator\" and\\\n",
    "            (not is_module(r)) and ((\\\n",
    "            r.parent is not None\n",
    "        ) or (\\\n",
    "            r.parent is None\\\n",
    "        )) :\n",
    "            ops.append(r)\n",
    "        else:\n",
    "            get_operators(r.children, ops)\n",
    "\n",
    "ops = []\n",
    "get_operators(roots, ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::ones',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'LookupFunction',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::view',\n",
      " 'aten::cat',\n",
      " 'aten::transpose',\n",
      " 'aten::bmm',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::detach_',\n",
      " 'aten::slice',\n",
      " 'aten::to',\n",
      " 'aten::to',\n",
      " 'aten::index',\n",
      " 'aten::cat',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::relu',\n",
      " 'aten::linear',\n",
      " 'aten::sigmoid',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::to',\n",
      " 'aten::binary_cross_entropy',\n",
      " 'aten::detach',\n",
      " 'aten::to',\n",
      " 'aten::zeros',\n",
      " 'aten::empty',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.zero_grad#SGD.zero_grad',\n",
      " 'aten::ones_like',\n",
      " 'BinaryCrossEntropyBackward',\n",
      " 'SigmoidBackward',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'CatBackward',\n",
      " 'IndexBackward',\n",
      " 'SliceBackward',\n",
      " 'BmmBackward0',\n",
      " 'TransposeBackward0',\n",
      " 'aten::add',\n",
      " 'CatBackward',\n",
      " 'ViewBackward',\n",
      " 'aten::add',\n",
      " 'LookupFunctionBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'ReluBackward0',\n",
      " 'AddmmBackward',\n",
      " 'aten::sum',\n",
      " 'aten::view',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'TBackward',\n",
      " 'torch::autograd::AccumulateGrad',\n",
      " 'aten::zeros',\n",
      " 'Optimizer.step#SGD.step',\n",
      " 'aten::zeros']\n"
     ]
    }
   ],
   "source": [
    "pprint([x.name() for x in ops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host runtime breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two iteration runtime:                92363 (in us, same below)\n",
      "     ## BENCHMARK ##:                           (91658.0, 99.24%, 10)\n",
      "          DLRM backward:                                  (56180.0, 61.29%, 10)\n",
      "               AddmmBackward:                                       (14715.0, 26.19%, 80)\n",
      "                    aten::mm:                                                 (10255.0, 69.69%, 150)\n",
      "                    aten::t:                                                   (2518.0, 17.11%, 230)\n",
      "                    aten::conj:                                                 (146.0, 0.99%, 150)\n",
      "                    Unaccounted:                                               (1796.0, 12.21%)\n",
      "               Optimizer.zero_grad#SGD.zero_grad:                    (5185.0, 9.23%, 10)\n",
      "                    aten::zero_:                                               (3563.0, 68.72%, 170)\n",
      "                    aten::empty:                                                 (20.0, 0.39%, 10)\n",
      "                    Unaccounted:                                               (1602.0, 30.90%)\n",
      "               Optimizer.step#SGD.step:                              (4895.0, 8.71%, 10)\n",
      "                    aten::add_:                                                (2928.0, 59.82%, 170)\n",
      "                    aten::empty:                                                 (21.0, 0.43%, 10)\n",
      "                    Unaccounted:                                               (1946.0, 39.75%)\n",
      "               Others:                                              (23621.0, 42.05%)\n",
      "               Unaccounted:                                          (7764.0, 13.82%)\n",
      "          DLRM forward:                                   (29162.0, 31.82%, 10)\n",
      "               module::forward_pass::top_mlp:                       (10822.0, 37.11%, 10)\n",
      "                    aten::linear:                                              (6237.0, 57.63%, 50)\n",
      "                    aten::relu:                                                (2301.0, 21.26%, 40)\n",
      "                    aten::sigmoid:                                              (320.0, 2.96%, 10)\n",
      "                    Others:                                                      (21.0, 0.19%)\n",
      "                    Unaccounted:                                               (1943.0, 17.95%)\n",
      "               module::forward_pass::bottom_mlp:                     (6860.0, 23.52%, 10)\n",
      "                    aten::linear:                                              (3813.0, 55.58%, 30)\n",
      "                    aten::relu:                                                (1736.0, 25.31%, 30)\n",
      "                    aten::empty:                                                 (21.0, 0.31%, 10)\n",
      "                    Unaccounted:                                               (1290.0, 18.80%)\n",
      "               module::forward_pass::interaction:                    (6317.0, 21.66%, 10)\n",
      "                    aten::cat:                                                 (1109.0, 17.56%, 20)\n",
      "                    aten::to:                                                   (985.0, 15.59%, 40)\n",
      "                    aten::index:                                                (749.0, 11.86%, 10)\n",
      "                    Others:                                                    (1208.0, 19.12%)\n",
      "                    Unaccounted:                                               (2266.0, 35.87%)\n",
      "               Others:                                               (3796.0, 13.02%)\n",
      "               Unaccounted:                                          (1367.0, 4.69%)\n",
      "          DLRM loss compute:                               (3202.0, 3.49%, 10)\n",
      "               aten::binary_cross_entropy:                           (1619.0, 50.56%, 10)\n",
      "                    aten::mean:                                                 (414.0, 25.57%, 10)\n",
      "                    aten::copy_:                                                (245.0, 15.13%, 10)\n",
      "                    aten::empty_like:                                           (157.0, 9.70%, 10)\n",
      "                    Others:                                                     (368.0, 22.73%)\n",
      "                    Unaccounted:                                                (435.0, 26.87%)\n",
      "               aten::to:                                              (981.0, 30.64%, 10)\n",
      "                    aten::copy_:                                                (756.0, 77.06%, 10)\n",
      "                    aten::empty_strided:                                        (133.0, 13.56%, 10)\n",
      "                    Unaccounted:                                                 (92.0, 9.38%)\n",
      "               aten::empty:                                            (23.0, 0.72%, 10)\n",
      "               Unaccounted:                                           (579.0, 18.08%)\n",
      "          Others:                                          (1170.0, 1.28%)\n",
      "          Unaccounted:                                     (1944.0, 2.12%)\n",
      "     aten::zeros:                                 (235.0, 0.25%, 20)\n",
      "          aten::empty:                                       (80.0, 34.04%, 20)\n",
      "          aten::zero_:                                       (28.0, 11.91%, 19)\n",
      "          Unaccounted:                                      (127.0, 54.04%)\n",
      "     Unaccounted:                                 (470.0, 0.51%)\n"
     ]
    }
   ],
   "source": [
    "depth_limit = 3\n",
    "truncate_count = 3\n",
    "host_runtime_breakdown = get_host_runtime_breakdown(roots, cc, host_runtime)\n",
    "print_host_results(host_runtime_breakdown, depth_limit, truncate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{53627: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 53630: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 53633: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 11.0}}},\n",
      " 53641: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 53649: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 53654: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 67.0}}},\n",
      " 53662: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 53667: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 53675: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 53687: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 54.0}}},\n",
      " 53696: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 92.0}}},\n",
      " 53702: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 195.0}}},\n",
      " 53715: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 53718: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 53721: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 22.0}}},\n",
      " 53728: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 53737: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 18.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 203.0}}},\n",
      " 53745: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 25.0}}},\n",
      " 53750: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 440.0}}},\n",
      " 53758: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 53763: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 220.0}}},\n",
      " 53771: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 53776: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 53784: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 53789: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 53797: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 53803: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 53806: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 5.0}}},\n",
      " 53823: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 53834: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 49.0}}},\n",
      " 53874: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 53878: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 53887: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 53890: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 10.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 53906: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 53909: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 53915: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 53917: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 53920: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 61.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 109.0}}},\n",
      " 53936: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 53939: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 53945: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 53947: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 53950: {7: {('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 199.0}}},\n",
      " 53966: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 53969: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 53975: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 9.0}}},\n",
      " 53977: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 53980: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 392.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 368.0}}},\n",
      " 53996: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 24.0}}},\n",
      " 53999: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54005: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 16.0}}},\n",
      " 54007: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 54010: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 189.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 173.0}}},\n",
      " 54026: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 21.0}}},\n",
      " 54029: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54035: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 54044: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 13.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 54079: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 54088: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 140.0}}},\n",
      " 54104: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 54115: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0}}},\n",
      " 54116: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 106.0}}},\n",
      " 54129: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 54132: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 22.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 22.0}}},\n",
      " 54148: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 12.0}}},\n",
      " 54151: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54157: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54159: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 10.0}}},\n",
      " 54162: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 59.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 118.0}}},\n",
      " 54178: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 54181: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54187: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 54189: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 54192: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 54202: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 54205: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54211: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54216: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 80.0}}},\n",
      " 54690: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 54693: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 54696: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 10.0}}},\n",
      " 54704: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 54712: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 54717: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 54725: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 54730: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 54738: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 54750: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 54.0}}},\n",
      " 54759: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 92.0}}},\n",
      " 54765: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 190.0}}},\n",
      " 54778: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 54781: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 54784: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 20.0}}},\n",
      " 54791: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 54800: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 18.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 202.0}}},\n",
      " 54808: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 54813: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 439.0}}},\n",
      " 54821: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 54826: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 220.0}}},\n",
      " 54834: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 54839: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 54847: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 54852: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 54860: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 54866: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 54869: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 54886: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 54897: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 48.0}}},\n",
      " 54937: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 54941: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54950: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54953: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 10.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 54969: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 54972: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 54978: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 54980: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 54983: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 133.0}}},\n",
      " 54999: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 55002: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55008: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 55010: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 19.0}}},\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55013: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 200.0}}},\n",
      " 55029: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 55032: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55038: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 55040: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 55043: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 392.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 365.0}}},\n",
      " 55059: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 55062: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55068: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 17.0}}},\n",
      " 55070: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 55073: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 172.0}}},\n",
      " 55089: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 22.0}}},\n",
      " 55092: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55098: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 55107: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 13.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 55142: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 55151: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 105.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 139.0}}},\n",
      " 55167: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 55178: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 10.0}}},\n",
      " 55179: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 111.0}}},\n",
      " 55192: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 55195: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 21.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 22.0}}},\n",
      " 55211: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 55214: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55220: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55222: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 55225: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 59.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 109.0}}},\n",
      " 55241: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 55244: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55250: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 55252: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 55255: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 55265: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 55268: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55274: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 55279: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 80.0}}},\n",
      " 55753: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 55756: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 55759: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 11.0}}},\n",
      " 55767: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 15.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 55775: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 55780: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 55788: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 55793: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 55801: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 55813: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 55.0}}},\n",
      " 55822: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 96.0}}},\n",
      " 55828: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 190.0}}},\n",
      " 55841: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 55844: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 55847: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 21.0}}},\n",
      " 55854: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 55863: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 18.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 202.0}}},\n",
      " 55871: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 55876: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 437.0}}},\n",
      " 55884: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 55889: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 220.0}}},\n",
      " 55897: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 55902: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 66.0}}},\n",
      " 55910: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 55915: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 55923: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 55929: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 55932: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 55949: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 55960: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 48.0}}},\n",
      " 56000: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56004: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56013: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56016: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 11.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 56032: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 56035: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56041: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 56043: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 56046: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 111.0}}},\n",
      " 56062: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 56065: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56071: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 56073: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 56076: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 196.0}}},\n",
      " 56092: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 56095: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56101: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 56103: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 56106: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 392.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 372.0}}},\n",
      " 56122: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 56125: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56131: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 17.0}}},\n",
      " 56133: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 56136: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 201.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 173.0}}},\n",
      " 56152: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 21.0}}},\n",
      " 56155: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56161: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 56170: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 13.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 56205: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 56214: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 105.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 138.0}}},\n",
      " 56230: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 56241: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0}}},\n",
      " 56242: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 109.0}}},\n",
      " 56255: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 56258: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 22.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 23.0}}},\n",
      " 56274: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 56277: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56283: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56285: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 56288: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 59.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 103.0}}},\n",
      " 56304: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 10.0}}},\n",
      " 56307: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56313: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 56315: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 56318: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 56328: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 56331: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56337: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 56342: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 81.0}}},\n",
      " 56816: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 56819: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 56822: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 10.0}}},\n",
      " 56830: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 56838: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 56843: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 56851: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 56856: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 56864: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 56876: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 54.0}}},\n",
      " 56885: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 91.0}}},\n",
      " 56891: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 190.0}}},\n",
      " 56904: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 56907: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 56910: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 20.0}}},\n",
      " 56917: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 56926: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 18.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 205.0}}},\n",
      " 56934: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 56939: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 439.0}}},\n",
      " 56947: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 56952: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 220.0}}},\n",
      " 56960: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 56965: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 66.0}}},\n",
      " 56973: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 56978: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 56986: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 56992: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 56995: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 57012: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 57023: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 48.0}}},\n",
      " 57063: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 57067: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57076: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57079: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 10.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 57095: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 57098: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57104: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 57106: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 57109: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 61.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 128.0}}},\n",
      " 57125: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 16.0}}},\n",
      " 57128: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57134: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 57136: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 57139: {7: {('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 199.0}}},\n",
      " 57155: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 57158: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57164: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 57166: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 57169: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 392.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 393.0}}},\n",
      " 57185: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 57188: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57194: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 17.0}}},\n",
      " 57196: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 57199: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 193.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 173.0}}},\n",
      " 57215: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 21.0}}},\n",
      " 57218: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57224: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 57233: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 13.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 320.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 32.0}}},\n",
      " 57268: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 57277: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 137.0}}},\n",
      " 57293: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 57304: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0}}},\n",
      " 57305: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 111.0}}},\n",
      " 57318: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 57321: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 21.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 23.0}}},\n",
      " 57337: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 57340: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57346: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57348: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 57351: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 78.0}}},\n",
      " 57367: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 57370: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57376: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 57378: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 57381: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 57391: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 57394: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57400: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 57405: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 80.0}}},\n",
      " 57879: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 57882: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57885: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 11.0}}},\n",
      " 57893: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 57901: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 57906: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 57914: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 57919: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 57927: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 57939: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 54.0}}},\n",
      " 57948: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 94.0}}},\n",
      " 57954: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 190.0}}},\n",
      " 57967: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 57970: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 57973: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 21.0}}},\n",
      " 57980: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 16.0}}},\n",
      " 57989: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 18.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 203.0}}},\n",
      " 57997: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 58002: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 438.0}}},\n",
      " 58010: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 58015: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 221.0}}},\n",
      " 58023: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 58028: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 67.0}}},\n",
      " 58036: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 58041: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 58049: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 58055: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 58058: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 5.0}}},\n",
      " 58075: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 58086: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 50.0}}},\n",
      " 58126: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 58130: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58139: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58142: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 9.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 58158: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 58161: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58167: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 58169: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 58172: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 61.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 101.0}}},\n",
      " 58188: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 16.0}}},\n",
      " 58191: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58197: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 58199: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 58202: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 193.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 198.0}}},\n",
      " 58218: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 16.0}}},\n",
      " 58221: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58227: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 58229: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 58232: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 393.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 395.0}}},\n",
      " 58248: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 58251: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58257: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 16.0}}},\n",
      " 58259: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 58262: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 174.0}}},\n",
      " 58278: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 21.0}}},\n",
      " 58281: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58287: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 58296: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 14.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 58331: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 58340: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 105.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 138.0}}},\n",
      " 58356: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 58367: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0}}},\n",
      " 58368: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 115.0}}},\n",
      " 58381: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 58384: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 21.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 23.0}}},\n",
      " 58400: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 58403: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58409: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        'runtime': 3.0}}},\n",
      " 58411: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 58414: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0}}},\n",
      " 58430: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 10.0}}},\n",
      " 58433: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58439: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 58441: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 58444: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 58454: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 58457: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58463: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 58468: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 79.0}}},\n",
      " 58942: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 58945: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 58948: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 10.0}}},\n",
      " 58956: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 15.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 58964: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 58969: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 58977: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 58982: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 58990: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 59002: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 54.0}}},\n",
      " 59011: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 91.0}}},\n",
      " 59017: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 190.0}}},\n",
      " 59030: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 59033: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 59036: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 20.0}}},\n",
      " 59043: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 59052: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 18.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 204.0}}},\n",
      " 59060: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 59065: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 439.0}}},\n",
      " 59073: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 59078: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 221.0}}},\n",
      " 59086: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 59091: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 59099: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 59104: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 59112: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 59118: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 59121: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 5.0}}},\n",
      " 59138: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 59149: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 48.0}}},\n",
      " 59189: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 59193: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59202: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59205: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 10.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 59221: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 59224: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59230: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 59232: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 9.0}}},\n",
      " 59235: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 61.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 79.0}}},\n",
      " 59251: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 59254: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59260: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 59262: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 19.0}}},\n",
      " 59265: {7: {('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 193.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 201.0}}},\n",
      " 59281: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 59284: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59290: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 59292: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 59295: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 391.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 374.0}}},\n",
      " 59311: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 24.0}}},\n",
      " 59314: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59320: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 16.0}}},\n",
      " 59322: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 59325: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 202.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 170.0}}},\n",
      " 59341: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 21.0}}},\n",
      " 59344: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59350: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 59359: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 14.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 59394: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 13.0}}},\n",
      " 59403: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 140.0}}},\n",
      " 59419: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 118.0}}},\n",
      " 59430: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 10.0}}},\n",
      " 59431: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 111.0}}},\n",
      " 59444: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 59447: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 21.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 22.0}}},\n",
      " 59463: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 59466: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59472: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59474: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 59477: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 59.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0}}},\n",
      " 59493: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 59496: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59502: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 59504: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 19.0}}},\n",
      " 59507: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 59517: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 59520: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59526: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 59531: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 82.0}}},\n",
      " 60005: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 60008: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 60011: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 11.0}}},\n",
      " 60019: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 60027: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 60032: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 60040: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 60045: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 60053: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 60065: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 54.0}}},\n",
      " 60074: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 91.0}}},\n",
      " 60080: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 191.0}}},\n",
      " 60093: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 60096: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 60099: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 21.0}}},\n",
      " 60106: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 60115: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 18.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 203.0}}},\n",
      " 60123: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 60128: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 441.0}}},\n",
      " 60136: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 60141: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 224.0}}},\n",
      " 60149: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 60154: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 60162: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 60167: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 60175: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 60181: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 60184: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 5.0}}},\n",
      " 60201: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 60212: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 48.0}}},\n",
      " 60252: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 60256: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60265: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60268: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 10.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 60284: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 60287: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60293: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60295: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 60298: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 61.0},\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 120.0}}},\n",
      " 60314: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 60317: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 60323: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 60325: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 60328: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 201.0}}},\n",
      " 60344: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 60347: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60353: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 60355: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 60358: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 393.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 394.0}}},\n",
      " 60374: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 60377: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60383: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 16.0}}},\n",
      " 60385: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 60388: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 197.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 173.0}}},\n",
      " 60404: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 21.0}}},\n",
      " 60407: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60413: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 60422: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 14.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 60457: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 13.0}}},\n",
      " 60466: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 140.0}}},\n",
      " 60482: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 60493: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0}}},\n",
      " 60494: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 116.0}}},\n",
      " 60507: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 60510: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 22.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 23.0}}},\n",
      " 60526: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 60529: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60535: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60537: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 60540: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 86.0}}},\n",
      " 60556: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 60559: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60565: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 60567: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 60570: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 60580: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 10.0}}},\n",
      " 60583: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60589: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 60594: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 82.0}}},\n",
      " 61068: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 61071: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 61074: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 11.0}}},\n",
      " 61082: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 61090: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 61095: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 61103: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 61108: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 61116: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 61128: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 54.0}}},\n",
      " 61137: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 92.0}}},\n",
      " 61143: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 189.0}}},\n",
      " 61156: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 61159: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 61162: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 21.0}}},\n",
      " 61169: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 61178: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 207.0}}},\n",
      " 61186: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 61191: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 438.0}}},\n",
      " 61199: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 23.0}}},\n",
      " 61204: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 222.0}}},\n",
      " 61212: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 61217: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 67.0}}},\n",
      " 61225: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 61230: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 61238: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 61244: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 61247: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 7.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        'runtime': 5.0}}},\n",
      " 61264: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 61275: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 49.0}}},\n",
      " 61315: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 61319: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61328: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61331: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 9.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 61347: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 61350: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61356: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 61358: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 61361: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 61.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 110.0}}},\n",
      " 61377: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 61380: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61386: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 61388: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 61391: {7: {('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 193.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 198.0}}},\n",
      " 61407: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 61410: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61416: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 61418: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 61421: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 390.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 376.0}}},\n",
      " 61437: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 61440: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61446: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 16.0}}},\n",
      " 61448: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 61451: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 200.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 170.0}}},\n",
      " 61467: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 22.0}}},\n",
      " 61470: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61476: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 61485: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 13.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 61520: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 61529: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 138.0}}},\n",
      " 61545: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 116.0}}},\n",
      " 61556: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 10.0}}},\n",
      " 61557: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 108.0}}},\n",
      " 61570: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 61573: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 22.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 22.0}}},\n",
      " 61589: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 61592: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 61598: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61600: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 61603: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 118.0}}},\n",
      " 61619: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 10.0}}},\n",
      " 61622: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61628: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 61630: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 61633: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 61643: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 61646: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61652: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 61657: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 80.0}}},\n",
      " 62131: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 62134: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 62137: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 11.0}}},\n",
      " 62145: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 62153: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 62158: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 66.0}}},\n",
      " 62166: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 62171: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 62179: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 62191: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 52.0}}},\n",
      " 62200: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 94.0}}},\n",
      " 62206: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 191.0}}},\n",
      " 62219: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 62222: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 62225: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 20.0}}},\n",
      " 62232: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 62241: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 205.0}}},\n",
      " 62249: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 62254: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 440.0}}},\n",
      " 62262: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 62267: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 213.0}}},\n",
      " 62275: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 62280: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 67.0}}},\n",
      " 62288: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 62293: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 62301: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 62307: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 62310: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 5.0}}},\n",
      " 62327: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 62338: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 48.0}}},\n",
      " 62378: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 62382: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62391: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62394: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 9.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 62410: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 62413: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62419: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 62421: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 62424: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 62.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 104.0}}},\n",
      " 62440: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 62443: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62449: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 62451: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 62454: {7: {('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 193.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 198.0}}},\n",
      " 62470: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 62473: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62479: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 62481: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 62484: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 392.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 374.0}}},\n",
      " 62500: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 62503: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62509: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 62511: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 62514: {7: {('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 201.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 171.0}}},\n",
      " 62530: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 21.0}}},\n",
      " 62533: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62539: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 62548: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 14.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 33.0}}},\n",
      " 62583: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n",
      " 62592: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 107.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 139.0}}},\n",
      " 62608: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 62619: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0}}},\n",
      " 62620: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 108.0}}},\n",
      " 62633: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 62636: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 22.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 22.0}}},\n",
      " 62652: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 62655: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62661: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62663: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 62666: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 106.0}}},\n",
      " 62682: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 10.0}}},\n",
      " 62685: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62691: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62693: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 62696: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 62706: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 62709: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62715: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 62720: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 80.0}}},\n",
      " 63194: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 63197: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 19.0}}},\n",
      " 63200: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 10.0}}},\n",
      " 63208: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 14.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 13.0}}},\n",
      " 63216: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 13.0}}},\n",
      " 63221: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 67.0}}},\n",
      " 63229: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 63234: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 6.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_tn', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 26.0}}},\n",
      " 63242: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 63254: {7: {('void batched_embedding_forward_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                  'runtime': 53.0}}},\n",
      " 63263: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 92.0}}},\n",
      " 63269: {7: {('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 191.0}}},\n",
      " 63282: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 63285: {7: {('Memcpy HtoD (Pageable -> Device)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 63288: {7: {('index_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 21.0}}},\n",
      " 63295: {7: {('CatArrayBatchedCopy', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 63304: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 208.0}}},\n",
      " 63312: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 63317: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 19.0},\n",
      "             ('volta_sgemm_128x64_tn', (-1,)): {'count': 1, 'runtime': 439.0}}},\n",
      " 63325: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 24.0}}},\n",
      " 63330: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 12.0},\n",
      "             ('volta_sgemm_32x128_tn', (-1,)): {'count': 1, 'runtime': 221.0}}},\n",
      " 63338: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 63343: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 8.0},\n",
      "             ('volta_sgemm_128x32_tn', (-1,)): {'count': 1, 'runtime': 68.0}}},\n",
      " 63351: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 6.0}}},\n",
      " 63356: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 4.0},\n",
      "             ('void gemv2T_kernel_val', (-1,)): {'count': 1, 'runtime': 7.0}}},\n",
      " 63364: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 63370: {7: {('Memcpy HtoD (Pinned -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 2.0}}},\n",
      " 63373: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 3.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 5.0}}},\n",
      " 63390: {7: {('Memcpy DtoH (Device -> Pageable)', (-1,)): {'count': 1,\n",
      "                                                           'runtime': 1.0}}},\n",
      " 63401: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 48.0}}},\n",
      " 63441: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 63445: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 9.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63454: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63457: {7: {('void gemmk1_kernel', (-1,)): {'count': 1, 'runtime': 6.0},\n",
      "             ('void gemvNSP_kernel', (-1,)): {'count': 1, 'runtime': 10.0},\n",
      "             ('void splitKreduce_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 4.0}}},\n",
      " 63473: {7: {('reduce_kernel', (-1,)): {'count': 1, 'runtime': 6.0}}},\n",
      " 63476: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63482: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 2.0}}},\n",
      " 63484: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 63487: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 60.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 116.0}}},\n",
      " 63503: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 63506: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63512: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 63514: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 63517: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 192.0},\n",
      "             ('volta_sgemm_128x32_nt', (-1,)): {'count': 1, 'runtime': 197.0}}},\n",
      " 63533: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 15.0}}},\n",
      " 63536: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63542: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 63544: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 63547: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 392.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 371.0}}},\n",
      " 63563: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 23.0}}},\n",
      " 63566: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63572: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 16.0}}},\n",
      " 63574: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 35.0}}},\n",
      " 63577: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_32x128_nn', (-1,)): {'count': 1, 'runtime': 200.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 174.0}}},\n",
      " 63593: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 22.0}}},\n",
      " 63596: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63602: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 8.0}}},\n",
      " 63611: {7: {('DeviceRadixSortSingleTileKernel', (-1,)): {'count': 1,\n",
      "                                                          'runtime': 13.0},\n",
      "             ('elementwise_kernel_with_index', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0},\n",
      "             ('indexing_backward_kernel', (-1,)): {'count': 1,\n",
      "                                                   'runtime': 319.0},\n",
      "             ('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 13.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 7,\n",
      "                                                        'runtime': 34.0}}},\n",
      " 63646: {7: {('Memcpy DtoD (Device -> Device)', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 20.0},\n",
      "             ('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 12.0}}},\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 63655: {7: {('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 105.0},\n",
      "             ('volta_sgemm_64x64_nn', (-1,)): {'count': 1, 'runtime': 139.0}}},\n",
      " 63671: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 117.0}}},\n",
      " 63682: {7: {('unrolled_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                      'runtime': 10.0}}},\n",
      " 63683: {7: {('void batched_embedding_backward_sgd_kernel_1', (-1,)): {'count': 1,\n",
      "                                                                       'runtime': 113.0}}},\n",
      " 63696: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 7.0}}},\n",
      " 63699: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 21.0},\n",
      "             ('volta_sgemm_64x32_sliced1x4_nt', (-1,)): {'count': 1,\n",
      "                                                         'runtime': 22.0}}},\n",
      " 63715: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 12.0}}},\n",
      " 63718: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63724: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63726: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 11.0}}},\n",
      " 63729: {7: {('Memset (Device)', (-1,)): {'count': 2, 'runtime': 2.0},\n",
      "             ('volta_sgemm_128x32_nn', (-1,)): {'count': 1, 'runtime': 59.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 110.0}}},\n",
      " 63745: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 10.0}}},\n",
      " 63748: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63754: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 4.0}}},\n",
      " 63756: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 18.0}}},\n",
      " 63759: {7: {('void splitKreduce_kernel', (-1,)): {'count': 1, 'runtime': 5.0},\n",
      "             ('volta_sgemm_32x128_nt', (-1,)): {'count': 1, 'runtime': 14.0}}},\n",
      " 63769: {7: {('Memset (Device)', (-1,)): {'count': 1, 'runtime': 1.0},\n",
      "             ('reduce_kernel', (-1,)): {'count': 1, 'runtime': 11.0}}},\n",
      " 63772: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63778: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 1,\n",
      "                                                        'runtime': 3.0}}},\n",
      " 63783: {7: {('vectorized_elementwise_kernel', (-1,)): {'count': 16,\n",
      "                                                        'runtime': 83.0}}}}\n"
     ]
    }
   ],
   "source": [
    "op_device_runtime = get_device_runtime(ops, cc) # dict: op ex_id -> all its device calls and stats\n",
    "pprint(op_device_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 11.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 14.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 67.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 54.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 92.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 195.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 22.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 18.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 203.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 25.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 440.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 220.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 7.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 5.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 49.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 5.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 10.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 61.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 109.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 192.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 199.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 9.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 392.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 368.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 24.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 16.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 189.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 173.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 21.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 34.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 13.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 140.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 106.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 22.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 22.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 12.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 10.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 59.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 118.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 15.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 80.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 10.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 14.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 54.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 92.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 190.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 20.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 18.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 202.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 439.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 220.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 6.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 6.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 48.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 8.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 5.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 10.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 133.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 19.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 192.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 200.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 392.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 365.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 17.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 192.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 172.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 22.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 33.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 13.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 105.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 139.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 10.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 111.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 21.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 22.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 59.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 109.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 14.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 6.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 80.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 11.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 15.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 14.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 55.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 96.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 190.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 21.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 18.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 202.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 437.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 220.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 66.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 6.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 6.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 48.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 6.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 11.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 111.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 192.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 196.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 392.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 372.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 17.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 201.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 173.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 21.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 33.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 13.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 105.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 138.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 109.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 22.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 23.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 59.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 103.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 10.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 13.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 81.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 10.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 13.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 54.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 91.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 190.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 20.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 18.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 205.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 439.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 220.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 66.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 6.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 6.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 48.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 6.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 10.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 61.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 128.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 16.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 192.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 199.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 33.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 392.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 393.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 17.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 193.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 173.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 21.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 32.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 14.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 13.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 320.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 137.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 111.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 21.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 23.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 78.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 14.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 6.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 80.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 11.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 14.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 54.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 94.0 )\n",
      "            aten::bmm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 190.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 21.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 16.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 18.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 203.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 438.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 221.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 67.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 7.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 5.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 50.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 5.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 9.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 61.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 101.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 16.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 193.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 198.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 16.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 393.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 395.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 16.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 192.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 174.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 21.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 33.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 14.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 105.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 138.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 115.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 21.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 23.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 10.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 14.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 79.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 10.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 15.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 14.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 54.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 91.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 190.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 20.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 18.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 204.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 439.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 221.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 7.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 5.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 48.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 5.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 10.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 9.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 61.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 79.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 19.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 193.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 201.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 391.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 374.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 24.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 16.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 202.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 170.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 21.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 33.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 14.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 13.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 140.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 118.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 10.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 111.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 21.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 22.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 59.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 19.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 14.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 82.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 11.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 14.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 54.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 91.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 191.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 21.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 18.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 203.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 441.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 224.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 6.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 5.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 48.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 5.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 10.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 61.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 120.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 192.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 201.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 393.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 394.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 16.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 197.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 173.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 21.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 33.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 14.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 13.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 140.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 116.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 22.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 23.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 86.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 14.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 10.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 82.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 11.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 13.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 54.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 92.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 189.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 21.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 207.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 438.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 23.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 222.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 67.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 7.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 7.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 5.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 7.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 49.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 5.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 9.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 61.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 110.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 193.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 198.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 33.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 390.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 376.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 16.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 200.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 170.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 22.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 33.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 13.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 138.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 116.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 10.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 108.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 22.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 22.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 118.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 10.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 14.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 80.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 11.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 14.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 12.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 66.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 52.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 94.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 191.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 20.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 205.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 440.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 213.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 67.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 7.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 5.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 48.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 6.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 9.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 62.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 104.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 193.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 198.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 33.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 392.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 374.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 201.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 171.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 21.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 33.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 2.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 14.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 107.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 139.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 108.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 22.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 22.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 13.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 106.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 10.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 15.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 80.0 )\n",
      "## BENCHMARK ##\n",
      "    DLRM forward\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 19.0 )\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 10.0 )\n",
      "        module::forward_pass::bottom_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 14.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 13.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 13.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 67.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 6.0 )\n",
      "                volta_sgemm_64x32_sliced1x4_tn:                           ( (-1,), 1, 26.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "        module::forward_pass::embedding_lookup\n",
      "            LookupFunction\n",
      "                void batched_embedding_forward_kernel_1:                  ( (-1,), 1, 53.0 )\n",
      "        module::forward_pass::interaction\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 92.0 )\n",
      "            aten::bmm\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 191.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::to\n",
      "                Memcpy HtoD (Pageable -> Device):                         ( (-1,), 1, 1.0 )\n",
      "            aten::index\n",
      "                index_elementwise_kernel:                                 ( (-1,), 1, 21.0 )\n",
      "            aten::cat\n",
      "                CatArrayBatchedCopy:                                      ( (-1,), 1, 15.0 )\n",
      "        module::forward_pass::top_mlp\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 208.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 19.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x64_tn:                                    ( (-1,), 1, 439.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 24.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 12.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_32x128_tn:                                    ( (-1,), 1, 221.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 11.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 8.0 )\n",
      "                Memset (Device):                                          ( (-1,), 1, 1.0 )\n",
      "                volta_sgemm_128x32_tn:                                    ( (-1,), 1, 68.0 )\n",
      "            aten::relu\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 6.0 )\n",
      "            aten::linear\n",
      "                unrolled_elementwise_kernel:                              ( (-1,), 1, 4.0 )\n",
      "                void gemv2T_kernel_val:                                   ( (-1,), 1, 7.0 )\n",
      "            aten::sigmoid\n",
      "                vectorized_elementwise_kernel:                            ( (-1,), 1, 4.0 )\n",
      "    DLRM loss compute\n",
      "        aten::to\n",
      "            Memcpy HtoD (Pinned -> Device):                       ( (-1,), 1, 2.0 )\n",
      "        aten::binary_cross_entropy\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 5.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 3.0 )\n",
      "    aten::to\n",
      "        Memcpy DtoH (Device -> Pageable):                 ( (-1,), 1, 1.0 )\n",
      "    DLRM backward\n",
      "        Optimizer.zero_grad#SGD.zero_grad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 48.0 )\n",
      "        aten::ones_like\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        BinaryCrossEntropyBackward\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 9.0 )\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        SigmoidBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        AddmmBackward\n",
      "            void gemmk1_kernel:                                   ( (-1,), 1, 6.0 )\n",
      "            void gemvNSP_kernel:                                  ( (-1,), 1, 10.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 4.0 )\n",
      "        aten::sum\n",
      "            reduce_kernel:                                        ( (-1,), 1, 6.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 2.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 60.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 116.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 192.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_128x32_nt:                                ( (-1,), 1, 197.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 15.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 34.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 392.0 )\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 371.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 23.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 16.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 35.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_32x128_nn:                                ( (-1,), 1, 200.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 174.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 22.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 8.0 )\n",
      "        IndexBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 7, 34.0 )\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 13.0 )\n",
      "            elementwise_kernel_with_index:                        ( (-1,), 1, 3.0 )\n",
      "            DeviceRadixSortSingleTileKernel:                      ( (-1,), 1, 13.0 )\n",
      "            indexing_backward_kernel:                             ( (-1,), 1, 319.0 )\n",
      "        SliceBackward\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 12.0 )\n",
      "            Memcpy DtoD (Device -> Device):                       ( (-1,), 1, 20.0 )\n",
      "        BmmBackward0\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 105.0 )\n",
      "            volta_sgemm_64x64_nn:                                 ( (-1,), 1, 139.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 117.0 )\n",
      "        aten::add\n",
      "            unrolled_elementwise_kernel:                          ( (-1,), 1, 10.0 )\n",
      "        LookupFunctionBackward\n",
      "            void batched_embedding_backward_sgd_kernel_1:          ( (-1,), 1, 113.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 7.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 21.0 )\n",
      "            volta_sgemm_64x32_sliced1x4_nt:                       ( (-1,), 1, 22.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 12.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 11.0 )\n",
      "        AddmmBackward\n",
      "            Memset (Device):                                      ( (-1,), 2, 2.0 )\n",
      "            volta_sgemm_128x32_nn:                                ( (-1,), 1, 59.0 )\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 110.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 10.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 4.0 )\n",
      "        ReluBackward0\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 18.0 )\n",
      "        AddmmBackward\n",
      "            volta_sgemm_32x128_nt:                                ( (-1,), 1, 14.0 )\n",
      "            void splitKreduce_kernel:                             ( (-1,), 1, 5.0 )\n",
      "        aten::sum\n",
      "            Memset (Device):                                      ( (-1,), 1, 1.0 )\n",
      "            reduce_kernel:                                        ( (-1,), 1, 11.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        torch::autograd::AccumulateGrad\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 1, 3.0 )\n",
      "        Optimizer.step#SGD.step\n",
      "            vectorized_elementwise_kernel:                        ( (-1,), 16, 83.0 )\n"
     ]
    }
   ],
   "source": [
    "print_all_device_results(roots, op_device_runtime, device_runtime, depth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device runtime breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream: 7\n",
      "{7: {'total': {'runtime': 49980.0,\n",
      "               'subs': {('## BENCHMARK ##', (-1,)): 49980.0}},\n",
      "     ('## BENCHMARK ##', (-1,)): {'runtime': 4993.0,\n",
      "                                  'subs': {('DLRM backward', (-1,)): 3318.0,\n",
      "                                           ('DLRM forward', (-1,)): 1657.0,\n",
      "                                           ('DLRM loss compute', (-1,)): 17.0,\n",
      "                                           ('aten::to', (-1,)): 1.0}},\n",
      "     ('AddmmBackward', (-1,)): {'runtime': 19.0,\n",
      "                                'subs': {('void gemmk1_kernel', (-1,)): 5.0,\n",
      "                                         ('void gemvNSP_kernel', (-1,)): 10.0,\n",
      "                                         ('void splitKreduce_kernel', (-1,)): 4.0}},\n",
      "     ('BinaryCrossEntropyBackward', (-1,)): {'runtime': 12.0,\n",
      "                                             'subs': {('unrolled_elementwise_kernel', (-1,)): 9.0,\n",
      "                                                      ('vectorized_elementwise_kernel', (-1,)): 3.0}},\n",
      "     ('BmmBackward0', (-1,)): {'runtime': 246.0,\n",
      "                               'subs': {('volta_sgemm_32x128_nt', (-1,)): 106.0,\n",
      "                                        ('volta_sgemm_64x64_nn', (-1,)): 140.0}},\n",
      "     ('DLRM backward', (-1,)): {'runtime': 3318.0,\n",
      "                                'subs': {('AddmmBackward', (-1,)): 1955.0,\n",
      "                                         ('BinaryCrossEntropyBackward', (-1,)): 12.0,\n",
      "                                         ('BmmBackward0', (-1,)): 246.0,\n",
      "                                         ('IndexBackward', (-1,)): 382.0,\n",
      "                                         ('LookupFunctionBackward', (-1,)): 106.0,\n",
      "                                         ('Optimizer.step#SGD.step', (-1,)): 80.0,\n",
      "                                         ('Optimizer.zero_grad#SGD.zero_grad', (-1,)): 49.0,\n",
      "                                         ('ReluBackward0', (-1,)): 131.0,\n",
      "                                         ('SigmoidBackward', (-1,)): 3.0,\n",
      "                                         ('SliceBackward', (-1,)): 31.0,\n",
      "                                         ('aten::add', (-1,)): 126.0,\n",
      "                                         ('aten::ones_like', (-1,)): 2.0,\n",
      "                                         ('aten::sum', (-1,)): 122.0,\n",
      "                                         ('torch::autograd::AccumulateGrad', (-1,)): 73.0}},\n",
      "     ('DLRM forward', (-1,)): {'runtime': 1657.0,\n",
      "                               'subs': {('aten::to', (-1,)): 49.0,\n",
      "                                        ('module::forward_pass::bottom_mlp', (-1,)): 158.0,\n",
      "                                        ('module::forward_pass::embedding_lookup', (-1,)): 54.0,\n",
      "                                        ('module::forward_pass::interaction', (-1,)): 326.0,\n",
      "                                        ('module::forward_pass::top_mlp', (-1,)): 1070.0}},\n",
      "     ('DLRM loss compute', (-1,)): {'runtime': 17.0,\n",
      "                                    'subs': {('aten::binary_cross_entropy', (-1,)): 15.0,\n",
      "                                             ('aten::to', (-1,)): 2.0}},\n",
      "     ('IndexBackward', (-1,)): {'runtime': 382.0,\n",
      "                                'subs': {('DeviceRadixSortSingleTileKernel', (-1,)): 13.0,\n",
      "                                         ('elementwise_kernel_with_index', (-1,)): 3.0,\n",
      "                                         ('indexing_backward_kernel', (-1,)): 319.0,\n",
      "                                         ('unrolled_elementwise_kernel', (-1,)): 13.0,\n",
      "                                         ('vectorized_elementwise_kernel', (-1,)): 34.0}},\n",
      "     ('LookupFunction', (-1,)): {'runtime': 54.0,\n",
      "                                 'subs': {('void batched_embedding_forward_kernel_1', (-1,)): 54.0}},\n",
      "     ('LookupFunctionBackward', (-1,)): {'runtime': 106.0,\n",
      "                                         'subs': {('void batched_embedding_backward_sgd_kernel_1', (-1,)): 106.0}},\n",
      "     ('Optimizer.step#SGD.step', (-1,)): {'runtime': 80.0,\n",
      "                                          'subs': {('vectorized_elementwise_kernel', (-1,)): 80.0}},\n",
      "     ('Optimizer.zero_grad#SGD.zero_grad', (-1,)): {'runtime': 49.0,\n",
      "                                                    'subs': {('vectorized_elementwise_kernel', (-1,)): 49.0}},\n",
      "     ('ReluBackward0', (-1,)): {'runtime': 8.0,\n",
      "                                'subs': {('vectorized_elementwise_kernel', (-1,)): 8.0}},\n",
      "     ('SigmoidBackward', (-1,)): {'runtime': 3.0,\n",
      "                                  'subs': {('vectorized_elementwise_kernel', (-1,)): 3.0}},\n",
      "     ('SliceBackward', (-1,)): {'runtime': 31.0,\n",
      "                                'subs': {('Memcpy DtoD (Device -> Device)', (-1,)): 19.0,\n",
      "                                         ('vectorized_elementwise_kernel', (-1,)): 12.0}},\n",
      "     ('aten::add', (-1,)): {'runtime': 117.0,\n",
      "                            'subs': {('unrolled_elementwise_kernel', (-1,)): 117.0}},\n",
      "     ('aten::binary_cross_entropy', (-1,)): {'runtime': 15.0,\n",
      "                                             'subs': {('Memcpy DtoD (Device -> Device)', (-1,)): 3.0,\n",
      "                                                      ('reduce_kernel', (-1,)): 7.0,\n",
      "                                                      ('vectorized_elementwise_kernel', (-1,)): 5.0}},\n",
      "     ('aten::bmm', (-1,)): {'runtime': 195.0,\n",
      "                            'subs': {('volta_sgemm_32x128_tn', (-1,)): 195.0}},\n",
      "     ('aten::cat', (-1,)): {'runtime': 92.0,\n",
      "                            'subs': {('CatArrayBatchedCopy', (-1,)): 92.0}},\n",
      "     ('aten::index', (-1,)): {'runtime': 22.0,\n",
      "                              'subs': {('index_elementwise_kernel', (-1,)): 22.0}},\n",
      "     ('aten::linear', (-1,)): {'runtime': 249.0,\n",
      "                               'subs': {('unrolled_elementwise_kernel', (-1,)): 32.0,\n",
      "                                        ('volta_sgemm_128x32_tn', (-1,)): 217.0}},\n",
      "     ('aten::ones_like', (-1,)): {'runtime': 2.0,\n",
      "                                  'subs': {('vectorized_elementwise_kernel', (-1,)): 2.0}},\n",
      "     ('aten::relu', (-1,)): {'runtime': 37.0,\n",
      "                             'subs': {('vectorized_elementwise_kernel', (-1,)): 37.0}},\n",
      "     ('aten::sigmoid', (-1,)): {'runtime': 4.0,\n",
      "                                'subs': {('vectorized_elementwise_kernel', (-1,)): 4.0}},\n",
      "     ('aten::sum', (-1,)): {'runtime': 6.0,\n",
      "                            'subs': {('reduce_kernel', (-1,)): 6.0}},\n",
      "     ('aten::to', (-1,)): {'runtime': 23.0,\n",
      "                           'subs': {('Memcpy DtoH (Device -> Pageable)', (-1,)): 1.0,\n",
      "                                    ('Memcpy HtoD (Pageable -> Device)', (-1,)): 1.0,\n",
      "                                    ('Memcpy HtoD (Pinned -> Device)', (-1,)): 21.0}},\n",
      "     ('module::forward_pass::bottom_mlp', (-1,)): {'runtime': 158.0,\n",
      "                                                   'subs': {('aten::linear', (-1,)): 136.0,\n",
      "                                                            ('aten::relu', (-1,)): 22.0}},\n",
      "     ('module::forward_pass::embedding_lookup', (-1,)): {'runtime': 54.0,\n",
      "                                                         'subs': {('LookupFunction', (-1,)): 54.0}},\n",
      "     ('module::forward_pass::interaction', (-1,)): {'runtime': 326.0,\n",
      "                                                    'subs': {('aten::bmm', (-1,)): 195.0,\n",
      "                                                             ('aten::cat', (-1,)): 107.0,\n",
      "                                                             ('aten::index', (-1,)): 22.0,\n",
      "                                                             ('aten::to', (-1,)): 2.0}},\n",
      "     ('module::forward_pass::top_mlp', (-1,)): {'runtime': 1070.0,\n",
      "                                                'subs': {('aten::linear', (-1,)): 1001.0,\n",
      "                                                         ('aten::relu', (-1,)): 65.0,\n",
      "                                                         ('aten::sigmoid', (-1,)): 4.0}},\n",
      "     ('torch::autograd::AccumulateGrad', (-1,)): {'runtime': 3.0,\n",
      "                                                  'subs': {('vectorized_elementwise_kernel', (-1,)): 3.0}}}}\n"
     ]
    }
   ],
   "source": [
    "dt_breakdown = device_runtime_breakdown(roots, op_device_runtime, depth=0)\n",
    "# pprint(dt_breakdown)\n",
    "truncate_count = 10\n",
    "flatten = {}\n",
    "for stream, v in dt_breakdown.items():\n",
    "    print(\"Stream: {}\".format(stream))\n",
    "    flatten[stream] = {}\n",
    "    get_major_device_results(device_runtime, dt_breakdown[stream], flatten[stream])\n",
    "pprint(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream: 7\n",
      "    Total device time: 92363 (in us, same below)\n",
      "    Device idle time: 42383.0 (45.89%)\n",
      "    Device active time: 49980.0 (54.11%)\n",
      "      ## BENCHMARK ##:                                     (49980.0, 100.00%, 10)                                                        (-1,)\n",
      "        DLRM backward:                                         (3318.0, 66.45%, 1)                                                        (-1,)\n",
      "          AddmmBackward:                                            (1955.0, 58.92%, 8)                                                        (-1,)\n",
      "            void gemvNSP_kernel:                                           (10.0, 52.63%, 1)                                                        (-1,)\n",
      "            void gemmk1_kernel:                                             (5.0, 26.32%, 1)                                                        (-1,)\n",
      "            void splitKreduce_kernel:                                       (4.0, 21.05%, 1)                                                        (-1,)\n",
      "          IndexBackward:                                             (382.0, 11.51%, 1)                                                        (-1,)\n",
      "            indexing_backward_kernel:                                     (319.0, 83.51%, 1)                                                        (-1,)\n",
      "            vectorized_elementwise_kernel:                                 (34.0, 8.90%, 7)                                                        (-1,)\n",
      "            unrolled_elementwise_kernel:                                   (13.0, 3.40%, 1)                                                        (-1,)\n",
      "            DeviceRadixSortSingleTileKernel:                               (13.0, 3.40%, 1)                                                        (-1,)\n",
      "            elementwise_kernel_with_index:                                  (3.0, 0.79%, 1)                                                        (-1,)\n",
      "          BmmBackward0:                                              (246.0, 7.41%, 1)                                                        (-1,)\n",
      "            volta_sgemm_64x64_nn:                                         (140.0, 56.91%, 1)                                                        (-1,)\n",
      "            volta_sgemm_32x128_nt:                                        (106.0, 43.09%, 1)                                                        (-1,)\n",
      "          ReluBackward0:                                             (131.0, 3.95%, 7)                                                        (-1,)\n",
      "            vectorized_elementwise_kernel:                                  (8.0, 100.00%, 1)                                                        (-1,)\n",
      "          aten::add:                                                 (126.0, 3.80%, 2)                                                        (-1,)\n",
      "            unrolled_elementwise_kernel:                                  (117.0, 100.00%, 1)                                                        (-1,)\n",
      "          aten::sum:                                                 (122.0, 3.68%, 8)                                                        (-1,)\n",
      "            reduce_kernel:                                                  (6.0, 100.00%, 1)                                                        (-1,)\n",
      "          LookupFunctionBackward:                                    (106.0, 3.19%, 1)                                                        (-1,)\n",
      "            void batched_embedding_backward_sgd_kernel_1:                 (106.0, 100.00%, 1)                                                        (-1,)\n",
      "          Optimizer.step#SGD.step:                                    (80.0, 2.41%, 1)                                                        (-1,)\n",
      "            vectorized_elementwise_kernel:                                 (80.0, 100.00%, 16)                                                        (-1,)\n",
      "          torch::autograd::AccumulateGrad:                            (73.0, 2.20%, 16)                                                        (-1,)\n",
      "            vectorized_elementwise_kernel:                                  (3.0, 100.00%, 1)                                                        (-1,)\n",
      "          Optimizer.zero_grad#SGD.zero_grad:                          (49.0, 1.48%, 1)                                                        (-1,)\n",
      "            vectorized_elementwise_kernel:                                 (49.0, 100.00%, 16)                                                        (-1,)\n",
      "          Others:                                                             (48.0, 1.45%)\n",
      "        DLRM forward:                                          (1657.0, 33.19%, 1)                                                        (-1,)\n",
      "          module::forward_pass::top_mlp:                            (1070.0, 64.57%, 1)                                                        (-1,)\n",
      "            aten::linear:                                                (1001.0, 93.55%, 5)                                                        (-1,)\n",
      "              volta_sgemm_128x32_tn:                                           (203.0, 91.86%, 1)                                                        (-1,)\n",
      "              unrolled_elementwise_kernel:                                      (18.0, 8.14%, 1)                                                        (-1,)\n",
      "            aten::relu:                                                    (65.0, 6.07%, 4)                                                        (-1,)\n",
      "              vectorized_elementwise_kernel:                                    (25.0, 100.00%, 1)                                                        (-1,)\n",
      "            aten::sigmoid:                                                  (4.0, 0.37%, 1)                                                        (-1,)\n",
      "              vectorized_elementwise_kernel:                                     (4.0, 100.00%, 1)                                                        (-1,)\n",
      "          module::forward_pass::interaction:                         (326.0, 19.67%, 1)                                                        (-1,)\n",
      "            aten::bmm:                                                    (195.0, 59.82%, 1)                                                        (-1,)\n",
      "              volta_sgemm_32x128_tn:                                           (195.0, 100.00%, 1)                                                        (-1,)\n",
      "            aten::cat:                                                    (107.0, 32.82%, 2)                                                        (-1,)\n",
      "              CatArrayBatchedCopy:                                              (92.0, 100.00%, 1)                                                        (-1,)\n",
      "            aten::index:                                                   (22.0, 6.75%, 1)                                                        (-1,)\n",
      "              index_elementwise_kernel:                                         (22.0, 100.00%, 1)                                                        (-1,)\n",
      "            aten::to:                                                       (2.0, 0.61%, 2)                                                        (-1,)\n",
      "              Memcpy HtoD (Pageable -> Device):                                  (1.0, 100.00%, 1)                                                        (-1,)\n",
      "          module::forward_pass::bottom_mlp:                          (158.0, 9.54%, 1)                                                        (-1,)\n",
      "            aten::linear:                                                 (136.0, 86.08%, 3)                                                        (-1,)\n",
      "              unrolled_elementwise_kernel:                                      (14.0, 50.00%, 1)                                                        (-1,)\n",
      "              volta_sgemm_128x32_tn:                                            (14.0, 50.00%, 1)                                                        (-1,)\n",
      "            aten::relu:                                                    (22.0, 13.92%, 3)                                                        (-1,)\n",
      "              vectorized_elementwise_kernel:                                    (12.0, 100.00%, 1)                                                        (-1,)\n",
      "          module::forward_pass::embedding_lookup:                     (54.0, 3.26%, 1)                                                        (-1,)\n",
      "            LookupFunction:                                                (54.0, 100.00%, 1)                                                        (-1,)\n",
      "              void batched_embedding_forward_kernel_1:                          (54.0, 100.00%, 1)                                                        (-1,)\n",
      "          aten::to:                                                   (49.0, 2.96%, 3)                                                        (-1,)\n",
      "            Memcpy HtoD (Pinned -> Device):                                (19.0, 100.00%, 1)                                                        (-1,)\n",
      "        DLRM loss compute:                                       (17.0, 0.34%, 1)                                                        (-1,)\n",
      "          aten::binary_cross_entropy:                                 (15.0, 88.24%, 1)                                                        (-1,)\n",
      "            reduce_kernel:                                                  (7.0, 46.67%, 1)                                                        (-1,)\n",
      "            vectorized_elementwise_kernel:                                  (5.0, 33.33%, 1)                                                        (-1,)\n",
      "            Memcpy DtoD (Device -> Device):                                 (3.0, 20.00%, 1)                                                        (-1,)\n",
      "          aten::to:                                                    (2.0, 11.76%, 1)                                                        (-1,)\n",
      "            Memcpy HtoD (Pinned -> Device):                                 (2.0, 100.00%, 1)                                                        (-1,)\n",
      "        aten::to:                                                 (1.0, 0.02%, 1)                                                        (-1,)\n",
      "          Memcpy DtoH (Device -> Pageable):                            (1.0, 100.00%, 1)                                                        (-1,)\n"
     ]
    }
   ],
   "source": [
    "for stream, v in dt_breakdown.items():\n",
    "    print(\"Stream: {}\".format(stream))\n",
    "    print_major_device_results(device_runtime, dt_breakdown[stream], flatten[stream], truncate_count=truncate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dt_breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "cs=cm.Set1([1, 3, 2, 25, 4, 5, 6, 7, 8, 11])\n",
    "\n",
    "def plot_pie_chart(flatten, key=\"total\", truncate_count=100, depth=0):\n",
    "    d = flatten[key]\n",
    "    \n",
    "    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "    stats = sorted(d[\"subs\"].items(), key=lambda x: x[1], reverse=True)\n",
    "    labels = [x[0] for x in stats]\n",
    "    runtime = [x[1] for x in stats]\n",
    "    explode = np.zeros(len(runtime))\n",
    "    if len(explode) > 2:\n",
    "        explode[1] = 0.1\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    wedges, texts = ax1.pie(runtime, explode=explode, shadow=True, startangle=90, colors=cs)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    ax1.set_title(key)\n",
    "    \n",
    "    ax1.legend(wedges, zip(labels, [\"{:.2f}%\".format(r / d[\"runtime\"] * 100) for r in runtime]),\n",
    "          title=\"Breakdown\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "          fontsize=14)\n",
    "    \n",
    "    for label in labels:\n",
    "        if label in flatten:\n",
    "            plot_pie_chart(flatten, key=label, truncate_count=truncate_count, depth=depth+1)\n",
    "\n",
    "    if depth == 0:\n",
    "        plt.show()\n",
    "\n",
    "for stream, v in flatten.items():\n",
    "    print(\"########################\")\n",
    "    print(\"STREAM: {}\".format(stream))\n",
    "    print(\"########################\")\n",
    "    plot_pie_chart(v, truncate_count=truncate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_name_to_legend(name):\n",
    "    if 'gemm' in name:\n",
    "        return 'gemm'\n",
    "    if 'gemv' in name:\n",
    "        return 'gemv'\n",
    "    if 'Memset' in name:\n",
    "        return 'Memset'\n",
    "    if 'Memcpy' in name:\n",
    "        return 'Memcpy'\n",
    "    if 'vectorized_elementwise' in name:\n",
    "        return 'elwt'\n",
    "    if 'unrolled_elementwise' in name:\n",
    "        return 'permute'\n",
    "    if 'embedding_forward' in name:\n",
    "        return 'ELF'\n",
    "    if 'embedding_backward' in name:\n",
    "        return 'ELB'\n",
    "    if 'splitK' in name:\n",
    "        return 'splitK'\n",
    "    if 'reduce_kernel' in name:\n",
    "        return 'reduce'\n",
    "    if 'CatArray' in name:\n",
    "        return 'CatCopy'\n",
    "    if 'indexing' in name:\n",
    "        return 'IdxBwd'\n",
    "    return name[:8]\n",
    "\n",
    "def plot_bar_chart(flatten, key=\"total\", truncate_count=100, depth=0):\n",
    "    per_op = {}\n",
    "    total = 0.0\n",
    "    for k, v in flatten.items():\n",
    "        if k == 'total' or 'DLRM ' in k[0] or 'module' in k[0]: # Skip all labels\n",
    "            continue\n",
    "        k0 = k[0] if '#' not in k[0] else k[0].split('#')[0]\n",
    "        if k0 not in per_op.keys():\n",
    "            per_op[k0] = 0.0\n",
    "        per_op[k0] += v['runtime']\n",
    "        total += v['runtime']\n",
    "        \n",
    "    tmp = sorted(per_op.items(), key=lambda x: x[1], reverse=True)\n",
    "    op = [x[0] for x in tmp]\n",
    "    p = [x[1] / total for x in tmp]\n",
    "    df0 = pd.DataFrame({\n",
    "        'Active time': [flatten['total']['runtime'] / device_runtime],\n",
    "        'Idle time': [1 - flatten['total']['runtime'] / device_runtime]\n",
    "    })\n",
    "    df = pd.DataFrame([p], columns=op)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 3))\n",
    "    axes[0] = plt.subplot2grid(shape=(2, 12), loc=(0, 1), colspan=10) # Uneven sizes of subplots\n",
    "    axes[1] = plt.subplot2grid(shape=(2, 12), loc=(1, 0), colspan=12)\n",
    "\n",
    "    ax0 = df0.plot(stacked=True, title=\" \", kind='barh', width=0.05, ax=axes[0], cmap='Set2')\n",
    "    vals = ax0.get_xticks()\n",
    "    ax0.set_xticklabels(['{:,.0%}'.format(x) for x in vals])\n",
    "    ax0.set_xlim((0.0, 1.0))\n",
    "    ax0.set_yticks([])\n",
    "    ax0.set_yticklabels([])\n",
    "    ax0.set_ylim((-0.03, 0.03))\n",
    "    ax0.legend(loc=\"lower center\", ncol=2, bbox_to_anchor=(0.5, -0.7), frameon=False, fontsize=10.5)\n",
    "    \n",
    "    ax1 = df.plot(stacked=True, kind='barh', width=0.05, ax=axes[1], cmap='tab20b') # https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    vals = ax1.get_xticks()\n",
    "    ax1.set_xticklabels(['{:,.0%}'.format(x) for x in vals])\n",
    "    ax1.set_xlim((0.0, 1.0))\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_ylim((-0.03, 0.03))\n",
    "    ax1.legend(loc=\"lower center\", ncol=4, bbox_to_anchor=(0.5, -1.7), frameon=False, fontsize=10.5)\n",
    "    \n",
    "    # Space between subplots\n",
    "    plt.subplots_adjust(hspace=0.7)\n",
    "    \n",
    "    # Lines across subplots\n",
    "    con1 = ConnectionPatch(xyA=(0,-0.025), xyB=(0,0.025), coordsA=\"data\", coordsB=\"data\", axesA=ax0, axesB=ax1, linestyle='dotted')\n",
    "    con2 = ConnectionPatch(xyA=(flatten['total']['runtime'] / device_runtime,-0.025), xyB=(1,0.025), coordsA=\"data\", coordsB=\"data\", axesA=ax0, axesB=ax1, linestyle='dotted')\n",
    "    ax1.add_artist(con1)\n",
    "    ax1.add_artist(con2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['figure.figsize'] = [12, 3]\n",
    "    plt.savefig('active_time_breakdown.pdf', bbox_inches='tight')\n",
    "    \n",
    "    t = {}\n",
    "    for k, v in flatten.items():\n",
    "        if k == 'total' or 'DLRM ' in k[0] or 'module' in k[0]: # Skip all labels\n",
    "            continue\n",
    "        k0 = k[0] if '#' not in k[0] else k[0].split('#')[0]\n",
    "        if k0 not in t.keys(): # Per Op\n",
    "            t[k0] = []\n",
    "        a = {}\n",
    "        total = 0.0\n",
    "        for kk, vv in v['subs'].items():\n",
    "            a[kk] = vv\n",
    "            total += vv\n",
    "        a = {x: y / total for x, y in a.items()}\n",
    "        t[k0].append(a)\n",
    "    so = sorted(t.items(), key=lambda x: per_op[x if isinstance(x, str) else x[0]], reverse=True)\n",
    "    pprint([len(s[1]) for s in so])\n",
    "    so = [list(x) for x in so]\n",
    "\n",
    "    for s in so:\n",
    "        keys = set()\n",
    "        to_be_deleted = []\n",
    "        # Find duplicate kernel combinations\n",
    "        for idx, b in enumerate(s[1]):\n",
    "            k = []\n",
    "            for x in b.keys():\n",
    "                if 'volta_sgemm' in x[0]:\n",
    "                    xx = 'volta_sgemm'\n",
    "                elif 'maxwell_sgemm' in x[0]:\n",
    "                    xx = 'maxwell_sgemm'\n",
    "                elif isinstance(x, str):\n",
    "                    xx = x\n",
    "                else:\n",
    "                    xx = x[0]\n",
    "                k.append(xx)\n",
    "            k = tuple(sorted(k))\n",
    "            if k in keys:\n",
    "                to_be_deleted.append(idx)\n",
    "            else:\n",
    "                keys.add(k)\n",
    "        # Delete duplicate kernel combinations\n",
    "        for idx in sorted(to_be_deleted, reverse=True):\n",
    "            del s[1][idx]\n",
    "    pprint([len(s[1]) for s in so])\n",
    "#     pprint(so)\n",
    "\n",
    "    nrows, ncols = 3, 8\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=8, figsize=(20, 13))\n",
    "    xx, yy = 0, 0\n",
    "    rects, texts = [], []\n",
    "    for s in so:\n",
    "        num_variants = len(s[1])\n",
    "        for idx, b in enumerate(s[1]):\n",
    "            ax = plt.subplot(nrows, ncols, 1+xx+idx+yy*ncols)\n",
    "            p = sorted(b.items(), key=lambda x: x[1], reverse=True)\n",
    "            kernel_name = [kernel_name_to_legend(x[0] if isinstance(x[0], str) else x[0][0]) for x in p]\n",
    "            perc = [x[1] for x in p]\n",
    "            df = pd.DataFrame([perc], columns=kernel_name)\n",
    "            _ = df.plot(stacked=True, kind='bar', ax=ax, cmap='tab20b')\n",
    "            ax.get_legend().remove()\n",
    "            if xx+idx == 0:\n",
    "                ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "                ax.set_yticklabels(['{:,.0%}'.format(x) for x in [0, 0.2, 0.4, 0.6, 0.8, 1.0]])\n",
    "            else:\n",
    "                ax.set_yticks([])\n",
    "                ax.set_yticklabels([])\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_ylim((0.0, 1.0))\n",
    "            ax.legend(bbox_to_anchor=(1.02, 1.04), loc='upper left', frameon=False, fontsize=14)\n",
    "            if idx == (len(s[1]) - 1) / 2:\n",
    "                ax.set_title(s[0], loc='left', fontsize=(16 if len(s[0]) < 20 else 12))\n",
    "            \n",
    "        # Borders\n",
    "        llc_x = 1.0 / ncols * (xx) + (0.014 if xx != 0 else 0)\n",
    "        llc_y = 1.0 / nrows * (nrows-yy-1) + 0.01\n",
    "        rx = 1.0 / ncols * (num_variants) + (0.014 if xx == 0 else 0)\n",
    "        ry = 1.0 / nrows\n",
    "        rects.append(plt.Rectangle(\n",
    "            (llc_x, llc_y), rx, ry, fill=False, color=\"k\", lw=2, zorder=1000, transform=fig.transFigure, figure=fig\n",
    "        ))\n",
    "\n",
    "        # Subplot position\n",
    "        xx += num_variants\n",
    "        if xx >= ncols:\n",
    "            xx = 0\n",
    "            yy += 1\n",
    "\n",
    "    # Hardcoded for now\n",
    "    axes[2,7].set_axis_off()\n",
    "    \n",
    "    fig.patches.extend(rects)\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['figure.figsize'] = [20, 13]\n",
    "    plt.savefig('dominating_op_breakdown.pdf', bbox_inches='tight')\n",
    "\n",
    "for stream, v in flatten.items():\n",
    "    print(\"########################\")\n",
    "    print(\"STREAM: {}\".format(stream))\n",
    "    print(\"########################\")\n",
    "    plot_bar_chart(v, truncate_count=truncate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def histogram(df, perc=True, is_abs=False, bins=[0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.8, 1.0, 1.5, 2.0, 3.0, 4.0]):\n",
    "    count = len(df)\n",
    "    ret = {}\n",
    "    if is_abs:\n",
    "        tmp_bins = []\n",
    "        for i in range(0, len(bins) - 1):\n",
    "            tmp_bins.append(-bins[len(bins) - 1 - i])\n",
    "        for b in bins:\n",
    "            tmp_bins.append(b)\n",
    "        bins = tmp_bins\n",
    "    for idx, b in enumerate(bins):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        ret[(bins[idx-1], bins[idx])] = 0\n",
    "    for x in df:\n",
    "        for idx, b in enumerate(bins):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            if x >= bins[idx-1] and x < bins[idx]:\n",
    "                ret[(bins[idx-1], bins[idx])] += 1\n",
    "                break\n",
    "    for b, c in sorted(ret.items(), key=lambda x: x[0]):\n",
    "        if perc:\n",
    "            print(\"{:.0f}% - {:.0f}%: {:.2f}%\".format(b[0] * 100, b[1] * 100, c / count * 100))\n",
    "        else:\n",
    "            print(\"{:.2f} - {:.2f}: {:.2f}%\".format(b[0], b[1], c / count * 100))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Type 1 overhead: between two op calls\n",
    "# Type 2 overhead: before the first device call, op-specific\n",
    "# Type 3 overhead: after the last device call, op-specific\n",
    "# Type 4 overhead: kernel launches themselves, kernel-launch-type-specific\n",
    "# Type 5 overhead: sum of gaps between kernel launches, op-specific\n",
    "overheads = {'independent': {}}\n",
    "overheads['independent']['t1'] = [] # Independent from names\n",
    "overheads['independent']['t4'] = {} # Independent from names\n",
    "launches_dict = {}\n",
    "\n",
    "for i, op in enumerate(ops):\n",
    "    name = op.name()\n",
    "    if name not in overheads.keys():\n",
    "        overheads[name] = {}\n",
    "\n",
    "    if 't2' not in overheads[name].keys():\n",
    "        overheads[name]['t2'] = []\n",
    "    if 't3' not in overheads[name].keys():\n",
    "        overheads[name]['t3'] = []\n",
    "    if 't5' not in overheads[name].keys():\n",
    "        overheads[name]['t5'] = []\n",
    "\n",
    "    launches = get_event_all_kernel_launches(op)\n",
    "    launches = [x for x in launches if x.name() == \"cudaMemcpyAsync\" or x.name() == \"cudaLaunchKernel\" or x.name() == \"cudaStreamSynchronize\"]\n",
    "    \n",
    "    if len(launches) > 0:\n",
    "        overheads[name]['t2'].append(launches[0].start_time() - op.start_time())\n",
    "        overheads[name]['t3'].append(op.end_time() - launches[-1].end_time())\n",
    "        if len(launches) > 1:\n",
    "            overheads[name]['t5'].extend([launches[i].start_time() - launches[i-1].end_time() for i in range(1, len(launches))])\n",
    "        else:\n",
    "            overheads[name]['t5'].append(0)\n",
    "        \n",
    "        # T4 is launch-type-dependent\n",
    "        for x in launches:\n",
    "            if x.name() not in overheads['independent']['t4']:\n",
    "                overheads['independent']['t4'][x.name()] = []\n",
    "            overheads['independent']['t4'][x.name()].append(x.duration())\n",
    "        \n",
    "        if op.name() not in launches_dict.keys():\n",
    "            launches_dict[op.name()] = []\n",
    "            for x in launches:\n",
    "                launches_dict[op.name()].append(x.name())\n",
    "    else:\n",
    "        # If an op doesn't have kernel calls it has only T5 overheads\n",
    "        if op.name() not in overheads[name].keys():\n",
    "            overheads[name]['t5'] = []\n",
    "        overheads[name]['t5'].append(op.duration())\n",
    "\n",
    "    if i == 0:\n",
    "        continue\n",
    "    prev_op = ops[i-1]\n",
    "    \n",
    "    # Only consider adjacent ops under the SAME MODULE\n",
    "    if prev_op.parent != op.parent:\n",
    "        continue\n",
    "        \n",
    "    gap = op.start_time() - prev_op.end_time()\n",
    "    if gap < 200: # Skip dataloading gaps\n",
    "        overheads['independent']['t1'].append(gap) # Some pairs of ops are actually inserted by a runtime call which has been filtered from ops. TODO: fix it.\n",
    "\n",
    "# T1: mean ~= 21, std ~= 20\n",
    "histogram(overheads['independent']['t1'], perc=False, bins=[0, 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 100000])\n",
    "print(np.mean(overheads['independent']['t1']), np.std(overheads['independent']['t1']))\n",
    "\n",
    "# T2, T3, T5\n",
    "t2 = {k: (np.mean(v['t2']), np.std(v['t2'])) for k, v in overheads.items() if k != 'independent' and len(v['t2']) > 0}\n",
    "pprint(t2)\n",
    "t3 = {k: (np.mean(v['t3']), np.std(v['t3'])) for k, v in overheads.items() if k != 'independent' and len(v['t3']) > 0}\n",
    "pprint(t3)\n",
    "t5 = {k: (np.mean(v['t5']), np.std(v['t5'])) for k, v in overheads.items() if k != 'independent' and len(v['t5']) > 0}\n",
    "pprint(t5)\n",
    "\n",
    "# T4\n",
    "for t, l in overheads['independent']['t4'].items():\n",
    "    print(t, np.mean(l), np.std(l))\n",
    "    \n",
    "o = {\n",
    "    \"t1\": (np.mean(overheads['independent']['t1']), np.std(overheads['independent']['t1'])),\n",
    "    \"t2\": t2,\n",
    "    \"t3\": t3,\n",
    "    \"t4\": {\n",
    "        t: (np.mean(l), np.std(l)) for t, l in overheads['independent']['t4'].items()\n",
    "    },\n",
    "    \"t5\": t5,\n",
    "    \"launches\": launches_dict\n",
    "}\n",
    "\n",
    "with open(\"overheads_{}.json\".format(model_name), \"w\") as f:\n",
    "    json.dump(o, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multistream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_kernels = []\n",
    "for _, c in cc.items():\n",
    "    for _, v in c[\"callees\"].items():\n",
    "        if v[\"executor\"] is not None:\n",
    "            all_kernels.append(v[\"executor\"])\n",
    "all_kernels = sorted(all_kernels, key=lambda x: x.start_time())\n",
    "\n",
    "idle_time = 0\n",
    "last_end = all_kernels[0].start_time() + all_kernels[0].duration()\n",
    "overlapped = 0\n",
    "for k in all_kernels:\n",
    "    if k.start_time() > last_end:\n",
    "        idle_time += k.start_time() - last_end\n",
    "        last_end = k.start_time() + k.duration()\n",
    "    else:\n",
    "        last_end = max(last_end, k.start_time() + k.duration())\n",
    "        overlapped += min(last_end, k.start_time() + k.duration()) - k.start_time()\n",
    "\n",
    "print(\"device_runtime\", device_runtime)\n",
    "print(\"idle_time:\", idle_time)\n",
    "print(\"overlapped_time\", overlapped)"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "312645203253544"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200830-210251",
   "description": "Analyze a two-iteration trace file generated by ATC ; extract multi-level (module/op) runtime breakdown and major input output shapes.",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/zhongyilin/fbsource/fbcode/bento/kernels/local/zhongyilin/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "303205",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "1425096571211434",
   "tags": "FBLSim,CEA,ATC,dyno,gputrace,trace",
   "tasks": "",
   "title": "GPU Trace Analysis"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
